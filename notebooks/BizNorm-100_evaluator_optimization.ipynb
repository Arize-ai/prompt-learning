{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba10b8c1",
   "metadata": {},
   "source": [
    "[![My Image](data:image/png;base64,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)](https://arize.com)\n",
    "\n",
    "# Evaluator Prompt Optimization\n",
    "\n",
    "In this notebook we'll be using the Prompt Learning SDK to optimize an LLM-as-Judge Eval Prompt. LLM-as-Judge evaluators use an LLM to evaluate LLM outputs, and are effective and versatile in testing/evaluating your LLM applications. You can learn more [here](https://arize.com/llm-as-a-judge/).\n",
    "\n",
    "Since your evals use LLMs, the prompts you provide to those LLMs dictate what your eval does. In practice, the goal is to ALIGN your eval with your goals. You want to bring your eval to a level of competence that you would expect from a human who manually evaluates outputs.\n",
    "\n",
    "This notebook shows you how to build an evaluator that checks if outputs are normalized/sanitized, and then align the evaluator with your expectations for normalization/sanitization so you can trust this eval in production by optimizing its prompt. \n",
    "\n",
    "[Learn more about Arize Prompt Learning here.](https://arize.com/blog/prompt-learning-using-english-feedback-to-optimize-llm-systems/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3751322",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"arize-phoenix-evals>=2.0.0\" arize-phoenix openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0d8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from phoenix.evals import llm_generate, OpenAIModel\n",
    "import warnings\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8d42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY'] or getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76bfaf",
   "metadata": {},
   "source": [
    "# BizNorm-100 Benchmark\n",
    "\n",
    "BizNorm-100 is a synthetically created dataset containing 100 queries. The goal is to normalize these queries with respect to certain ruleset. \n",
    "\n",
    "For example, the query\n",
    "\n",
    "```My card 3333-4444-5555-6666 was charged $1200 on 1/12/2025. The record still shows my old phone, 646-555-2201, and the system emailed the receipt to anthony.rogers@company.org. Can you fix this ASAP?```\n",
    "\n",
    "should be normalized to\n",
    "\n",
    "```[PII ALERT] My card [CARD] was charged usd 1200.00 on 2025-01-12. The record still shows my old phone, [PHONE], and the system emailed the receipt to [EMAIL]. Can you fix this as soon as possible? -- Company Confidential --```\n",
    "\n",
    "**See the normalization ruleset in BizNorm-ruleset.md.**\n",
    "\n",
    "## Train/Test Split\n",
    "\n",
    "We will be using the training set to train our evaluator with Prompt Learning.\n",
    "We will be using the test set to test our evaluator's accuracy on data it has not been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85df205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BizNorm_100 = pd.read_csv(\"../datasets/BizNorm-100.csv\")\n",
    "train = BizNorm_100.sample(frac=0.7, random_state=42)\n",
    "test = BizNorm_100.drop(train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca9824",
   "metadata": {},
   "source": [
    "## Application System Prompt\n",
    "\n",
    "This is the application system prompt, or the prompt to the LLM used to generate outputs.\n",
    "\n",
    "This is **NOT** the prompt we are optimizing! This simply generates outputs. \n",
    "\n",
    "We are optimizing the **evaluator prompt**, or the prompt for the LLM-as-judge eval which EVALUATES the generated outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78daecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT the prompt we are optimizing\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that assists in normalizing text. Your task is to normalize the text based on the following rules:\n",
    "\n",
    "Replace any email address with [EMAIL].\n",
    "Replace any Social Security–like number (###-##-####) with [SSN].\n",
    "If a person’s full name (two capitalized words) appears, replace with [NAME].\n",
    "Normalize percentages (e.g., 15%, 15 percent) → 0.15.\n",
    "If a number is over 1000, and contains commas, remove the commas (10,000 → 10000).\n",
    "Remove extra whitespace and replace with a single space.\n",
    "Replace slang like u → you, thx → thanks, pls → please.\n",
    "If a message contains banned words (e.g., refund, lawsuit, fraud), replace them with [FLAGGED].\n",
    "Append a compliance footer: -- Company Confidential -- at the end of every message.\n",
    "\n",
    "Here is the original text: {query}\n",
    "Return just the normalized text, no other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226089aa",
   "metadata": {},
   "source": [
    "## Output Generator\n",
    "Uses the application system prompt to generate outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62b665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(dataset, system_prompt):\n",
    "    model = OpenAIModel(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "    outputs = llm_generate(\n",
    "        dataframe=dataset,\n",
    "        template=system_prompt,\n",
    "        model=model,\n",
    "        concurrency=40,\n",
    "    )\n",
    "    return outputs[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0408e1e",
   "metadata": {},
   "source": [
    "## Sanitization Helpers\n",
    "\n",
    "clean and clean_series are used to normalize text before comparing generated outputs with ground truths. This prevents false mismatches caused by superficial formatting differences.\n",
    "\n",
    "For example, the string:\n",
    "\n",
    "```\"today's year is 1/1/2025\"```\n",
    "\n",
    "\n",
    "might be normalized to:\n",
    "\n",
    "```\"today's year is 2025-01-01\"```\n",
    "\n",
    "\n",
    "If we compare it against a ground truth like:\n",
    "\n",
    "```\"today’s year is 2025-01-01\"```\n",
    "\n",
    "\n",
    "a raw string comparison would incorrectly flag them as different because of the straight vs. curly apostrophe. Normalization ensures both strings are treated as equivalent, so the comparison is judged correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aba5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def clean(label):\n",
    "    if pd.isna(label):\n",
    "        return \"\"\n",
    "    label = str(label)\n",
    "    # map curly/smart quotes → straight quotes\n",
    "    smart_map = {\n",
    "        ord('‘'): \"'\",\n",
    "        ord('’'): \"'\",\n",
    "        ord('‚'): \"'\",\n",
    "        ord('‛'): \"'\",\n",
    "        ord('“'): '\"',\n",
    "        ord('”'): '\"',\n",
    "        ord('„'): '\"',\n",
    "        ord('‟'): '\"',\n",
    "    }\n",
    "    # normalize Unicode width/compatibility, apply smart_map, strip + lower\n",
    "    cleaned = unicodedata.normalize('NFKC', label).translate(smart_map)\n",
    "    return cleaned.strip().strip('\"').strip(\"'\").lower()\n",
    "\n",
    "clean_series = lambda s: s.astype(str).apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32448c97",
   "metadata": {},
   "source": [
    "## Accuracy Computation\n",
    "\n",
    "Computes accuracy, f1, precision, recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b6fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        y_pred = [clean(label) for label in y_pred]\n",
    "        y_true = [clean(label) for label in y_true]\n",
    "        return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae6995",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "This is the code for our LLM-as-Judge evaluator.\n",
    "\n",
    "It checks whether outputs are normalized properly.\n",
    "\n",
    "You can see the prompt below. THIS IS THE PROMPT WE ARE OPTIMIZING. \n",
    "\n",
    "We want to build evals that align with how we expect them to perform. Good evals are very important. They allow you to filter and classify the information you feed to your users. Because LLM outputs are not deterministic, you need something to check those outputs. It's too time consuminng to do this manually, so employing an LLM to evaluate these outputs is a common and essential practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from phoenix.evals.llm import LLM\n",
    "from phoenix.evals import ClassificationEvaluator, evaluate_dataframe, create_evaluator\n",
    "\n",
    "evaluator_prompt = \"\"\"\n",
    "\n",
    "You are a helpful assistant that evaluates normalized text. Your task is to evaluate whether the text redacts secure information and normalizes any numbers properly.\n",
    "If secure information is redacted and numbers are normalized properly, return \"correct\". Otherwise, return \"incorrect\".\n",
    "\n",
    "Here is the original text: {query}\n",
    "Here is the normalized text: {output}\n",
    "Return in JSON format with the following 2 fields:\n",
    "explanation: Brief explanation of why the normalized text is correct (normalized properly) or incorrect (normalized incorrectly), referencing the correct label if relevant.\n",
    "correctness: \"correct\" or \"incorrect\".\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_output(dataset, evaluator_prompt):\n",
    "    llm = LLM(provider=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "    evaluator = ClassificationEvaluator(\n",
    "        \"sanity_checker\",\n",
    "        llm,\n",
    "        evaluator_prompt,\n",
    "        choices={\n",
    "            \"correct\": 1,\n",
    "            \"incorrect\": 0\n",
    "        }\n",
    "    )\n",
    "    evaluation_results = evaluate_dataframe(\n",
    "        dataset,\n",
    "        [evaluator]\n",
    "    )[\"sanity_checker_score\"]\n",
    "\n",
    "    evaluation_results = pd.DataFrame(\n",
    "        evaluation_results.apply(json.loads).tolist(),\n",
    "        index=evaluation_results.index\n",
    "    )\n",
    "\n",
    "    return evaluation_results[\"label\"], evaluation_results[\"explanation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d2340",
   "metadata": {},
   "source": [
    "## Generate Output and Evaluate\n",
    "\n",
    "This combines our output generator and our evaluator into one function, and also computes accuracies for our outputs and also our evaluator.\n",
    "\n",
    "Evaluator accuracy is computed by comparing what the eval thinks (\"correct\" or \"incorrect\") versus whether the output is equal to the ground truth or not (actual \"correct\" or \"incorrect\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d3e50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output_and_evaluate(dataset, system_prompt, evaluator_prompt):\n",
    "    dataset[\"output\"] = generate_output(dataset, system_prompt)\n",
    "    dataset_accuracy = compute_accuracy(dataset[\"output\"], dataset[\"ground_truth\"])\n",
    "    dataset[\"eval_ground_truth\"] = [\"correct\" if clean(row[\"ground_truth\"]) == clean(row[\"output\"]) else \"incorrect\" for _, row in dataset.iterrows()]\n",
    "    dataset[\"eval\"], dataset[\"eval_explanation\"] = evaluate_output(dataset, evaluator_prompt)\n",
    "    dataset_evaluator_accuracy = compute_accuracy(dataset[\"eval\"], dataset[\"eval_ground_truth\"])\n",
    "    return dataset, dataset_accuracy, dataset_evaluator_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576920a3",
   "metadata": {},
   "source": [
    "### Run this below cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2176c0",
   "metadata": {},
   "source": [
    "## Helper Function - calling the Prompt Learning SDK\n",
    "\n",
    "You can see the `optimize_iteration` helper function here actually initializes the optimizer with feedback and produces a new, optimized prompt. \n",
    "\n",
    "**The next step is figuring out what feedback to provide to the optimizer in order for it to generate optimized prompts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c858d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "def test_evaluator_performance(train, test, system_prompt, evaluator_prompt):\n",
    "    train, train_accuracy, train_evaluator_accuracy = generate_output_and_evaluate(\n",
    "        train, system_prompt, evaluator_prompt\n",
    "    )\n",
    "    test, test_accuracy, test_evaluator_accuracy = generate_output_and_evaluate(\n",
    "        test, system_prompt, evaluator_prompt\n",
    "    )\n",
    "    return train, test, train_evaluator_accuracy, test_evaluator_accuracy\n",
    "\n",
    "def optimize_iteration(feedback_set, feedback_columns, evaluator_prompt):\n",
    "    optimizer = PromptLearningOptimizer(\n",
    "        prompt=evaluator_prompt,\n",
    "        model_choice=\"o3-2025-04-16\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "\n",
    "    evaluator_prompt = optimizer.optimize(\n",
    "        feedback_set,\n",
    "        \"eval\",\n",
    "        feedback_columns=feedback_columns,\n",
    "        context_size_k=90000,\n",
    "    )\n",
    "    return evaluator_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df9056",
   "metadata": {},
   "source": [
    "# 🔄 Optimization Workflow using Prompt Learning\n",
    "\n",
    "This notebook implements an **interactive optimization loop** where we:\n",
    "1. **Collect Feedback** — Display examples from the dataset, and let the user label correctness/explanations.\n",
    "2. **Optimize Prompt** — Use the feedback to generate an updated evaluator prompt.\n",
    "3. **Review & Confirm** — Show the optimized prompt, allow manual edits for formatting, and confirm it.\n",
    "4. **Evaluate** — Re-run the evaluator with the new prompt on train/test sets, log metrics, and save results.\n",
    "5. **Loop** — Repeat the cycle for `N` rounds, carrying forward the updated evaluator prompt and re-evaluated outputs.\n",
    "\n",
    "The feedback we provide to the Prompt Learning optimizer is HUMAN ANNOTATED FEEDBACK. We show the power of just needing to annotate 5 examples per loop, and seeing optimization boosts! This shows the data efficiency of Prompt Learning. Rather than RL or an programmatic optimizer, where you need lots of data to make effective accuracy boosts, just hand annotating 5 outputs and giving that feedback to Prompt Learning allows for huge boosts in accuracy.\n",
    "\n",
    "The workflow is composed of modular helper functions:\n",
    "\n",
    "- `collect_feedback_ui`: interactive widget interface for gathering manual feedback.  \n",
    "- `review_and_confirm_prompt`: UI for reviewing and editing the optimized prompt before saving.  \n",
    "- `run_one_round`: runs a single loop round (feedback → optimize → confirm → evaluate).  \n",
    "- `interactive_optimization_loop`: orchestrates the full multi-round optimization process.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bbbf45",
   "metadata": {},
   "source": [
    "## 📝 `collect_feedback_ui`\n",
    "\n",
    "This function creates an **interactive feedback form** using `ipywidgets`:\n",
    "\n",
    "- Displays a sample of `query`, `ground_truth`, `output`, and evaluator outputs.\n",
    "- Provides dropdowns / textareas for feedback fields (`evaluator_correctness`, `evaluator_explanation`).\n",
    "- Saves the annotated feedback set (`feedback_set`) to CSV.\n",
    "- Calls `on_save(feedback_set)` after the user clicks **Save Feedback**, triggering the next step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def collect_feedback_ui(feedback_set, feedback_columns, on_save):\n",
    "    \"\"\"\n",
    "    Display feedback UI for a batch of examples.\n",
    "    Calls `on_save(feedback_set)` when user saves.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_widgets = []\n",
    "\n",
    "    display(HTML(\"<h3>Manual Feedback Collection</h3>\"))\n",
    "    for idx, row in feedback_set.iterrows():\n",
    "        example_html = f\"\"\"\n",
    "        <div style=\"border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "            <h4>Example {idx + 1}</h4>\n",
    "            <p><b>Query:</b><br>{row['query']}</p>\n",
    "            <p><b>Ground Truth:</b><br>{row['ground_truth']}</p>\n",
    "            <p><b>Generated Output:</b><br>{row['output']}</p>\n",
    "            <p><b>Evaluation:</b><br>{row['eval']}</p>\n",
    "            <p><b>Evaluation Explanation:</b><br>{row['eval_explanation']}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        display(widgets.HTML(value=example_html))\n",
    "\n",
    "        feedback_widgets = {}\n",
    "        for col in feedback_columns:\n",
    "            if \"correctness\" in col:\n",
    "                widget = widgets.Dropdown(\n",
    "                    options=['correct', 'incorrect'],\n",
    "                    description=col,\n",
    "                    style={'description_width': '150px'},\n",
    "                    layout=widgets.Layout(width='300px')\n",
    "                )\n",
    "            else:\n",
    "                widget = widgets.Textarea(\n",
    "                    description=col,\n",
    "                    placeholder=f'Enter {col}...',\n",
    "                    style={'description_width': '150px'},\n",
    "                    layout=widgets.Layout(width='500px', height='80px')\n",
    "                )\n",
    "            display(widget)\n",
    "            feedback_widgets[col] = widget\n",
    "\n",
    "        all_widgets.append((idx, feedback_widgets))\n",
    "        display(HTML(\"<hr>\"))\n",
    "\n",
    "    save_btn = widgets.Button(description=\"💾 Save Feedback\", button_style=\"success\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def _save(_):\n",
    "        for idx, widgets_dict in all_widgets:\n",
    "            for col, widget in widgets_dict.items():\n",
    "                feedback_set.loc[idx, col] = widget.value\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(\"✅ Feedback saved.\")\n",
    "        on_save(feedback_set)\n",
    "\n",
    "    save_btn.on_click(_save)\n",
    "    display(save_btn, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc336ab",
   "metadata": {},
   "source": [
    "## 🔍 `review_and_confirm_prompt`\n",
    "\n",
    "This function displays the **auto-optimized evaluator prompt**:\n",
    "\n",
    "- Shows the generated prompt in a styled block.  \n",
    "- Provides a large text area for manual edits (to fix formatting, braces, JSON requirements, etc.).  \n",
    "- Only after the user clicks **Confirm Prompt** does it call `on_confirm(edited_prompt)`.  \n",
    "- Ensures the downstream evaluation always uses a user-validated prompt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e08bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "def review_and_confirm_prompt(initial_prompt, set_callback):\n",
    "    \"\"\"\n",
    "    Display the optimized prompt, allow user to edit it, \n",
    "    and confirm before passing it to the set_callback.\n",
    "    \"\"\"\n",
    "    display(HTML(f\"<h4>Auto-Optimized Prompt</h4><pre>{initial_prompt}</pre>\"))\n",
    "    display(HTML(\n",
    "        \"<p><b>✍️ Please review the optimized prompt below.</b><br>\"\n",
    "        \"Ensure there are no stray braces or brackets around anything other than {query} and {output}.<br>\"\n",
    "        \"Ensure that the output format instructions contains JSON (required for openai models)\"\n",
    "        \"Make any edits to the prompt that you like!\"\n",
    "    ))\n",
    "\n",
    "    prompt_editor = widgets.Textarea(\n",
    "        value=initial_prompt,\n",
    "        placeholder=\"Edit the prompt if needed...\",\n",
    "        layout=widgets.Layout(width='800px', height='200px')\n",
    "    )\n",
    "\n",
    "    confirm_button = widgets.Button(description=\"✅ Confirm Prompt\", button_style=\"success\")\n",
    "    editor_output = widgets.Output()\n",
    "\n",
    "    def confirm_prompt(b):\n",
    "        with editor_output:\n",
    "            clear_output()\n",
    "            print(\"📌 Prompt confirmed and saved for next round.\")\n",
    "        set_callback(prompt_editor.value)\n",
    "\n",
    "    confirm_button.on_click(confirm_prompt)\n",
    "    display(prompt_editor, confirm_button, editor_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba892638",
   "metadata": {},
   "source": [
    "## 🔁 `run_one_round`\n",
    "\n",
    "Runs a **single optimization cycle**:\n",
    "\n",
    "1. Samples a batch of examples from the dataset for feedback.  \n",
    "2. Calls `collect_feedback_ui` to gather manual corrections.  \n",
    "3. Optimizes the evaluator prompt using `optimize_iteration`.  \n",
    "4. Calls `review_and_confirm_prompt` to display and edit the new prompt.  \n",
    "5. After confirmation:\n",
    "   - Saves the new prompt to file.  \n",
    "   - Re-evaluates train/test with the updated prompt.  \n",
    "   - Logs metrics and appends results.  \n",
    "   - Starts the next round (if any).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def run_one_round(train, test, feedback_columns, current_prompt, loop_idx, total_loops, results, continue_callback):\n",
    "    \"\"\"\n",
    "    Runs one interactive round: feedback → optimize → review → evaluate.\n",
    "    Calls `continue_callback(updated_prompt)` when finished (to start next round).\n",
    "    \"\"\"\n",
    "    # Select feedback set\n",
    "    feedback_set = train[\n",
    "        ((train[\"eval\"] == \"correct\") & \n",
    "         (clean_series(train[\"ground_truth\"]) != clean_series(train[\"output\"]))) |\n",
    "        ((train[\"eval\"] == \"incorrect\") & \n",
    "         (clean_series(train[\"ground_truth\"]) == clean_series(train[\"output\"])))\n",
    "    ].sample(5).copy()\n",
    "\n",
    "    for col in feedback_columns:\n",
    "        feedback_set[col] = None\n",
    "\n",
    "    def after_feedback(filled_feedback):\n",
    "        display(HTML(f\"<p style='color:green;'>✅ Feedback collected for round {loop_idx}. Running optimization...</p>\"))\n",
    "        auto_prompt = optimize_iteration(filled_feedback, feedback_columns, current_prompt)\n",
    "        auto_prompt = auto_prompt.replace('{\\n', '').replace('\\n}', '')\n",
    "\n",
    "        def after_confirm(edited_prompt):\n",
    "            nonlocal train, test\n",
    "            results[\"prompts\"].append(edited_prompt)\n",
    "\n",
    "            train, test, train_acc, test_acc = test_evaluator_performance(train, test, system_prompt, edited_prompt)\n",
    "            results[\"metrics\"].append({\n",
    "                \"round\": loop_idx,\n",
    "                \"train_accuracy\": train_acc,\n",
    "                \"test_accuracy\": test_acc\n",
    "            })\n",
    "            pd.DataFrame(results[\"metrics\"]).to_csv(\"all_metrics.csv\", index=False)\n",
    "\n",
    "            display(HTML(f\"\"\"\n",
    "            <div style=\"border:1px solid #ddd; padding:10px; margin:10px 0; border-radius:5px; background:#f9f9f9;\">\n",
    "                <b>Round {loop_idx} complete</b><br>\n",
    "                ✅ Train Accuracy: {train_acc:.3f}<br>\n",
    "                ✅ Test Accuracy: {test_acc:.3f}\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "\n",
    "            if loop_idx < total_loops:\n",
    "                display(HTML(\"<p style='color:blue;'>➡️ Starting next round...</p>\"))\n",
    "                continue_callback(edited_prompt)\n",
    "            else:\n",
    "                display(HTML(\"<p style='color:green;'>🎉 Optimization loop finished!</p>\"))\n",
    "\n",
    "        review_and_confirm_prompt(auto_prompt, after_confirm)\n",
    "\n",
    "    collect_feedback_ui(feedback_set, feedback_columns, after_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86921ae",
   "metadata": {},
   "source": [
    "## 🚀 `interactive_optimization_loop`\n",
    "\n",
    "The **master orchestrator** of the workflow:\n",
    "\n",
    "- Computes baseline evaluator performance with the initial prompt.  \n",
    "- Logs round 0 metrics.  \n",
    "- Iteratively calls `run_one_round` for the specified number of loops.  \n",
    "- Maintains a record of:\n",
    "  - All prompts across rounds (`results[\"prompts\"]`)  \n",
    "  - Evaluation metrics (`results[\"metrics\"]`)  \n",
    "- Saves metrics history to `all_metrics.csv`.  \n",
    "- Stops after `N` confirmed rounds of optimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_optimization_loop(train, test, feedback_columns, evaluator_prompt, loops=5):\n",
    "    \"\"\"\n",
    "    Master function orchestrating N interactive optimization rounds.\n",
    "    \"\"\"\n",
    "    # Baseline evaluation\n",
    "    train, test, train_acc, test_acc = test_evaluator_performance(train, test, system_prompt, evaluator_prompt)\n",
    "    print(\"Baseline Train Accuracy:\", train_acc)\n",
    "    print(\"Baseline Test Accuracy:\", test_acc)\n",
    "\n",
    "    results = {\n",
    "        \"prompts\": [evaluator_prompt],\n",
    "        \"metrics\": [{\n",
    "            \"round\": 0,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"test_accuracy\": test_acc\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    def next_round(prompt, loop_idx=1):\n",
    "        run_one_round(train, test, feedback_columns, prompt, loop_idx, loops, results,\n",
    "                      lambda updated_prompt: next_round(updated_prompt, loop_idx + 1))\n",
    "\n",
    "    next_round(evaluator_prompt, 1)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72db1f",
   "metadata": {},
   "source": [
    "# Run the Prompt Learning Loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_columns = [\"evaluator_correctness\", \"evaluator_explanation\"]\n",
    "\n",
    "results = interactive_optimization_loop(train, test, feedback_columns, evaluator_prompt, loops=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2bd0c",
   "metadata": {},
   "source": [
    "# View your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(results[\"metrics\"])\n",
    "\n",
    "# Find index of max test accuracy\n",
    "best_idx = metrics_df[\"test_accuracy\"].idxmax()\n",
    "best_round = metrics_df.loc[best_idx, \"round\"]\n",
    "best_acc = metrics_df.loc[best_idx, \"test_accuracy\"]\n",
    "\n",
    "# Get corresponding prompt\n",
    "best_prompt = results[\"prompts\"][best_round]\n",
    "\n",
    "# Display the best prompt\n",
    "print(\"Best Prompt:\")\n",
    "print(best_prompt)\n",
    "\n",
    "print(\"Best Accuracy: \", best_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
