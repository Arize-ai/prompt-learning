{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166be50e",
   "metadata": {},
   "source": [
    "# Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/phx.jpeg\" width=\"800\">\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in datasets/support_queries.csv.\n",
    "\n",
    "**Note: This notebook `phoenix_support_query_classification.ipynb` complements `support_query_classification.ipynb` by using Phoenix datasets, experiments, and prompt management for Prompt Learning. It's a more end to end way for you to visualize your iterative prompt improvement and see how it performs on train/test sets, and also leverages Phoenix methods for advanced features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb715e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arize-phoenix openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a2b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "openai_client = AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3bec",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b707d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# If you're self-hosting Phoenix, change this value:\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = getpass.getpass('Phoenix Collector Endpoint:')\n",
    "\n",
    "PHOENIX_API_KEY = getpass.getpass('Phoenix API Key:')\n",
    "os.environ[\"PHOENIX_API_KEY\"] = PHOENIX_API_KEY\n",
    "\n",
    "from phoenix.client import Client\n",
    "phoenix_client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463fac",
   "metadata": {},
   "source": [
    "## **Make train/test sets**\n",
    "\n",
    "We use an 80/20 train/test split to train our prompt. The optimizer will use the training set to visualize and analyze its errors and successes, and make prompt updates based on these results. We will then test on the test set to see how that prompt performs on unseen data. \n",
    "\n",
    "We will be exporting these datasets to Phoenix. In Phoenix you will be able to view the experiments we run on the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "\n",
    "train_set = data.sample(frac=0.7, random_state=42)\n",
    "test_set = data.drop(train_set.index)\n",
    "\n",
    "train_dataset = phoenix_client.datasets.create_dataset(\n",
    "        name=\"training_data_support_query_classification_2\",\n",
    "        dataframe=train_set,\n",
    "        input_keys=['query'],\n",
    "        output_keys=['ground_truth'],\n",
    "    )\n",
    "\n",
    "test_dataset = phoenix_client.datasets.create_dataset(\n",
    "        name=\"test_data_support_query_classification_2\",\n",
    "        dataframe=test_set,\n",
    "        input_keys=['query'],\n",
    "        output_keys=['ground_truth'],\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3184e0",
   "metadata": {},
   "source": [
    "## **Base Prompt for Optimization**\n",
    "\n",
    "This is our base prompt - our 0th iteration. This is the prompt we will be optimizing for our task.\n",
    "\n",
    "We also upload our prompt to Phoenix. Phoenix Prompt Hub serves as a repository for your prompts. You will be able to view all iterations of your prompt as its optimized, along with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Account Creation\n",
    "Login Issues\n",
    "Password Reset\n",
    "Two-Factor Authentication\n",
    "Profile Updates\n",
    "Billing Inquiry\n",
    "Refund Request\n",
    "Subscription Upgrade/Downgrade\n",
    "Payment Method Update\n",
    "Invoice Request\n",
    "Order Status\n",
    "Shipping Delay\n",
    "Product Return\n",
    "Warranty Claim\n",
    "Technical Bug Report\n",
    "Feature Request\n",
    "Integration Help\n",
    "Data Export\n",
    "Security Concern\n",
    "Terms of Service Question\n",
    "Privacy Policy Question\n",
    "Compliance Inquiry\n",
    "Accessibility Support\n",
    "Language Support\n",
    "Mobile App Issue\n",
    "Desktop App Issue\n",
    "Email Notifications\n",
    "Marketing Preferences\n",
    "Beta Program Enrollment\n",
    "General Feedback\n",
    "\n",
    "Return just the category, no other text for the support query.\n",
    "\"\"\"\n",
    "\n",
    "def upload_prompt_phoenix(system_prompt, name, iteration, prompt_versions, train_metric, test_metric):\n",
    "    prompt_version = PromptVersion(\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}],  # System message\n",
    "        model_name=\"gpt-4o-mini-2024-07-18\",  # Model being used\n",
    "        description=\"Prompt for support query classification\",\n",
    "        model_provider=\"OPENAI\"\n",
    "    )\n",
    "\n",
    "    # Create prompt in Phoenix\n",
    "    initial_prompt_version = phoenix_client.prompts.create(\n",
    "        name=name,\n",
    "        version=prompt_version,\n",
    "    )\n",
    "\n",
    "    prompt_versions.append({\n",
    "        \"iteration\": iteration,\n",
    "        \"prompt\": system_prompt,\n",
    "        \"phoenix_id\": initial_prompt_version.id if hasattr(initial_prompt_version, 'id') else None,\n",
    "        \"train_metric\": train_metric,\n",
    "        \"test_metric\": test_metric\n",
    "    })\n",
    "    return prompt_versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21e149",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task(system_prompt):\n",
    "\n",
    "    async def output_task(input):\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": f\"query: {input.get('query')}\"}],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return output_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(label):\n",
    "        return label.strip().strip('\"').strip(\"'\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886cda",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1be0f",
   "metadata": {},
   "source": [
    "Our evaluator leverages llm_generate once again to build these llm evals with concurrency. We use an output parser to ensure that our eval is returned in proper json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import create_evaluator\n",
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"correctness\": {\"type\": \"string\", \"enum\": [\"correct\", \"incorrect\"]},\n",
    "        \"explanation\": {\"type\": \"string\"},\n",
    "        \"confusion_reason\": {\"type\": \"string\"},\n",
    "        \"error_type\": {\"type\": \"string\"},\n",
    "        \"evidence_span\": {\"type\": \"string\"},\n",
    "        \"prompt_fix_suggestion\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"correctness\",\n",
    "        \"explanation\",\n",
    "        \"confusion_reason\",\n",
    "        \"error_type\",\n",
    "        \"evidence_span\",\n",
    "        \"prompt_fix_suggestion\",\n",
    "    ],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "@create_evaluator(name=\"output_evaluator\", source=\"llm\")\n",
    "def output_evaluator(query: str, ground_truth: str, output: str):\n",
    "    with open(\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        template = file.read()\n",
    "\n",
    "    prompt = (\n",
    "        template.replace(\"{query}\", query)\n",
    "            .replace(\"{ground_truth}\", ground_truth)\n",
    "            .replace(\"{output}\", output)\n",
    "    )\n",
    "    obj = llm.generate_object(prompt=prompt, schema=SCHEMA)\n",
    "    correctness = obj[\"correctness\"]\n",
    "    score = 1.0 if correctness == \"correct\" else 0.0\n",
    "    explanation = (\n",
    "        f'correctness: {correctness}; '\n",
    "        f'explanation: {obj.get(\"explanation\",\"\")}; '\n",
    "        f'confusion_reason: {obj.get(\"confusion_reason\",\"\")}; '\n",
    "        f'error_type: {obj.get(\"error_type\",\"\")}; '\n",
    "        f'evidence_span: {obj.get(\"evidence_span\",\"\")}; '\n",
    "        f'prompt_fix_suggestion: {obj.get(\"prompt_fix_suggestion\",\"\")};'\n",
    "    )\n",
    "    return {\"score\": score, \"label\": correctness, \"explanation\": explanation}\n",
    "\n",
    "async def test_evaluator(expected, output):\n",
    "    return normalize(expected.get(\"ground_truth\")) == normalize(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ca3",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import requests\n",
    "def compute_metric(experiment, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested classification metric from a Phoenix experiment.\n",
    "\n",
    "    Args:\n",
    "        experiment: an object with an .id field (Phoenix Experiment).\n",
    "        scorer (str): one of \"accuracy\", \"f1\", \"precision\", \"recall\".\n",
    "        average (str): averaging method for multi-class classification.\n",
    "    \n",
    "    Returns:\n",
    "        float: computed metric value.\n",
    "    \"\"\"\n",
    "    print(experiment)\n",
    "    experiment_id = experiment[\"experiment_id\"]\n",
    "    url = f\"{os.environ['PHOENIX_COLLECTOR_ENDPOINT']}/v1/experiments/{experiment_id}/json\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['PHOENIX_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch experiment data: {response.status_code} {response.text}\")\n",
    "\n",
    "    results = response.json()\n",
    "    \n",
    "    y_true = [normalize(entry[\"reference_output\"][\"ground_truth\"]) for entry in results]\n",
    "    y_pred = [normalize(entry[\"output\"]) for entry in results]\n",
    "\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9be50",
   "metadata": {},
   "source": [
    "## Experiment Processor\n",
    "\n",
    "This function pulls a Phoenix experiment and loads the data into a pandas dataframe so it can run through the optimizer.\n",
    "\n",
    "Specifically it:\n",
    "- Pulls the experiment data from Phoenix\n",
    "- Adds the input column to the dataframe\n",
    "- Adds the evals to the dataframe\n",
    "- Adds the output to the dataframe\n",
    "- Returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def process_experiment(experiment, train_set, input_column_name, output_column_name,feedback_columns = None):\n",
    "    \"\"\"\n",
    "    Update existing columns in `train_set` with feedback from experiment annotations.\n",
    "\n",
    "    Args:\n",
    "        experiment_json (list): JSON data from experiment.\n",
    "        train_set (pd.DataFrame): DataFrame that already contains the feedback columns.\n",
    "        feedback_columns (list): List of feedback field names to update.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with values filled in from experiment annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_id = experiment[\"experiment_id\"]\n",
    "    url = f\"{os.environ['PHOENIX_COLLECTOR_ENDPOINT']}/v1/experiments/{experiment_id}/json\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['PHOENIX_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch experiment data: {response.status_code} {response.text}\")\n",
    "\n",
    "    results = response.json()\n",
    "\n",
    "    train_set[\"ground_truth\"] = [None] * len(train_set)\n",
    "    if feedback_columns:\n",
    "        for col in feedback_columns:\n",
    "            train_set[col] = [None] * len(train_set)\n",
    "\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "    for i, entry in enumerate(results):\n",
    "        eval_output = entry[\"annotations\"][0][\"explanation\"]\n",
    "        train_set.loc[i, \"ground_truth\"] = entry[\"reference_output\"][\"ground_truth\"]\n",
    "        if feedback_columns:\n",
    "            for item in eval_output.split(\";\"):\n",
    "                key_value = item.split(\":\")\n",
    "                if key_value[0].strip() in feedback_columns:\n",
    "                    key, value = key_value[0].strip(), key_value[1].strip()\n",
    "                    train_set.loc[i, key] = value\n",
    "\n",
    "    if \"output\" in train_set.columns:\n",
    "        train_set.rename(columns={\"output\": \"ground_truth\"}, inplace=True)\n",
    "\n",
    "    train_set[output_column_name] = [entry.get(\"output\") for entry in results]\n",
    "\n",
    "    train_set[input_column_name] = [entry.get(\"input\") for entry in results]\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93dace",
   "metadata": {},
   "source": [
    "# Prompt Optimization Loop with Phoenix Experiments\n",
    "\n",
    "This code implements an iterative prompt optimization system that uses **Phoenix experiments** to evaluate and improve prompts based on feedback from LLM evaluators.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `optimize_loop` function automates prompt engineering by:\n",
    "\n",
    "- Evaluating prompts using Phoenix experiments  \n",
    "- Collecting detailed feedback from LLM evaluators  \n",
    "- Optimizing prompts via a learning-based optimizer  \n",
    "- Iterating until the performance threshold is met or the loop limit is reached  \n",
    "\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "Each of these numbers are added as comments in the code.\n",
    "\n",
    "### 1. Initialization\n",
    "\n",
    "- Set up tracking variables:\n",
    "  - `train_metrics`, `test_metrics`, `raw_dfs` for storing evaluation results\n",
    "- Convert training dataset to a DataFrame for easy updates\n",
    "\n",
    "### 2. Baseline Evaluation\n",
    "\n",
    "- Run an initial experiment using the **test set**\n",
    "- Establish a **baseline metric** (e.g., accuracy, F1) to compare against future improvements\n",
    "\n",
    "### 3. Early Exit Check\n",
    "\n",
    "- If the **initial prompt already meets the performance threshold**, skip further optimization to save time and compute\n",
    "\n",
    "### 4. Main Optimization Loop\n",
    "\n",
    "For each iteration (up to `loops`):\n",
    "\n",
    "#### 4a. Run Training Experiment\n",
    "\n",
    "- Execute the current prompt on the **training set**\n",
    "- Use LLM evaluators to generate **natural language feedback**\n",
    "\n",
    "#### 4b. Process Feedback\n",
    "\n",
    "- Extract structured information from evaluator outputs:\n",
    "  - Correctness\n",
    "  - Explanation\n",
    "  - Confusion reason\n",
    "  - Error type\n",
    "  - Prompt fix suggestions\n",
    "- Update the training DataFrame with this feedback\n",
    "\n",
    "#### 4c. Generate Learning Annotations\n",
    "\n",
    "- Convert feedback into structured annotations for the optimizer to learn from\n",
    "- This allows learning from evaluator insights in a consistent format\n",
    "\n",
    "#### 4d. Optimize the Prompt\n",
    "\n",
    "- Pass feedback to the **PromptLearningOptimizer**\n",
    "- Generate an **improved prompt** that attempts to correct issues found in the previous iteration\n",
    "\n",
    "#### 4e. Evaluate on Test Set\n",
    "\n",
    "- Evaluate the updated prompt on the **held-out test set**\n",
    "- Assess **generalization** beyond the training data\n",
    "\n",
    "#### 4f. Track Metrics\n",
    "\n",
    "- Log metrics for:\n",
    "  - Training set performance\n",
    "  - Test set performance\n",
    "- Store raw results for further analysis or visualization\n",
    "\n",
    "#### 4g. Convergence Check\n",
    "\n",
    "- If the new prompt's test metric **meets or exceeds the threshold**, exit the loop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "from phoenix.client.experiments import async_run_experiment\n",
    "import copy\n",
    "import asyncio\n",
    "\n",
    "prompt_name = \"support_query_classification\"\n",
    "\n",
    "async def optimize_loop(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    system_prompt,\n",
    "    evaluators,\n",
    "    feedback_columns,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "    prompt_versions=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    \"\"\"\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    raw_dfs = []\n",
    "    train_df = train_dataset.to_dataframe()\n",
    "\n",
    "    print(f\"ðŸš€ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"ï¿½ï¿½ Initial evaluation:\")\n",
    "\n",
    "    task = generate_task(system_prompt)\n",
    "\n",
    "    initial_experiment = await async_run_experiment(\n",
    "        dataset=test_dataset,\n",
    "        task=task,\n",
    "        evaluators=[test_evaluator]\n",
    "    )\n",
    "\n",
    "    initial_metric_value = compute_metric(initial_experiment, scorer)\n",
    "    print(f\"âœ… Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"ðŸŽ‰ Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompt_versions,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    prompt_versions = upload_prompt_phoenix(system_prompt, prompt_name, 0, [], None, initial_metric_value)\n",
    "\n",
    "    # Initialize all feedback columns\n",
    "\n",
    "    while loops > 0:\n",
    "        print(f\"ðŸ“Š Loop {curr_loop}: Optimizing prompt...\")\n",
    "        \n",
    "        task = generate_task(system_prompt)\n",
    "\n",
    "        train_experiment = await async_run_experiment(\n",
    "            dataset=train_dataset,\n",
    "            task=task,\n",
    "            evaluators=evaluators,\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        train_df = process_experiment(train_experiment, train_set, \"query\", \"output\", feedback_columns)\n",
    "\n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,\n",
    "            model_choice=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_df,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_df,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "        train_metric_post_value = compute_metric(train_experiment, scorer)\n",
    "        train_metrics.append(train_metric_post_value)\n",
    "\n",
    "        test_experiment = await async_run_experiment(\n",
    "            dataset=test_dataset,\n",
    "            task=generate_task(system_prompt),\n",
    "            evaluators=[test_evaluator]\n",
    "        )\n",
    "        test_metric_post_value = compute_metric(test_experiment, scorer)\n",
    "        test_metrics.append(test_metric_post_value)\n",
    "\n",
    "        print(f\"âœ… Train {scorer}: {train_metric_post_value}\")\n",
    "        print(f\"âœ… Test {scorer}: {test_metric_post_value}\")\n",
    "\n",
    "        prompt_versions = upload_prompt_phoenix(system_prompt, prompt_name, curr_loop, prompt_versions, train_metric_post_value, test_metric_post_value)\n",
    "\n",
    "        if test_metric_post_value >= threshold:\n",
    "            print(\"ðŸŽ‰ Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"prompt\": prompt_versions,\n",
    "        \"raw\": raw_dfs\n",
    "    }\n",
    "\n",
    "# Main execution - use asyncio.run() to run the async function\n",
    "evaluators = [output_evaluator]\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "result = asyncio.run(optimize_loop(train_dataset, test_dataset, system_prompt, evaluators, feedback_columns, loops=5, scorer=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7627",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd42533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompt\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\nðŸ” Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"ðŸ§ª Initial Test Accuracy: {initial_test_acc}\")\n",
    "print(f\"ðŸ§ª Optimized Test Accuracy: {best_test_acc} (Î” {best_test_acc - initial_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
