{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166be50e",
   "metadata": {},
   "source": [
    "# Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/phx.jpeg\" width=\"800\">\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in datasets/support_queries.csv.\n",
    "\n",
    "**Note: This notebook `phoenix_support_query_classification.ipynb` complements `support_query_classification.ipynb` by using Phoenix datasets, experiments, and prompt management for Prompt Learning. It's a more end to end way for you to visualize your iterative prompt improvement and see how it performs on train/test sets, and also leverages Phoenix methods for advanced features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74a2b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee7ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY'] or getpass.getpass('OpenAI API Key:')\n",
    "client = AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d31f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3bec",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b707d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from phoenix.client import Client\n",
    "# If you're self-hosting Phoenix, change this value:\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = os.environ['PHOENIX_COLLECTOR_ENDPOINT'] or getpass.getpass('Phoenix Collector Endpoint:')\n",
    "\n",
    "PHOENIX_API_KEY = os.environ['PHOENIX_API_KEY'] or getpass.getpass('Phoenix API Key:')\n",
    "os.environ[\"PHOENIX_API_KEY\"] = PHOENIX_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463fac",
   "metadata": {},
   "source": [
    "## **Make train/test sets**\n",
    "\n",
    "We use an 80/20 train/test split to train our prompt. The optimizer will use the training set to visualize and analyze its errors and successes, and make prompt updates based on these results. We will then test on the test set to see how that prompt performs on unseen data. \n",
    "\n",
    "We will be exporting these datasets to Phoenix. In Phoenix you will be able to view the experiments we run on the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246OTI=\n",
      "üì§ Uploading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Examples uploaded: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246OTM=\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "\n",
    "train_set = data.sample(frac=0.8, random_state=42)\n",
    "test_set = data.drop(train_set.index)\n",
    "\n",
    "train_dataset = px.Client().upload_dataset(\n",
    "        dataset_name=\"training_data_\",\n",
    "        dataframe=train_set,\n",
    "        input_keys=['query'],\n",
    "        output_keys=['ground_truth'],\n",
    "    )\n",
    "\n",
    "test_dataset = px.Client().upload_dataset(\n",
    "        dataset_name=\"test_data_\",\n",
    "        dataframe=test_set,\n",
    "        input_keys=['query'],\n",
    "        output_keys=['ground_truth'],\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3184e0",
   "metadata": {},
   "source": [
    "## **Base Prompt for Optimization**\n",
    "\n",
    "This is our base prompt - our 0th iteration. This is the prompt we will be optimizing for our task.\n",
    "\n",
    "We also upload our prompt to Phoenix. Phoenix Prompt Hub serves as a repository for your prompts. You will be able to view all iterations of your prompt as its optimized, along with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "support query: {query}\n",
    "Account Creation\n",
    "Login Issues\n",
    "Password Reset\n",
    "Two-Factor Authentication\n",
    "Profile Updates\n",
    "Billing Inquiry\n",
    "Refund Request\n",
    "Subscription Upgrade/Downgrade\n",
    "Payment Method Update\n",
    "Invoice Request\n",
    "Order Status\n",
    "Shipping Delay\n",
    "Product Return\n",
    "Warranty Claim\n",
    "Technical Bug Report\n",
    "Feature Request\n",
    "Integration Help\n",
    "Data Export\n",
    "Security Concern\n",
    "Terms of Service Question\n",
    "Privacy Policy Question\n",
    "Compliance Inquiry\n",
    "Accessibility Support\n",
    "Language Support\n",
    "Mobile App Issue\n",
    "Desktop App Issue\n",
    "Email Notifications\n",
    "Marketing Preferences\n",
    "Beta Program Enrollment\n",
    "General Feedback\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\"\n",
    "\n",
    "def upload_prompt_phoenix(system_prompt, name, iteration, prompt_versions, train_metric, test_metric):\n",
    "    prompt_version = PromptVersion(\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}],  # System message\n",
    "        model_name=\"gpt-3.5-turbo\",  # Model being used\n",
    "        description=\"Prompt for JSON webpage generation\",\n",
    "        model_provider=\"OPENAI\"\n",
    "    )\n",
    "\n",
    "    # Create prompt in Phoenix\n",
    "    initial_prompt_version = Client().prompts.create(\n",
    "        name=name,\n",
    "        version=prompt_version,\n",
    "    )\n",
    "\n",
    "    prompt_versions.append({\n",
    "        \"iteration\": iteration,\n",
    "        \"prompt\": system_prompt,\n",
    "        \"phoenix_id\": initial_prompt_version.id if hasattr(initial_prompt_version, 'id') else None,\n",
    "        \"train_metric\": train_metric,\n",
    "        \"test_metric\": test_metric\n",
    "    })\n",
    "    return prompt_versions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21e149",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "691d829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task(system_prompt):\n",
    "\n",
    "    async def output_task(input):\n",
    "        formatted_prompt = system_prompt.replace(\"{query}\", input.get(\"query\"))\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return output_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886cda",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1be0f",
   "metadata": {},
   "source": [
    "Our evaluator leverages llm_generate once again to build these llm evals with concurrency. We use an output parser to ensure that our eval is returned in proper json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from phoenix.experiments.types import EvaluationResult\n",
    "\n",
    "def find_attributes(output):\n",
    "    patterns = {\n",
    "        \"correctness\": r'\"correctness\":\\s*\"([^\"]*)\"',\n",
    "        \"explanation\": r'\"explanation\":\\s*\"([^\"]*)\"',\n",
    "        \"confusion_reason\": r'\"confusion_reason\":\\s*\"([^\"]*)\"',\n",
    "        \"error_type\": r'\"error_type\":\\s*\"([^\"]*)\"',\n",
    "        \"evidence_span\": r'\"evidence_span\":\\s*\"([^\"]*)\"',\n",
    "        \"prompt_fix_suggestion\": r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    }\n",
    "\n",
    "    return tuple(\n",
    "        (match := re.search(pattern, output, re.IGNORECASE)) and match.group(1)\n",
    "        for pattern in patterns.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_parser(response: str) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "\n",
    "async def output_evaluator(input, expected, output):\n",
    "    with open(\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{query}\", input.get(\"query\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{ground_truth}\", expected.get(\"ground_truth\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{output}\", output)\n",
    "\n",
    "    eval_result = await client.chat.completions.create(\n",
    "        model=\"gpt-5-2025-08-07\",\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluator_prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    response = eval_result.choices[0].message.content\n",
    "    parsed_eval_result = eval_parser(response)\n",
    "    explanation=f\"\"\"correctness: {parsed_eval_result.get(\"correctness\", \"\")};\n",
    "        explanation: {parsed_eval_result.get(\"explanation\", \"\")};\n",
    "        confusion_reason: {parsed_eval_result.get(\"confusion_reason\", \"\")};\n",
    "        error_type: {parsed_eval_result.get(\"error_type\", \"\")};\n",
    "        evidence_span: {parsed_eval_result.get(\"evidence_span\", \"\")};\n",
    "        prompt_fix_suggestion: {parsed_eval_result.get(\"prompt_fix_suggestion\", \"\")};\"\"\"\n",
    "\n",
    "    return EvaluationResult(\n",
    "        score=float(parsed_eval_result.get(\"correctness\") == \"correct\"),\n",
    "        label=parsed_eval_result.get(\"correctness\", \"\"),\n",
    "        explanation=explanation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ca3",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba30cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import requests\n",
    "def compute_metric(experiment, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested classification metric from a Phoenix experiment.\n",
    "\n",
    "    Args:\n",
    "        experiment: an object with an .id field (Phoenix Experiment).\n",
    "        scorer (str): one of \"accuracy\", \"f1\", \"precision\", \"recall\".\n",
    "        average (str): averaging method for multi-class classification.\n",
    "    \n",
    "    Returns:\n",
    "        float: computed metric value.\n",
    "    \"\"\"\n",
    "    experiment_id = experiment.id\n",
    "    url = f\"{os.environ['PHOENIX_COLLECTOR_ENDPOINT']}/v1/experiments/{experiment_id}/json\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['PHOENIX_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch experiment data: {response.status_code} {response.text}\")\n",
    "\n",
    "    results = response.json()\n",
    "\n",
    "    def normalize(label):\n",
    "        return label.strip().strip('\"').strip(\"'\").lower()\n",
    "    \n",
    "    y_true = [normalize(entry[\"reference_output\"][\"ground_truth\"]) for entry in results]\n",
    "    y_pred = [normalize(entry[\"output\"]) for entry in results]\n",
    "\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9be50",
   "metadata": {},
   "source": [
    "## Experiment Processor\n",
    "\n",
    "This function pulls a Phoenix experiment and loads the data into a pandas dataframe so it can run through the optimizer.\n",
    "\n",
    "Specifically it:\n",
    "- Pulls the experiment data from Phoenix\n",
    "- Adds the input column to the dataframe\n",
    "- Adds the evals to the dataframe\n",
    "- Adds the output to the dataframe\n",
    "- Returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9dc4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def process_experiment(experiment, train_set, input_column_name, output_column_name,feedback_columns = None):\n",
    "    \"\"\"\n",
    "    Update existing columns in `train_set` with feedback from experiment annotations.\n",
    "\n",
    "    Args:\n",
    "        experiment_json (list): JSON data from experiment.\n",
    "        train_set (pd.DataFrame): DataFrame that already contains the feedback columns.\n",
    "        feedback_columns (list): List of feedback field names to update.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with values filled in from experiment annotations.\n",
    "    \"\"\"\n",
    "\n",
    "    experiment_id = experiment.id\n",
    "    url = f\"{os.environ['PHOENIX_COLLECTOR_ENDPOINT']}/v1/experiments/{experiment_id}/json\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['PHOENIX_API_KEY']}\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch experiment data: {response.status_code} {response.text}\")\n",
    "\n",
    "    results = response.json()\n",
    "\n",
    "    train_set[\"ground_truth\"] = [None] * len(train_set)\n",
    "    if feedback_columns:\n",
    "        for col in feedback_columns:\n",
    "            train_set[col] = [None] * len(train_set)\n",
    "\n",
    "    train_set = train_set.reset_index(drop=True)\n",
    "\n",
    "    for i, entry in enumerate(results):\n",
    "        eval_output = entry[\"annotations\"][0][\"explanation\"]\n",
    "        train_set.loc[i, \"ground_truth\"] = entry[\"reference_output\"][\"ground_truth\"]\n",
    "        if feedback_columns:\n",
    "            for item in eval_output.split(\";\"):\n",
    "                key_value = item.split(\":\")\n",
    "                if key_value[0].strip() in feedback_columns:\n",
    "                    key, value = key_value[0].strip(), key_value[1].strip()\n",
    "                    train_set.loc[i, key] = value\n",
    "\n",
    "    if \"output\" in train_set.columns:\n",
    "        train_set.rename(columns={\"output\": \"ground_truth\"}, inplace=True)\n",
    "\n",
    "    train_set[output_column_name] = [entry.get(\"output\") for entry in results]\n",
    "\n",
    "    train_set[input_column_name] = [entry.get(\"input\") for entry in results]\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93dace",
   "metadata": {},
   "source": [
    "# Prompt Optimization Loop with Phoenix Experiments\n",
    "\n",
    "This code implements an iterative prompt optimization system that uses **Phoenix experiments** to evaluate and improve prompts based on feedback from LLM evaluators.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `optimize_loop` function automates prompt engineering by:\n",
    "\n",
    "- Evaluating prompts using Phoenix experiments  \n",
    "- Collecting detailed feedback from LLM evaluators  \n",
    "- Optimizing prompts via a learning-based optimizer  \n",
    "- Iterating until the performance threshold is met or the loop limit is reached  \n",
    "\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "Each of these numbers are added as comments in the code.\n",
    "\n",
    "### 1. Initialization\n",
    "\n",
    "- Set up tracking variables:\n",
    "  - `train_metrics`, `test_metrics`, `raw_dfs` for storing evaluation results\n",
    "- Convert training dataset to a DataFrame for easy updates\n",
    "\n",
    "### 2. Baseline Evaluation\n",
    "\n",
    "- Run an initial experiment using the **test set**\n",
    "- Establish a **baseline metric** (e.g., accuracy, F1) to compare against future improvements\n",
    "\n",
    "### 3. Early Exit Check\n",
    "\n",
    "- If the **initial prompt already meets the performance threshold**, skip further optimization to save time and compute\n",
    "\n",
    "### 4. Main Optimization Loop\n",
    "\n",
    "For each iteration (up to `loops`):\n",
    "\n",
    "#### 4a. Run Training Experiment\n",
    "\n",
    "- Execute the current prompt on the **training set**\n",
    "- Use LLM evaluators to generate **natural language feedback**\n",
    "\n",
    "#### 4b. Process Feedback\n",
    "\n",
    "- Extract structured information from evaluator outputs:\n",
    "  - Correctness\n",
    "  - Explanation\n",
    "  - Confusion reason\n",
    "  - Error type\n",
    "  - Prompt fix suggestions\n",
    "- Update the training DataFrame with this feedback\n",
    "\n",
    "#### 4c. Generate Learning Annotations\n",
    "\n",
    "- Convert feedback into structured annotations for the optimizer to learn from\n",
    "- This allows learning from evaluator insights in a consistent format\n",
    "\n",
    "#### 4d. Optimize the Prompt\n",
    "\n",
    "- Pass feedback to the **PromptLearningOptimizer**\n",
    "- Generate an **improved prompt** that attempts to correct issues found in the previous iteration\n",
    "\n",
    "#### 4e. Evaluate on Test Set\n",
    "\n",
    "- Evaluate the updated prompt on the **held-out test set**\n",
    "- Assess **generalization** beyond the training data\n",
    "\n",
    "#### 4f. Track Metrics\n",
    "\n",
    "- Log metrics for:\n",
    "  - Training set performance\n",
    "  - Test set performance\n",
    "- Store raw results for further analysis or visualization\n",
    "\n",
    "#### 4g. Convergence Check\n",
    "\n",
    "- If the new prompt's test metric **meets or exceeds the threshold**, exit the loop early\n",
    "\n",
    "## Summary\n",
    "\n",
    "This loop enables **data-driven, feedback-informed prompt improvement** using Phoenix experiments, making it a robust tool for systematic prompt tuning in real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting prompt optimization with 5 iterations (scorer: accuracy, threshold: 1)\n",
      "ÔøΩÔøΩ Initial evaluation:\n",
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMDg=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:07<00:00 |  4.88it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMDg=\n",
      "\n",
      "Tasks Summary (08/07/25 03:11 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initial accuracy: 0.6774193548387096\n",
      "üìä Loop 1: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:08<00:00 |  3.53it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMDk=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 00:15<00:00 |  8.19it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 04:14<00:00 |  2.07s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMDk=\n",
      "\n",
      "Experiment Summary (08/07/25 03:15 PM -0700)\n",
      "--------------------------------------------\n",
      "          evaluator    n  n_scores  avg_score  n_labels  \\\n",
      "0  output_evaluator  123       123   0.674797       123   \n",
      "\n",
      "                       top_2_labels  \n",
      "0  {'correct': 83, 'incorrect': 40}  \n",
      "\n",
      "Tasks Summary (08/07/25 03:11 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\n",
      "üîç Running annotator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 90,000 token limit\n",
      "üìä Processing 123 examples in 1 batches\n",
      "   ‚úÖ Batch 1/1: Optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTA=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:07<00:00 |  4.21it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTA=\n",
      "\n",
      "Tasks Summary (08/07/25 03:16 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n",
      "‚úÖ Train accuracy: 0.6666666666666666\n",
      "‚úÖ Test accuracy: 0.5806451612903226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:09<00:00 |  3.29it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loop 2: Optimizing prompt...\n",
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTE=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 00:15<00:00 |  7.92it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 03:59<00:00 |  1.94s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTE=\n",
      "\n",
      "Experiment Summary (08/07/25 03:21 PM -0700)\n",
      "--------------------------------------------\n",
      "          evaluator    n  n_scores  avg_score  n_labels  \\\n",
      "0  output_evaluator  123       123   0.682927       123   \n",
      "\n",
      "                       top_2_labels  \n",
      "0  {'correct': 84, 'incorrect': 39}  \n",
      "\n",
      "Tasks Summary (08/07/25 03:17 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\n",
      "üîç Running annotator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 90,000 token limit\n",
      "üìä Processing 123 examples in 1 batches\n",
      "   ‚úÖ Batch 1/1: Optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTI=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:07<00:00 |  4.92it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTI=\n",
      "\n",
      "Tasks Summary (08/07/25 03:22 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train accuracy: 0.6747967479674797\n",
      "‚úÖ Test accuracy: 0.5806451612903226\n",
      "üìä Loop 3: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:09<00:00 |  3.31it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTM=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 00:15<00:00 |  7.94it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 03:54<00:00 |  1.90s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTM=\n",
      "\n",
      "Experiment Summary (08/07/25 03:26 PM -0700)\n",
      "--------------------------------------------\n",
      "          evaluator    n  n_scores  avg_score  n_labels  \\\n",
      "0  output_evaluator  123       123   0.691057       123   \n",
      "\n",
      "                       top_2_labels  \n",
      "0  {'correct': 85, 'incorrect': 38}  \n",
      "\n",
      "Tasks Summary (08/07/25 03:22 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\n",
      "üîç Running annotator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 90,000 token limit\n",
      "üìä Processing 123 examples in 1 batches\n",
      "   ‚úÖ Batch 1/1: Optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTQ=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 (96.8%) | ‚è≥ 00:06<00:00 |  4.89it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTQ=\n",
      "\n",
      "Tasks Summary (08/07/25 03:26 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train accuracy: 0.6829268292682927\n",
      "‚úÖ Test accuracy: 0.7096774193548387\n",
      "üìä Loop 4: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:08<00:00 |  3.63it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTU=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 00:15<00:00 |  8.19it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 04:10<00:00 |  2.04s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTU=\n",
      "\n",
      "Experiment Summary (08/07/25 03:31 PM -0700)\n",
      "--------------------------------------------\n",
      "          evaluator    n  n_scores  avg_score  n_labels  \\\n",
      "0  output_evaluator  123       123   0.707317       123   \n",
      "\n",
      "                       top_2_labels  \n",
      "0  {'correct': 87, 'incorrect': 36}  \n",
      "\n",
      "Tasks Summary (08/07/25 03:27 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\n",
      "üîç Running annotator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 90,000 token limit\n",
      "üìä Processing 123 examples in 1 batches\n",
      "   ‚úÖ Batch 1/1: Optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTY=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:08<00:00 |  4.61it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTY=\n",
      "\n",
      "Tasks Summary (08/07/25 03:32 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train accuracy: 0.7073170731707317\n",
      "‚úÖ Test accuracy: 0.7096774193548387\n",
      "üìä Loop 5: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 (100.0%) | ‚è≥ 00:09<00:00 |  3.26it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTc=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 00:17<00:00 |  7.11it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 (100.0%) | ‚è≥ 04:02<00:00 |  1.97s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mg==/compare?experimentId=RXhwZXJpbWVudDoyMTc=\n",
      "\n",
      "Experiment Summary (08/07/25 03:36 PM -0700)\n",
      "--------------------------------------------\n",
      "          evaluator    n  n_scores  avg_score  n_labels  \\\n",
      "0  output_evaluator  123       123   0.707317       123   \n",
      "\n",
      "                       top_2_labels  \n",
      "0  {'correct': 87, 'incorrect': 36}  \n",
      "\n",
      "Tasks Summary (08/07/25 03:32 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\n",
      "üîç Running annotator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 90,000 token limit\n",
      "üìä Processing 123 examples in 1 batches\n",
      "   ‚úÖ Batch 1/1: Optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/phoenix/utilities/client.py:60: UserWarning: The Phoenix server (11.19.0) and client (11.20.0) versions are mismatched and may have compatibility issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/experiments\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTg=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 30/31 (96.8%) | ‚è≥ 00:07<00:00 |  3.84it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "\n",
      "üîó View this experiment: https://app.phoenix.arize.com/s/prompt-opt/datasets/RGF0YXNldDo5Mw==/compare?experimentId=RXhwZXJpbWVudDoyMTg=\n",
      "\n",
      "Tasks Summary (08/07/25 03:36 PM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\n",
      "‚úÖ Train accuracy: 0.7073170731707317\n",
      "‚úÖ Test accuracy: 0.7419354838709677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    }
   ],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "from phoenix.experiments import run_experiment\n",
    "import copy\n",
    "import asyncio\n",
    "\n",
    "prompt_name = \"support_query_classification\"\n",
    "\n",
    "async def optimize_loop(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    system_prompt,\n",
    "    evaluators,\n",
    "    feedback_columns,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    \"\"\"\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    raw_dfs = []\n",
    "    train_df = train_dataset.as_dataframe()\n",
    "\n",
    "    print(f\"üöÄ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"ÔøΩÔøΩ Initial evaluation:\")\n",
    "\n",
    "    task = generate_task(system_prompt)\n",
    "\n",
    "    initial_experiment = run_experiment(\n",
    "        dataset=test_dataset,\n",
    "        task=task,\n",
    "    )\n",
    "\n",
    "    initial_metric_value = compute_metric(initial_experiment, scorer)\n",
    "    print(f\"‚úÖ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"üéâ Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompt_versions,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    prompt_versions = upload_prompt_phoenix(system_prompt, prompt_name, 0, [], None, initial_metric_value)\n",
    "\n",
    "    # Initialize all feedback columns\n",
    "\n",
    "    while loops > 0:\n",
    "        print(f\"üìä Loop {curr_loop}: Optimizing prompt...\")\n",
    "        \n",
    "        task = generate_task(system_prompt)\n",
    "\n",
    "        train_experiment = run_experiment(\n",
    "            dataset=train_dataset,\n",
    "            task=task,\n",
    "            evaluators=evaluators,\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        train_df = process_experiment(train_experiment, train_set, \"query\", \"output\", feedback_columns)\n",
    "\n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,\n",
    "            model_choice=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_df,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_df,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "        train_metric_post_value = compute_metric(train_experiment, scorer)\n",
    "        train_metrics.append(train_metric_post_value)\n",
    "\n",
    "        test_experiment = run_experiment(\n",
    "            dataset=test_dataset,\n",
    "            task=generate_task(system_prompt),\n",
    "        )\n",
    "        test_metric_post_value = compute_metric(test_experiment, scorer)\n",
    "        test_metrics.append(test_metric_post_value)\n",
    "\n",
    "        print(f\"‚úÖ Train {scorer}: {train_metric_post_value}\")\n",
    "        print(f\"‚úÖ Test {scorer}: {test_metric_post_value}\")\n",
    "\n",
    "        prompt_versions = upload_prompt_phoenix(system_prompt, prompt_name, curr_loop, prompt_versions, train_metric_post_value, test_metric_post_value)\n",
    "\n",
    "        if test_metric_post_value >= threshold:\n",
    "            print(\"üéâ Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"prompt\": prompt_versions,\n",
    "        \"raw\": raw_dfs\n",
    "    }\n",
    "\n",
    "# Main execution - use asyncio.run() to run the async function\n",
    "evaluators = [output_evaluator]\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "result = asyncio.run(optimize_loop(train_dataset, test_dataset, system_prompt, evaluators, feedback_columns, loops=5, scorer=\"accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7627",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddd42533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Best Prompt Found:\n",
      "{'iteration': 4, 'prompt': '**Optimized Prompt for Accurate Classification of Support Queries**\\n\\nYou are given a support query:\\nsupport query: {query}\\n\\nClassify the support query into one of the following categories:\\n- Account Creation\\n- Login Issues\\n- Password Reset\\n- Two-Factor Authentication\\n- Profile Updates\\n- Billing Inquiry\\n- Refund Request\\n- Subscription Upgrade/Downgrade\\n- Payment Method Update\\n- Invoice Request\\n- Order Status\\n- Shipping Delay\\n- Product Return\\n- Warranty Claim\\n- Technical Bug Report\\n- Feature Request\\n- Integration Help\\n- Data Export\\n- Security Concern\\n- Terms of Service Question\\n- Privacy Policy Question\\n- Compliance Inquiry\\n- Accessibility Support\\n- Language Support\\n- Mobile App Issue\\n- Desktop App Issue\\n- Email Notifications\\n- Marketing Preferences\\n- Beta Program Enrollment\\n- General Feedback\\n\\n**Guidelines:**\\n1. **Context Sensitivity**: Focus on understanding the full context of the query. Prioritize the user\\'s main intent over isolated keywords. Consider the sequence of actions described to determine the most relevant category.\\n2. **Specificity vs. Generalization**: Differentiate between broader and more specific categories. Prioritize specific categories when the query clearly aligns with them. Use detailed examples to guide distinctions, such as differentiating \"Invoice Request\" from \"Billing Inquiry.\"\\n3. **Keyword Bias**: Avoid relying solely on keywords. Ensure the context supports the chosen category. For example, if a query mentions \"invoice,\" determine if it is about requesting or correcting an invoice (Invoice Request) rather than a general billing issue.\\n4. **Multi-Intent Handling**: If a query fits multiple categories, prioritize based on the user\\'s primary concern or the most actionable intent. For instance, if a query involves both \"payment\" and \"subscription change,\" align it under \"Subscription Upgrade/Downgrade.\"\\n5. **Ambiguous or Colloquial Queries**: Translate vague or colloquial language into formal equivalents to better understand the user\\'s intent. Map common paraphrases to their formal equivalents.\\n6. **Technical and Integration Issues**: Clearly distinguish between technical software bugs and third-party integration issues. Classify references to external services (e.g., Google Calendar, Slack) as \"Integration Help\" when relevant.\\n7. **Handling Ambiguity**: When faced with ambiguous queries, use context clues to infer the most likely intent. If uncertain, choose the category that aligns with the user\\'s primary action or request.\\n8. **Feedback and Iteration**: Incorporate user feedback to refine and improve classification accuracy over time.\\n\\n**Examples:**\\n- If a user mentions duplicate charges without asking for a refund, classify as Billing Inquiry.\\n- If a query involves a password reset email, classify as Password Reset, even if login issues are mentioned.\\n- For queries about connecting to third-party services, classify as Integration Help.\\n- If a user reports a package marked as delivered but not received, classify as Shipping Delay.\\n- When a user asks about changing the app language, classify as Language Support.\\n- If a user reports a malfunction where an expected action does nothing, classify as Technical Bug Report.\\n- If a query mentions a recent account/profile change that precedes another symptom, prioritize the category tied to the initiating change (Profile Updates).\\n- If a user reports being charged after switching to a free plan, classify as Subscription Upgrade/Downgrade.\\n- If a user mentions a UI element being greyed out and asks how to enable it, classify as Permission/Access Issue unless a malfunction is reported.\\n- If a user mentions not receiving an expected email tied to signup or verification, classify as Account Creation rather than Email Notifications.\\n- If a user reports receiving an item that appears used, opened, or damaged upon arrival, classify it as Product Return even if \\'return\\' or \\'refund\\' is not explicitly stated.\\n\\nReturn just the category, no other text.', 'phoenix_id': 'UHJvbXB0VmVyc2lvbjo5NA==', 'train_metric': 0.7073170731707317, 'test_metric': 0.7096774193548387}\n",
      "üß™ Initial Test Accuracy: 0.6774193548387096\n",
      "üß™ Optimized Test Accuracy: 0.7419354838709677 (Œî 0.0645)\n"
     ]
    }
   ],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompt\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüîç Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"üß™ Initial Test Accuracy: {initial_test_acc}\")\n",
    "print(f\"üß™ Optimized Test Accuracy: {best_test_acc} (Œî {best_test_acc - initial_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
