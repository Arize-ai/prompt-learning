{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124686e7",
   "metadata": {},
   "source": [
    "# Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "[![My Image](data:image/png;base64,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)](https://arize.com)\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in support_query_classification/hard_queries.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fa90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.evals import OpenAIModel, llm_generate\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f560fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY'] or getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714c0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb99228",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f910df",
   "metadata": {},
   "source": [
    "## **Make train/test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e83230",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../support_query_classification/hard_queries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../support_query_classification/hard_queries.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m train_set = data.sample(frac=\u001b[32m0.8\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      3\u001b[39m test_set = data.drop(train_set.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/base2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/base2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/base2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/base2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/base2/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../support_query_classification/hard_queries.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../support_query_classification/hard_queries.csv\")\n",
    "train_set = data.sample(frac=0.8, random_state=42)\n",
    "test_set = data.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a77b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are given a support query:\n",
    "support query: {query}\n",
    "\n",
    "Account Creation – Questions about creating a new user account.\n",
    "Login Issues – Trouble accessing an existing account, including login errors.\n",
    "Password Reset – Requests to reset or recover a forgotten password.\n",
    "Two-Factor Authentication – Problems related to 2FA codes or verification steps.\n",
    "Profile Updates – Questions about changing profile details like email or name.\n",
    "Billing Inquiry – Questions about charges, billing cycles, or transaction history.\n",
    "Refund Request – Asking for a refund due to dissatisfaction or error.\n",
    "Subscription Upgrade/Downgrade – Requests to change the user's subscription plan.\n",
    "Payment Method Update – Updating or replacing the payment method on file.\n",
    "Invoice Request – Requests for a copy of an invoice or receipt.\n",
    "Order Status – Inquiries about the delivery or progress of an order.\n",
    "Shipping Delay – Reporting or asking about delayed shipments.\n",
    "Product Return – Requests for returning purchased products.\n",
    "Warranty Claim – Submitting a claim for defective items under warranty.\n",
    "Technical Bug Report – Reporting crashes, glitches, or other software bugs.\n",
    "Feature Request – Suggestions to add or improve product features.\n",
    "Integration Help – Issues with connecting third-party services or tools.\n",
    "Data Export – Requests to download or export personal or usage data.\n",
    "Security Concern – Reporting suspicious activity or potential security issues.\n",
    "Terms of Service Question – Questions about cancellation, usage rules, or rights.\n",
    "Privacy Policy Question – Questions about how user data is collected or used.\n",
    "Compliance Inquiry – Questions about legal compliance (e.g., GDPR, CCPA).\n",
    "Accessibility Support – Requests for help using the service with a disability.\n",
    "Language Support – Questions about multilingual support or language settings.\n",
    "Mobile App Issue – Problems specifically with the mobile version of the app.\n",
    "Desktop App Issue – Issues related to the desktop version or installation.\n",
    "Email Notifications – Not receiving expected emails such as confirmations.\n",
    "Marketing Preferences – Requests to manage or stop promotional emails.\n",
    "Beta Program Enrollment – Interest in joining early access or beta programs.\n",
    "General Feedback – General praise, criticism, or user suggestions.\n",
    "\n",
    "\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee9d0c",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(dataset, system_prompt):\n",
    "    output_model = OpenAIModel(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "    outputs = llm_generate(\n",
    "        dataframe=dataset,\n",
    "        template=system_prompt,\n",
    "        model=output_model,\n",
    "        concurrency=40,\n",
    "        verbose=True\n",
    "    )\n",
    "    return outputs[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3e25e",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **top_3_classes**: [Best match class, Second best match class, Third best match class]\n",
    "\n",
    "-  **ground_truth**: The correct classification\n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:20<00:00 |  1.51it/s\n"
     ]
    }
   ],
   "source": [
    "def find_attributes(output):\n",
    "    \"\"\"Extract fields from evaluator output\"\"\"\n",
    "    correctness_pattern = r'\"correctness\":\\s*\"([^\"]*)\"'\n",
    "    explanation_pattern = r'\"explanation\":\\s*\"([^\"]*)\"'\n",
    "    confusion_reason_pattern = r'\"confusion_reason\":\\s*\"([^\"]*)\"'\n",
    "    error_type_pattern = r'\"error_type\":\\s*\"([^\"]*)\"'\n",
    "    top_3_classes_pattern = r'\"top_3_classes\":\\s*\\[(.*?)\\]'\n",
    "    evidence_span_pattern = r'\"evidence_span\":\\s*\"([^\"]*)\"'\n",
    "    prompt_fix_suggestion_pattern = r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    \n",
    "    match_correctness = re.search(correctness_pattern, output, re.IGNORECASE)\n",
    "    match_explanation = re.search(explanation_pattern, output, re.IGNORECASE)\n",
    "    match_confusion_reason = re.search(confusion_reason_pattern, output, re.IGNORECASE)\n",
    "    match_error_type = re.search(error_type_pattern, output, re.IGNORECASE)\n",
    "    match_top_3_classes = re.search(top_3_classes_pattern, output, re.IGNORECASE)\n",
    "    match_evidence_span = re.search(evidence_span_pattern, output, re.IGNORECASE)\n",
    "    match_prompt_fix_suggestion = re.search(prompt_fix_suggestion_pattern, output, re.IGNORECASE)\n",
    "    correctness = match_correctness.group(1) if match_correctness else None\n",
    "    explanation = match_explanation.group(1) if match_explanation else None\n",
    "    confusion_reason = match_confusion_reason.group(1) if match_confusion_reason else None\n",
    "    error_type = match_error_type.group(1) if match_error_type else None\n",
    "    top_3_classes = match_top_3_classes.group(1) if match_top_3_classes else None\n",
    "    evidence_span = match_evidence_span.group(1) if match_evidence_span else None\n",
    "    prompt_fix_suggestion = match_prompt_fix_suggestion.group(1) if match_prompt_fix_suggestion else None\n",
    "    return correctness, explanation, confusion_reason, error_type, top_3_classes, evidence_span, prompt_fix_suggestion\n",
    "\n",
    "def output_parser(response: str, row_index: int) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, top_3_classes, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"top_3_classes\": top_3_classes,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "def output_evaluator(dataset):\n",
    "    with open(\"../support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    eval_model = OpenAIModel(\n",
    "        model=\"o3-2025-04-16\",\n",
    "        model_kwargs={\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluation_results = llm_generate(\n",
    "        dataframe=dataset,\n",
    "        template=evaluator_prompt,\n",
    "        model=eval_model,\n",
    "        output_parser=output_parser,\n",
    "        concurrency=40,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "    feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"top_3_classes\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "    for col in feedback_columns:\n",
    "        if col in evaluation_results.columns:\n",
    "            dataset[col] = evaluation_results[col]\n",
    "\n",
    "    return dataset, feedback_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0953a6a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def compute_metric(y_pred, y_true, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested metric for multiclass classification.\n",
    "    \"\"\"\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577bfa8",
   "metadata": {},
   "source": [
    "# **Optimization**\n",
    "\n",
    "## **Optimization Loop**\n",
    "\n",
    "Below are the steps taken to optimize the prompt.\n",
    "\n",
    "**0. Baseline Evaluation**  \n",
    "- Test the base prompt on the **test set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "- If performance meets the target threshold, skip optimization.  \n",
    "\n",
    "**1. Feedback Generation**  \n",
    "- Run the evaluator on the **training set**.  \n",
    "- Generate natural language feedback for the prompt optimizer.  \n",
    "\n",
    "**2. Prompt Optimization**  \n",
    "- Pass the feedback to the prompt optimizer, `PromptLearningOptimizer`  \n",
    "- Generate a new optimized prompt.  \n",
    "\n",
    "**3. Training Evaluation**  \n",
    "- Test the optimized prompt on the **training set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "\n",
    "**4. Test Evaluation**  \n",
    "- Test the optimized prompt on the **test set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "\n",
    "**5. Iteration Check**  \n",
    "- If **test metrics** meet the target threshold → stop.  \n",
    "- Otherwise → repeat steps 1–5 until:  \n",
    "  - Threshold is met, or  \n",
    "  - Maximum iteration count is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00294599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting prompt optimization with 5 iterations (scorer: accuracy, threshold: 1)\n",
      "📊 Initial evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:05<00:00 |  6.06it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initial accuracy: 0.45161290322580644\n",
      "📊 Loop 1: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 00:13<00:00 |  9.20it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 01:30<00:00 |  1.36it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.5121951219512195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:04<00:00 |  6.66it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.45161290322580644\n",
      "📊 Loop 2: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 00:14<00:00 |  8.47it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 14:09<00:00 |  6.91s/it\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                                                \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker timeout, requeuing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 03:01<00:00 |  1.48s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.5772357723577236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:04<00:00 |  4.88it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:05<00:00 |  5.53it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.41935483870967744\n",
      "📊 Loop 3: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 00:19<00:00 |  6.44it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 01:35<00:00 |  1.29it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.5609756097560976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:04<00:00 |  4.74it/s/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:05<00:00 |  5.55it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.4838709677419355\n",
      "📊 Loop 4: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |███████▋  | 94/123 (76.4%) | ⏳ 00:09<00:03 |  9.07it/s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised InternalServerError('upstream connect error or disconnect/reset before headers. reset reason: connection termination')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |█████████▉| 122/123 (99.2%) | ⏳ 00:12<00:00 | 11.19it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 00:31<00:00 | 11.19it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 01:50<00:00 |  1.11it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.5528455284552846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n",
      "llm_generate |██████████| 31/31 (100.0%) | ⏳ 00:05<00:00 |  5.53it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.5806451612903226\n",
      "📊 Loop 5: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 00:13<00:00 |  9.37it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                                       \n",
      "\n",
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 02:39<00:00 | 11.19it/s\n",
      "                                                                       \n",
      "\n",
      "llm_generate |██████████| 123/123 (100.0%) | ⏳ 02:39<00:00 | 11.19it/s\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised InternalServerError('upstream connect error or disconnect/reset before headers. reset reason: connection timeout')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.6341463414634146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.41935483870967744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    }
   ],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "def optimize_loop(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    system_prompt,\n",
    "    evaluators,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    threshold: float, threshold for the selected metric\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    prompts = []\n",
    "    raw_dfs = []\n",
    "\n",
    "    print(f\"🚀 Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"📊 Initial evaluation:\")\n",
    "    test_set[\"output\"] = generate_output(test_set, system_prompt)\n",
    "    initial_metric_value = compute_metric(test_set[\"output\"], test_set[\"ground_truth\"], scorer)\n",
    "    print(f\"✅ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    prompts.append(system_prompt)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"🎉 Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompts,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    # Initialize all feedback columns\n",
    "    feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"top_3_classes\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "    for col in feedback_columns:\n",
    "        train_set[col] = [None for _ in range(len(train_set))]\n",
    "    \n",
    "    while loops > 0:\n",
    "        print(f\"📊 Loop {curr_loop}: Optimizing prompt...\")\n",
    "        train_set[\"output\"] = generate_output(train_set, system_prompt)\n",
    "\n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,\n",
    "            model_choice=\"o3-2025-04-16\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        train_set, _ = optimizer.run_evaluators(\n",
    "            train_set,\n",
    "            evaluators,\n",
    "            feedback_columns=feedback_columns\n",
    "        )\n",
    "\n",
    "        with open(\"../support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_set,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_set,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "\n",
    "        prompts.append(system_prompt)\n",
    "\n",
    "        train_metric_post_value = compute_metric(train_set[\"output\"], train_set[\"ground_truth\"], scorer)\n",
    "        train_metrics.append(train_metric_post_value)\n",
    "        print(f\"✅ Train {scorer}: {train_metric_post_value}\")\n",
    "\n",
    "        test_set[\"output\"] = generate_output(test_set, system_prompt)\n",
    "        test_metric_post_value = compute_metric(test_set[\"output\"], test_set[\"ground_truth\"], scorer)\n",
    "        test_metrics.append(test_metric_post_value)\n",
    "        print(f\"✅ Test {scorer}: {test_metric_post_value}\")\n",
    "\n",
    "        if test_metric_post_value >= threshold:\n",
    "            print(\"🎉 Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompts\": prompts,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "\n",
    "\n",
    "evaluators = [output_evaluator]\n",
    "result = optimize_loop(train_set, test_set, system_prompt, evaluators, loops=5, scorer=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f3768",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026586f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Best Prompt Found:\n",
      "You are a customer-support ticket classifier.\n",
      "\n",
      "INPUT  \n",
      "support query: {query}\n",
      "\n",
      "TASK  \n",
      "Read the entire message, identify the user’s single primary intent, and output the one best-matching category name from the list below.  \n",
      "Return ONLY that name—no other words, numbers, or punctuation.\n",
      "\n",
      "GENERAL RULES  \n",
      "1. Output must be one (and only one) name that appears verbatim in the Category List. Never invent or shorten names.  \n",
      "2. Choose the most specific class that solves the user’s main problem; prefer the child over its parent.  \n",
      "3. Use full-message meaning, not isolated keywords. If words conflict with context, trust the context.  \n",
      "4. When several issues are mentioned, pick the one the user wants fixed first (usually the obstacle blocking them now).  \n",
      "5. If intent is unclear after careful reading, pick the most probable class—not “General Feedback”.  \n",
      "6. Slang, typos, emojis, or missing words still map to their standard meaning.\n",
      "\n",
      "COMMON DECISION PIVOTS  \n",
      "• RETURN / EXCHANGE  \n",
      "  – Item already received and user asks about sending it back, labels, packaging, or status of a sent-back item → Product Return.  \n",
      "  – User hasn’t returned anything yet but wants money back → Refund Request.  \n",
      "\n",
      "• CHARGES & MONEY  \n",
      "  – Duplicate, wrong, or unclear charges → Billing Inquiry.  \n",
      "  – Wants an invoice copy, correction, or number → Invoice Request (even if charge seems wrong).  \n",
      "  – Needs to add/remove/change credit-card or split payment → Payment Method Update.  \n",
      "\n",
      "• SUBSCRIPTIONS & PLANS  \n",
      "  – Any upgrade, downgrade, or unexpected change of plan/tier (even if money is also mentioned) → Subscription Upgrade/Downgrade.  \n",
      "\n",
      "• AUTHENTICATION  \n",
      "  – Missing, invalid, or stuck codes/OTP/2FA apps/texts → Two-Factor Authentication.  \n",
      "  – Reset links/emails or forgotten passwords → Password Reset.  \n",
      "  – Repeated or strange login prompts, generic inability to sign in (no code or reset focus) → Login Issues.  \n",
      "\n",
      "• ACCESS & PERMISSIONS  \n",
      "  – Greyed-out button or “not enough rights” → Permission/Access Issue.  \n",
      "  – Feature exists for others but not this user → Feature Access Issue.  \n",
      "\n",
      "• APP-SPECIFIC BUGS  \n",
      "  – Mobile-only malfunction → Mobile App Issue.  \n",
      "  – Desktop-only malfunction → Desktop App Issue.  \n",
      "  – Anything else broken or erroring → Technical Bug Report (unless it fits a more specific rule above).\n",
      "\n",
      "• DOCUMENTS & DATA  \n",
      "  – Downloading/exporting user data/history → Data Export.  \n",
      "  – Questions on data retention/deletion/sharing → Privacy Policy Question.  \n",
      "  – Compliance with external regulations (GDPR, HIPAA, SOC 2, etc.) → Compliance Inquiry.\n",
      "\n",
      "• MISC  \n",
      "  – Opinion with no request → General Feedback.  \n",
      "  – UI or capability improvement request → Feature Request.  \n",
      "  – Accessibility accommodation (font size, screen reader, colour contrast) → Accessibility Support.  \n",
      "  – Unauthorised activity / hacking fears → Security Concern.\n",
      "\n",
      "FEW-SHOT GUIDANCE  \n",
      "(Queries are shortened for space; follow the mapping pattern.)\n",
      "\n",
      "1. “Returned my headphones last month, still no refund.” → Product Return  \n",
      "2. “Got double charged again??” → Billing Inquiry  \n",
      "3. “Change card option just spins forever.” → Payment Method Update  \n",
      "4. “Little pop-up with the numbers never comes.” → Two-Factor Authentication  \n",
      "5. “Charged even after switching to free plan.” → Subscription Upgrade/Downgrade  \n",
      "6. “Delete my info if I leave?” → Privacy Policy Question  \n",
      "7. “Beta sign-up—nothing looks different yet.” → Beta Program Enrollment  \n",
      "8. “App freezes when uploading PNGs.” → Technical Bug Report\n",
      "\n",
      "CATEGORY LIST  \n",
      "Account Creation  \n",
      "Login Issues  \n",
      "Password Reset  \n",
      "Two-Factor Authentication  \n",
      "Profile Updates  \n",
      "Billing Inquiry  \n",
      "Refund Request  \n",
      "Subscription Upgrade/Downgrade  \n",
      "Payment Method Update  \n",
      "Invoice Request  \n",
      "Order Status  \n",
      "Shipping Delay  \n",
      "Product Return  \n",
      "Warranty Claim  \n",
      "Technical Bug Report  \n",
      "Feature Request  \n",
      "Feature Access Issue  \n",
      "Permission/Access Issue  \n",
      "Integration Help  \n",
      "Data Export  \n",
      "Security Concern  \n",
      "Terms of Service Question  \n",
      "Privacy Policy Question  \n",
      "Compliance Inquiry  \n",
      "Accessibility Support  \n",
      "Language Support  \n",
      "Mobile App Issue  \n",
      "Desktop App Issue  \n",
      "Email Notifications  \n",
      "Marketing Preferences  \n",
      "Beta Program Enrollment  \n",
      "General Feedback\n",
      "\n",
      "OUTPUT  \n",
      "One exact category name from the list above.\n",
      "🏋️ Train Accuracy: 0.5528455284552846 (Δ 0.0407)\n",
      "🧪 Test Accuracy: 0.5806451612903226 (Δ 0.1290)\n"
     ]
    }
   ],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompts\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\n🔍 Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"🏋️ Train Accuracy: {best_train_acc} (Δ {best_train_acc - initial_train_acc:.4f})\" if best_train_acc is not None else \"Train accuracy not available\")\n",
    "print(f\"🧪 Test Accuracy: {best_test_acc} (Δ {best_test_acc - initial_test_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c5eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
