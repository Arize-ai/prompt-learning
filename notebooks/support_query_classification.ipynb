{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124686e7",
   "metadata": {},
   "source": [
    "# Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "[![My Image](data:image/png;base64,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)](https://arize.com)\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in support_query_classification/hard_queries.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fa90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.evals import OpenAIModel, llm_generate\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f560fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.environ['OPENAI_API_KEY'] or getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714c0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb99228",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f910df",
   "metadata": {},
   "source": [
    "## **Make train/test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e83230",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "train_set = data.sample(frac=0.8, random_state=42)\n",
    "test_set = data.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a77b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are given a support query:\n",
    "support query: {query}\n",
    "\n",
    "Account Creation – Questions about creating a new user account.\n",
    "Login Issues – Trouble accessing an existing account, including login errors.\n",
    "Password Reset – Requests to reset or recover a forgotten password.\n",
    "Two-Factor Authentication – Problems related to 2FA codes or verification steps.\n",
    "Profile Updates – Questions about changing profile details like email or name.\n",
    "Billing Inquiry – Questions about charges, billing cycles, or transaction history.\n",
    "Refund Request – Asking for a refund due to dissatisfaction or error.\n",
    "Subscription Upgrade/Downgrade – Requests to change the user's subscription plan.\n",
    "Payment Method Update – Updating or replacing the payment method on file.\n",
    "Invoice Request – Requests for a copy of an invoice or receipt.\n",
    "Order Status – Inquiries about the delivery or progress of an order.\n",
    "Shipping Delay – Reporting or asking about delayed shipments.\n",
    "Product Return – Requests for returning purchased products.\n",
    "Warranty Claim – Submitting a claim for defective items under warranty.\n",
    "Technical Bug Report – Reporting crashes, glitches, or other software bugs.\n",
    "Feature Request – Suggestions to add or improve product features.\n",
    "Integration Help – Issues with connecting third-party services or tools.\n",
    "Data Export – Requests to download or export personal or usage data.\n",
    "Security Concern – Reporting suspicious activity or potential security issues.\n",
    "Terms of Service Question – Questions about cancellation, usage rules, or rights.\n",
    "Privacy Policy Question – Questions about how user data is collected or used.\n",
    "Compliance Inquiry – Questions about legal compliance (e.g., GDPR, CCPA).\n",
    "Accessibility Support – Requests for help using the service with a disability.\n",
    "Language Support – Questions about multilingual support or language settings.\n",
    "Mobile App Issue – Problems specifically with the mobile version of the app.\n",
    "Desktop App Issue – Issues related to the desktop version or installation.\n",
    "Email Notifications – Not receiving expected emails such as confirmations.\n",
    "Marketing Preferences – Requests to manage or stop promotional emails.\n",
    "Beta Program Enrollment – Interest in joining early access or beta programs.\n",
    "General Feedback – General praise, criticism, or user suggestions.\n",
    "\n",
    "\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee9d0c",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0dad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(dataset, system_prompt):\n",
    "    output_model = OpenAIModel(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "    outputs = llm_generate(\n",
    "        dataframe=dataset,\n",
    "        template=system_prompt,\n",
    "        model=output_model,\n",
    "        concurrency=40,\n",
    "        verbose=True\n",
    "    )\n",
    "    return outputs[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3e25e",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **top_3_classes**: [Best match class, Second best match class, Third best match class]\n",
    "\n",
    "-  **ground_truth**: The correct classification\n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2136889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_attributes(output):\n",
    "    \"\"\"Extract fields from evaluator output\"\"\"\n",
    "    correctness_pattern = r'\"correctness\":\\s*\"([^\"]*)\"'\n",
    "    explanation_pattern = r'\"explanation\":\\s*\"([^\"]*)\"'\n",
    "    confusion_reason_pattern = r'\"confusion_reason\":\\s*\"([^\"]*)\"'\n",
    "    error_type_pattern = r'\"error_type\":\\s*\"([^\"]*)\"'\n",
    "    top_3_classes_pattern = r'\"top_3_classes\":\\s*\\[(.*?)\\]'\n",
    "    evidence_span_pattern = r'\"evidence_span\":\\s*\"([^\"]*)\"'\n",
    "    prompt_fix_suggestion_pattern = r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    \n",
    "    match_correctness = re.search(correctness_pattern, output, re.IGNORECASE)\n",
    "    match_explanation = re.search(explanation_pattern, output, re.IGNORECASE)\n",
    "    match_confusion_reason = re.search(confusion_reason_pattern, output, re.IGNORECASE)\n",
    "    match_error_type = re.search(error_type_pattern, output, re.IGNORECASE)\n",
    "    match_top_3_classes = re.search(top_3_classes_pattern, output, re.IGNORECASE)\n",
    "    match_evidence_span = re.search(evidence_span_pattern, output, re.IGNORECASE)\n",
    "    match_prompt_fix_suggestion = re.search(prompt_fix_suggestion_pattern, output, re.IGNORECASE)\n",
    "    correctness = match_correctness.group(1) if match_correctness else None\n",
    "    explanation = match_explanation.group(1) if match_explanation else None\n",
    "    confusion_reason = match_confusion_reason.group(1) if match_confusion_reason else None\n",
    "    error_type = match_error_type.group(1) if match_error_type else None\n",
    "    top_3_classes = match_top_3_classes.group(1) if match_top_3_classes else None\n",
    "    evidence_span = match_evidence_span.group(1) if match_evidence_span else None\n",
    "    prompt_fix_suggestion = match_prompt_fix_suggestion.group(1) if match_prompt_fix_suggestion else None\n",
    "    return correctness, explanation, confusion_reason, error_type, top_3_classes, evidence_span, prompt_fix_suggestion\n",
    "\n",
    "def output_parser(response: str, row_index: int) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, top_3_classes, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"top_3_classes\": top_3_classes,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "def output_evaluator(dataset):\n",
    "    with open(f\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    eval_model = OpenAIModel(\n",
    "        model=\"o3-2025-04-16\",\n",
    "        model_kwargs={\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluation_results = llm_generate(\n",
    "        dataframe=dataset,\n",
    "        template=evaluator_prompt,\n",
    "        model=eval_model,\n",
    "        output_parser=output_parser,\n",
    "        concurrency=40,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "    feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"top_3_classes\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "    for col in feedback_columns:\n",
    "        if col in evaluation_results.columns:\n",
    "            dataset[col] = evaluation_results[col]\n",
    "\n",
    "    return dataset, feedback_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0953a6a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eae5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(label):\n",
    "    return label.strip().strip('\"').strip(\"'\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5dba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def compute_metric(y_pred, y_true, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested metric for multiclass classification.\n",
    "    \"\"\"\n",
    "    y_pred = [normalize(label) for label in y_pred]\n",
    "    y_true = [normalize(label) for label in y_true]\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577bfa8",
   "metadata": {},
   "source": [
    "# **Optimization**\n",
    "\n",
    "## **Optimization Loop**\n",
    "\n",
    "Below are the steps taken to optimize the prompt.\n",
    "\n",
    "**0. Baseline Evaluation**  \n",
    "- Test the base prompt on the **test set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "- If performance meets the target threshold, skip optimization.  \n",
    "\n",
    "**1. Feedback Generation**  \n",
    "- Run the evaluator on the **training set**.  \n",
    "- Generate natural language feedback for the prompt optimizer.  \n",
    "\n",
    "**2. Prompt Optimization**  \n",
    "- Pass the feedback to the prompt optimizer, `PromptLearningOptimizer`  \n",
    "- Generate a new optimized prompt.  \n",
    "\n",
    "**3. Training Evaluation**  \n",
    "- Test the optimized prompt on the **training set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "\n",
    "**4. Test Evaluation**  \n",
    "- Test the optimized prompt on the **test set**.  \n",
    "- Compute metrics (precision, recall, F1, accuracy).  \n",
    "\n",
    "**5. Iteration Check**  \n",
    "- If **test metrics** meet the target threshold → stop.  \n",
    "- Otherwise → repeat steps 1–5 until:  \n",
    "  - Threshold is met, or  \n",
    "  - Maximum iteration count is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00294599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting prompt optimization with 5 iterations (scorer: accuracy, threshold: 1)\n",
      "📊 Initial evaluation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517f02cc650e4002a63eec8ef3c16648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initial accuracy: 0.6129032258064516\n",
      "📊 Loop 1: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5da23ec0714317a293ddfae6bb39c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabfa2ab56b646dc830d783ee55dbeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.6016260162601627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffc432e21ff41088181b9081cbb3a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.7096774193548387\n",
      "📊 Loop 2: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf9a8bb29a04bf2be9e5330b019d6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad32cf017a449ef9706c6a15051ec0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.7073170731707317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce8b604f0da487196b8ebdf8c0a8374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.8064516129032258\n",
      "📊 Loop 3: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af00bcbbeab4d67b05e2e85eeebe028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b806b7e0884d0eae60e45f1b5e6b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.7560975609756098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ace75518cd475ab561b6e26f12a99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.8064516129032258\n",
      "📊 Loop 4: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8eeaff3e694d7382b59b792633e59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64706412d3a94e2db55fdfb8894442c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.7723577235772358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13a86a6a85548f480fcc4144e387b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.6774193548387096\n",
      "📊 Loop 5: Optimizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea289c2ebce24460ad4b654eb3881b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running 1 evaluator(s)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6799d08ebd49438d9e8d50dab29afe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/123 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker timeout, requeuing\n",
      "   ✅ Evaluator 1: prompt_fix_suggestion\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'top_3_classes', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "✅ Train accuracy: 0.7154471544715447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfa939e536141529ab31569ca8051b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_generate |          | 0/31 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test accuracy: 0.7096774193548387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:98: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_true = type_of_target(y_true, input_name=\"y_true\")\n",
      "/opt/anaconda3/envs/base2/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples.\n",
      "  type_pred = type_of_target(y_pred, input_name=\"y_pred\")\n"
     ]
    }
   ],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "def optimize_loop(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    system_prompt,\n",
    "    evaluators,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    threshold: float, threshold for the selected metric\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    prompts = []\n",
    "    raw_dfs = []\n",
    "\n",
    "    print(f\"🚀 Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"📊 Initial evaluation:\")\n",
    "    test_set[\"output\"] = generate_output(test_set, system_prompt)\n",
    "    initial_metric_value = compute_metric(test_set[\"output\"], test_set[\"ground_truth\"], scorer)\n",
    "    print(f\"✅ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    prompts.append(system_prompt)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"🎉 Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompts,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    # Initialize all feedback columns\n",
    "    feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"top_3_classes\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "    for col in feedback_columns:\n",
    "        train_set[col] = [None for _ in range(len(train_set))]\n",
    "    \n",
    "    while loops > 0:\n",
    "        print(f\"📊 Loop {curr_loop}: Optimizing prompt...\")\n",
    "        train_set[\"output\"] = generate_output(train_set, system_prompt)\n",
    "\n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,\n",
    "            model_choice=\"o3-2025-04-16\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        train_set, _ = optimizer.run_evaluators(\n",
    "            train_set,\n",
    "            evaluators,\n",
    "            feedback_columns=feedback_columns\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_set,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_set,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "\n",
    "        prompts.append(system_prompt)\n",
    "\n",
    "        train_metric_post_value = compute_metric(train_set[\"output\"], train_set[\"ground_truth\"], scorer)\n",
    "        train_metrics.append(train_metric_post_value)\n",
    "        print(f\"✅ Train {scorer}: {train_metric_post_value}\")\n",
    "\n",
    "        test_set[\"output\"] = generate_output(test_set, system_prompt)\n",
    "        test_metric_post_value = compute_metric(test_set[\"output\"], test_set[\"ground_truth\"], scorer)\n",
    "        test_metrics.append(test_metric_post_value)\n",
    "        print(f\"✅ Test {scorer}: {test_metric_post_value}\")\n",
    "\n",
    "        if test_metric_post_value >= threshold:\n",
    "            print(\"🎉 Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompts\": prompts,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "\n",
    "\n",
    "evaluators = [output_evaluator]\n",
    "result = optimize_loop(train_set, test_set, system_prompt, evaluators, loops=5, scorer=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f3768",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026586f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Best Prompt Found:\n",
      "You are a triage assistant for customer-support tickets.  \n",
      "Goal: map each user message to ONE—and only one—help-desk category from the list below and output that category name exactly, with no extra text.\n",
      "\n",
      "How to choose:  \n",
      "1. Read the whole message; ignore isolated keywords—use context.  \n",
      "2. Pick the most specific category that solves the user’s MAIN request.  \n",
      "   • If a message fits both a broad and a narrow class, pick the narrow one.  \n",
      "   • If several issues are mentioned, pick the one that blocks the user first or that they explicitly ask help for.  \n",
      "3. If the intent is unclear, choose the best match rather than “guessing” from keywords.\n",
      "\n",
      "Categories (alphabetical)\n",
      "\n",
      "Account Creation – Trouble signing up or activating a brand-new account (e.g., no verification email).  \n",
      "Accessibility Support – Interface is hard to use because of a disability (font size, color contrast, screen-reader, etc.).  \n",
      "Billing Inquiry – Questions or confusion about charges, pricing, or payment amounts—BUT NOT when the user is asking for an invoice, a refund, a plan change, or to update card details (those have their own categories below).  \n",
      "Beta Program Enrollment – Joining, leaving, or checking status of an early-access/beta program.  \n",
      "Compliance Inquiry – Legal/regulatory questions (GDPR, HIPAA, etc.).  \n",
      "Data Export – Downloading or exporting personal or usage data.  \n",
      "Desktop App Issue – Installing, launching, or running the Windows/Mac/Linux app.  \n",
      "Email Notifications – Missing or unwanted system emails (order updates, alerts) that are NOT signup, password-reset, or 2FA messages.  \n",
      "Feature Access Issue – A feature exists and works for others but is missing or disabled for this user (eligibility, rollout, entitlement).  \n",
      "Feature Request – User explicitly asks to add, remove, or improve a capability/UI; not just a bug report or general praise/criticism.  \n",
      "General Feedback – Opinions, praise, or criticism without asking for a change.  \n",
      "Integration Help – Problems connecting or syncing with an external service (calendar, CRM, payment gateway).  \n",
      "Invoice Request – Wants a copy or correction of an invoice/receipt, or asks where to find past invoices.  \n",
      "Language Support – Needs the UI or content in another language or multiple languages.  \n",
      "Login Issues – Cannot sign in or stay signed in (wrong password, “invalid user”, etc.) but NOT if the message focuses on password reset or 2-factor codes.  \n",
      "Marketing Preferences – Opt in/out of promotional emails, SMS, etc.  \n",
      "Mobile App Issue – Installing, launching, or running the iOS/Android app.  \n",
      "Order Status – Normal “where is my order?” or tracking look-up (no unusual delay reported).  \n",
      "Payment Method Update – Add, remove, or change credit-card / PayPal details, split payments, etc.  \n",
      "Permission/Access Issue – Button or menu is greyed out/disabled until prerequisites or permissions are met (not a bug).  \n",
      "Privacy Policy Question – How data is collected, shared, sold, or deleted.  \n",
      "Product Return – Sending back or exchanging a received item; also trouble printing return labels.  \n",
      "Refund Request – Wants money back for a purchase or after cancellation/downgrade.  \n",
      "Security Concern – Possible hacking, suspicious logins, fraud, or account compromise.  \n",
      "Shipping Delay – Order stuck, tracking not moving, “delivered but not received,” or any shipping timeframe issue.  \n",
      "Subscription Upgrade/Downgrade – Change plan tier, switch to yearly/monthly, unexpected downgrade/upgrade.  \n",
      "Technical Bug Report – A function that SHOULD work is broken (crash, freeze, error), excluding categories above that have their own queues.  \n",
      "Terms of Service Question – Rights, usage rules, cancellations, or contract terms.  \n",
      "Two-Factor Authentication – Problems receiving or entering verification/OTP codes, lost 2FA device.  \n",
      "Warranty Claim – Questions or claims about warranty coverage or registration.\n",
      "\n",
      "Example snippets for tricky distinctions  \n",
      "• “Send me December’s invoice” → Invoice Request (not Billing Inquiry)  \n",
      "• “Charged twice” → Billing Inquiry (unless asking refund → Refund Request)  \n",
      "• “Paid for pro but still on free” → Subscription Upgrade/Downgrade  \n",
      "• “Code never arrives when logging in” → Two-Factor Authentication  \n",
      "• “Return label prints blank” → Product Return (label problem is part of return)  \n",
      "• “Button greyed out—how enable?” → Permission/Access Issue  \n",
      "• “Feature works for my colleague but not for me” → Feature Access Issue  \n",
      "• “Google Calendar sync deletes events” → Integration Help  \n",
      "• “Tracking says delivered but nothing here” → Shipping Delay\n",
      "\n",
      "Task input\n",
      "support query: {query}\n",
      "\n",
      "Task output  \n",
      "Return the single best category name exactly as written above, nothing else.\n",
      "🏋️ Train Accuracy: 0.7073170731707317 (Δ 0.1057)\n",
      "🧪 Test Accuracy: 0.8064516129032258 (Δ 0.1935)\n"
     ]
    }
   ],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompts\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\n🔍 Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"🏋️ Train Accuracy: {best_train_acc} (Δ {best_train_acc - initial_train_acc:.4f})\" if best_train_acc is not None else \"Train accuracy not available\")\n",
    "print(f\"🧪 Test Accuracy: {best_test_acc} (Δ {best_test_acc - initial_test_acc:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c5eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
