{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166be50e",
   "metadata": {},
   "source": [
    "# Arize AX: Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"600\">\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in datasets/support_queries.csv.\n",
    "\n",
    "**Note: This notebook `arizeax_support_query_classification.ipynb` complements `support_query_classification.ipynb` by using Arize Phoenix datasets, experiments, and prompt management for Prompt Learning. It's a more end to end way for you to visualize your iterative prompt improvement and see how it performs on train/test sets, and also leverages methods for advanced features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb715e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q arize-phoenix openai pandas arize gql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a2b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee7ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "openai_client = AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "ARIZE_API_KEY = getpass.getpass('ARIZE_API_KEY')\n",
    "ARIZE_SPACE_ID = getpass.getpass('ARIZE_SPACE_ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d31f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3bec",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a45a44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "import pandas as pd\n",
    "\n",
    "# add your Arize API key here\n",
    "client = ArizeDatasetsClient(api_key=ARIZE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463fac",
   "metadata": {},
   "source": [
    "## **Make train/test sets**\n",
    "\n",
    "We use an 80/20 train/test split to train our prompt. The optimizer will use the training set to visualize and analyze its errors and successes, and make prompt updates based on these results. We will then test on the test set to see how that prompt performs on unseen data. \n",
    "\n",
    "We will be exporting these datasets to Arize AX. In Arize you will be able to view the experiments we run on the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acf0fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset id: RGF0YXNldDozMDQyNDM6dmRqYw==\n",
      "test dataset id: RGF0YXNldDozMDQyNDQ6U0tTSw==\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "\n",
    "\n",
    "#train_set = data.sample(frac=0.8, random_state=42).sample(frac=0.02) #small sample test\n",
    "train_set = data.sample(frac=0.8, random_state=42) #full dataset test train 123 / test 31 examples\n",
    "test_set = data.drop(train_set.index)\n",
    "\n",
    "UUID = str(uuid1())[:8] #associate UUID with datasets and prompt name to track across runs\n",
    "TRAIN_DATASET_NAME = \"prompt_optimizer_training_data+\" + UUID\n",
    "TEST_DATASET_NAME = \"prompt_optimizer_test_data+\" + UUID\n",
    "\n",
    "train_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TRAIN_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=train_set,\n",
    "    )\n",
    "\n",
    "test_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TEST_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=test_set,\n",
    "    )\n",
    "\n",
    "# Get datasets as Dataframe\n",
    "train_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TRAIN_DATASET_NAME)\n",
    "test_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TEST_DATASET_NAME)\n",
    "\n",
    "print(\"train dataset id:\", train_dataset_id)\n",
    "print(\"test dataset id:\", test_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3184e0",
   "metadata": {},
   "source": [
    "## **Base Prompt for Optimization**\n",
    "\n",
    "This is our base prompt - our 0th iteration. This is the prompt we will be optimizing for our task.\n",
    "\n",
    "We also upload our prompt to Arize AX. Arize's Prompt Hub serves as a repository for your prompts. You will be able to view all iterations of your prompt as its optimized, along with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "902beeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q \"arize[PromptHub]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e78fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from phoenix.client.types import PromptVersion\n",
    "# Prompt Hub docs: https://arize.com/docs/ax/reference/reference/prompt-hub-api\n",
    "\n",
    "from arize.experimental.prompt_hub import ArizePromptClient, Prompt, LLMProvider\n",
    "\n",
    "# Initialize the client with your Arize credentials\n",
    "prompt_client = ArizePromptClient(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    api_key=ARIZE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "support query: {query}\n",
    "Account Creation\n",
    "Login Issues\n",
    "Password Reset\n",
    "Two-Factor Authentication\n",
    "Profile Updates\n",
    "Billing Inquiry\n",
    "Refund Request\n",
    "Subscription Upgrade/Downgrade\n",
    "Payment Method Update\n",
    "Invoice Request\n",
    "Order Status\n",
    "Shipping Delay\n",
    "Product Return\n",
    "Warranty Claim\n",
    "Technical Bug Report\n",
    "Feature Request\n",
    "Integration Help\n",
    "Data Export\n",
    "Security Concern\n",
    "Terms of Service Question\n",
    "Privacy Policy Question\n",
    "Compliance Inquiry\n",
    "Accessibility Support\n",
    "Language Support\n",
    "Mobile App Issue\n",
    "Desktop App Issue\n",
    "Email Notifications\n",
    "Marketing Preferences\n",
    "Beta Program Enrollment\n",
    "General Feedback\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_prompt_version(system_prompt, prompt_name, model_name, test_metric, loop_number):\n",
    "    try:\n",
    "        existing_prompt = prompt_client.pull_prompt(prompt_name=prompt_name)\n",
    "        existing_prompt.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} - Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "    except:\n",
    "        existing_prompt = Prompt(\n",
    "            name=prompt_name,\n",
    "            model_name=model_name,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
    "            provider=LLMProvider.OPENAI,\n",
    "        )\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} \\n Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21e149",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "691d829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task(system_prompt):\n",
    "\n",
    "    async def output_task(dataset_row):\n",
    "        formatted_prompt = system_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return output_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "091f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(label):\n",
    "        return label.strip().strip('\"').strip(\"'\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886cda",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1be0f",
   "metadata": {},
   "source": [
    "Our evaluator leverages llm_generate once again to build these llm evals with concurrency. We use an output parser to ensure that our eval is returned in proper json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a96edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#from phoenix.experiments.types import EvaluationResult\n",
    "from arize.experimental.datasets.experiments.evaluators.base import EvaluationResult\n",
    "\n",
    "\n",
    "\n",
    "def find_attributes(output):\n",
    "    patterns = {\n",
    "        \"correctness\": r'\"correctness\":\\s*\"([^\"]*)\"',\n",
    "        \"explanation\": r'\"explanation\":\\s*\"([^\"]*)\"',\n",
    "        \"confusion_reason\": r'\"confusion_reason\":\\s*\"([^\"]*)\"',\n",
    "        \"error_type\": r'\"error_type\":\\s*\"([^\"]*)\"',\n",
    "        \"evidence_span\": r'\"evidence_span\":\\s*\"([^\"]*)\"',\n",
    "        \"prompt_fix_suggestion\": r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    }\n",
    "\n",
    "    return tuple(\n",
    "        (match := re.search(pattern, output, re.IGNORECASE)) and match.group(1)\n",
    "        for pattern in patterns.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_parser(response: str) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "\n",
    "async def output_evaluator(dataset_row, output):\n",
    "    with open(\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{ground_truth}\", dataset_row.get(\"ground_truth\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{output}\", output)\n",
    "\n",
    "    eval_result = await openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluator_prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    response = eval_result.choices[0].message.content\n",
    "    parsed_eval_result = eval_parser(response)\n",
    "    explanation=f\"\"\"correctness: {parsed_eval_result.get(\"correctness\", \"\")};\n",
    "        explanation: {parsed_eval_result.get(\"explanation\", \"\")};\n",
    "        confusion_reason: {parsed_eval_result.get(\"confusion_reason\", \"\")};\n",
    "        error_type: {parsed_eval_result.get(\"error_type\", \"\")};\n",
    "        evidence_span: {parsed_eval_result.get(\"evidence_span\", \"\")};\n",
    "        prompt_fix_suggestion: {parsed_eval_result.get(\"prompt_fix_suggestion\", \"\")};\"\"\"\n",
    "\n",
    "    score = float(parsed_eval_result.get(\"correctness\") == \"correct\")\n",
    "    label = parsed_eval_result.get(\"correctness\", \"\")\n",
    "    explanation = explanation\n",
    "\n",
    "    return EvaluationResult(\n",
    "        score=score,\n",
    "        label=label,\n",
    "        explanation=explanation,\n",
    "    )\n",
    "\n",
    "async def test_evaluator(dataset_row, output):\n",
    "    label=str(normalize(dataset_row.get(\"ground_truth\")) == normalize(output))\n",
    "    return EvaluationResult(\n",
    "        label=label,\n",
    "        score = float(label==\"True\"),\n",
    "        explanation=\"placeholder\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ca3",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba30cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "def compute_metric(experiment_id, prediction_column_name, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested classification metric from a Arize experiment.\n",
    "    \"\"\"\n",
    "    experiment_df = client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    print(experiment_df.head())\n",
    "\n",
    "    y_pred = experiment_df[prediction_column_name]\n",
    "    y_true = [1]*len(experiment_df)\n",
    "\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9be50",
   "metadata": {},
   "source": [
    "## Experiment Processor\n",
    "\n",
    "This function pulls an Arize experiment and loads the data into a pandas dataframe so it can run through the optimizer.\n",
    "\n",
    "Specifically it:\n",
    "- Pulls the experiment data from Arize\n",
    "- Adds the input column to the dataframe\n",
    "- Adds the evals to the dataframe\n",
    "- Adds the output to the dataframe\n",
    "- Returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9dc4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(arize_client, experiment_id, train_set, input_column_name, output_column_name, feedback_columns = None):\n",
    "\n",
    "    experiment_df = arize_client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    train_set_with_experiment_results = pd.merge(train_set, experiment_df, left_on='id', right_on='example_id', how='inner')\n",
    "    \n",
    "    for column in feedback_columns:\n",
    "        train_set[column] = [None] * len(train_set)\n",
    "    \n",
    "    for idx, row in train_set_with_experiment_results.iterrows():\n",
    "        index = row[\"example_id\"]\n",
    "        eval_output = row[\"eval.output_evaluator.explanation\"]\n",
    "        if feedback_columns:\n",
    "            for item in eval_output.split(\";\"):\n",
    "                key_value = item.split(\":\")\n",
    "                if key_value[0].strip() in feedback_columns:\n",
    "                    key, value = key_value[0].strip(), key_value[1].strip()\n",
    "                    train_set.loc[train_set[\"id\"] == index, key] = value\n",
    "\n",
    "            \n",
    "\n",
    "    train_set[output_column_name] = train_set_with_experiment_results[\"result\"]\n",
    "\n",
    "    train_set.rename(columns={\"query\": input_column_name}, inplace=True)\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93dace",
   "metadata": {},
   "source": [
    "# Prompt Optimization Loop with Arize Experiments\n",
    "\n",
    "This code implements an iterative prompt optimization system that uses **Arize AX experiments** to evaluate and improve prompts based on feedback from LLM evaluators.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `optimize_loop` function automates prompt engineering by:\n",
    "\n",
    "- Evaluating prompts using Arize experiments  \n",
    "- Collecting detailed feedback from LLM evaluators  \n",
    "- Optimizing prompts via a learning-based optimizer  \n",
    "- Iterating until the performance threshold is met or the loop limit is reached  \n",
    "\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "Each of these numbers are added as comments in the code.\n",
    "\n",
    "### 1. Initialization\n",
    "\n",
    "- Set up tracking variables:\n",
    "  - `train_metrics`, `test_metrics`, `raw_dfs` for storing evaluation results\n",
    "- Convert training dataset to a DataFrame for easy updates\n",
    "\n",
    "### 2. Baseline Evaluation\n",
    "\n",
    "- Run an initial experiment using the **test set**\n",
    "- Establish a **baseline metric** (e.g., accuracy, F1) to compare against future improvements\n",
    "\n",
    "### 3. Early Exit Check\n",
    "\n",
    "- If the **initial prompt already meets the performance threshold**, skip further optimization to save time and compute\n",
    "\n",
    "### 4. Main Optimization Loop\n",
    "\n",
    "For each iteration (up to `loops`):\n",
    "\n",
    "#### 4a. Run Training Experiment\n",
    "\n",
    "- Execute the current prompt on the **training set**\n",
    "- Use LLM evaluators to generate **natural language feedback**\n",
    "\n",
    "#### 4b. Process Feedback\n",
    "\n",
    "- Extract structured information from evaluator outputs:\n",
    "  - Correctness\n",
    "  - Explanation\n",
    "  - Confusion reason\n",
    "  - Error type\n",
    "  - Prompt fix suggestions\n",
    "- Update the training DataFrame with this feedback\n",
    "\n",
    "#### 4c. Generate Learning Annotations\n",
    "\n",
    "- Convert feedback into structured annotations for the optimizer to learn from\n",
    "- This allows learning from evaluator insights in a consistent format\n",
    "\n",
    "#### 4d. Optimize the Prompt\n",
    "\n",
    "- Pass feedback to the **PromptLearningOptimizer**\n",
    "- Generate an **improved prompt** that attempts to correct issues found in the previous iteration\n",
    "\n",
    "#### 4e. Evaluate on Test Set\n",
    "\n",
    "- Evaluate the updated prompt on the **held-out test set**\n",
    "- Assess **generalization** beyond the training data\n",
    "\n",
    "#### 4f. Track Metrics\n",
    "\n",
    "- Log metrics for:\n",
    "  - Training set performance\n",
    "  - Test set performance\n",
    "- Store raw results for further analysis or visualization\n",
    "\n",
    "#### 4g. Convergence Check\n",
    "\n",
    "- If the new prompt's test metric **meets or exceeds the threshold**, exit the loop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be67e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting prompt optimization with 5 iterations (scorer: accuracy, threshold: 1)\n",
      "�� Initial evaluation:\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:27<00:00 |  1.22it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:10 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:29<00:00 |  1.06it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:18<00:00 |  1.63it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id            result  \\\n",
      "0  EXP_ID_a571db  e229198e-c98b-4dee-a5b8-a6f3f36d18e4  Account Creation   \n",
      "1  EXP_ID_97b182  0e780f04-2457-469c-9bc9-954db24b3583   Billing Inquiry   \n",
      "2  EXP_ID_7325f2  abe65d8c-297a-4759-b187-2f48cd510b3d      Order Status   \n",
      "3  EXP_ID_96fe1e  8287e65d-8fb9-470f-9bcf-cc68491f230a    Password Reset   \n",
      "4  EXP_ID_a0ab3d  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1      Login Issues   \n",
      "\n",
      "                    result.trace.id  result.trace.timestamp  \\\n",
      "0  503b767c1a09da8da1a540e281602b90           1756905015754   \n",
      "1  a33f37c414b7d8b665e6629a244ee5af           1756905016765   \n",
      "2  2d9bf1d868203b103ff11241b81ddd0c           1756905017755   \n",
      "3  029370f301629ef18760545795751f05           1756905018738   \n",
      "4  5277b47c30c9b29847dd989daef74b29           1756905019707   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        0.0                     False   \n",
      "2                        0.0                     False   \n",
      "3                        0.0                     False   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  f41ec274799bd2751dc0342d75801595   \n",
      "1                     placeholder  841bfa7746365e83ab32d68fe7f4c4b7   \n",
      "2                     placeholder  11da39365a2937152e242ae8652266ce   \n",
      "3                     placeholder  31bfbfd39e32e8c822e9bb281a07192a   \n",
      "4                     placeholder  6273a74a4bba9f2fc59635c8df50ff16   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756905042875  \n",
      "1                        1756905043001  \n",
      "2                        1756905043109  \n",
      "3                        1756905043203  \n",
      "4                        1756905043319  \n",
      "✅ Initial accuracy: 0.6129032258064516\n",
      "📊 Loop 1: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:20<00:00 |  2.41it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:12 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:21<00:00 |  1.51it/s\n",
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 00:55<00:00 |  1.89s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_9b66fb  dd17a2a0-2912-4d4c-bf7e-b3b3027b75fe   \n",
      "1  EXP_ID_41a1ad  c820437c-026e-4cf5-9e0a-1d190fa39fae   \n",
      "2  EXP_ID_6e739f  787aa17a-da44-4c75-b4fd-cc303be0bd56   \n",
      "3  EXP_ID_856820  36e38c54-b02b-4219-94d1-8da3bb5e14d9   \n",
      "4  EXP_ID_35cea8  ce75bd3c-9abe-4e4c-aaea-3f1fa1361886   \n",
      "\n",
      "                      result                   result.trace.id  \\\n",
      "0    Privacy Policy Question  cef78b4e554c06dc6fcb9759b87f8d0e   \n",
      "1            Billing Inquiry  1d0143c3ff57677d955029da9dabfb55   \n",
      "2  Return label prints blank  39fbdb431830a2a9618ad829fd9deae6   \n",
      "3            Billing Inquiry  f3e0b076a74b2a85cb3e8f9b16aa2ca2   \n",
      "4           General Feedback  f72a2e1f78425abacd5c196aa7500a87   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1756905074351                          1.0   \n",
      "1           1756905075341                          1.0   \n",
      "2           1756905076368                          0.0   \n",
      "3           1756905077354                          1.0   \n",
      "4           1756905078229                          0.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                   incorrect   \n",
      "3                     correct   \n",
      "4                   incorrect   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: incorrect;\\n        explanation: ...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: incorrect;\\n        explanation: ...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  debad9b4e8e7fbf7dc85fc02a59df734                          1756905154824   \n",
      "1  d6b0a3553b1858b72805951ff5d3d891                          1756905154904   \n",
      "2  de822b096d51c5649a7a1237e21d9213                          1756905154978   \n",
      "3  a7b8ad05b1dd43218fa724d18bb6e0be                          1756905155053   \n",
      "4  51422939fc2e003f884f3ac6f17e7239                          1756905155129   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        0.0                     False   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  e3ce94880d6d6e513a37413f12608b12   \n",
      "1                     placeholder  45ac669f8a120d016629fc666dc51975   \n",
      "2                     placeholder  d6c87ae564a8a1a186c08c233ab32996   \n",
      "3                     placeholder  2e8c48b0bf735bfbfbc79d5418fd87b8   \n",
      "4                     placeholder  6577f835c5ddb073157e1dab85c8a70c   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756905154827  \n",
      "1                        1756905154906  \n",
      "2                        1756905154982  \n",
      "3                        1756905155058  \n",
      "4                        1756905155133  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 01:16<00:00 |  3.22it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training accuracy: 0.5772357723577236\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:27<00:00 |  1.10s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:15 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:29<00:00 |  1.07it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:17<00:00 |  1.79it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id  \\\n",
      "0  EXP_ID_821972  e229198e-c98b-4dee-a5b8-a6f3f36d18e4   \n",
      "1  EXP_ID_cdfe5b  0e780f04-2457-469c-9bc9-954db24b3583   \n",
      "2  EXP_ID_18c746  abe65d8c-297a-4759-b187-2f48cd510b3d   \n",
      "3  EXP_ID_dca0d6  8287e65d-8fb9-470f-9bcf-cc68491f230a   \n",
      "4  EXP_ID_324a0c  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                    Login Issues  8ef1f6a5aaeab6793cf77ad12c20f09a   \n",
      "1  Subscription Upgrade/Downgrade  fbf1485120e77c40ae37a2587ff42bdc   \n",
      "2                     Data Export  d12803c1ec835fb429c15b6ff2ce7cbb   \n",
      "3                  Password Reset  bcb301b819adc1b749a932e492a3a102   \n",
      "4                    Login Issues  dda6f07d37517e0b77f234231c78f30d   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1756905288188                        0.0   \n",
      "1           1756905289239                        0.0   \n",
      "2           1756905290152                        1.0   \n",
      "3           1756905291183                        0.0   \n",
      "4           1756905294006                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  0196d3ce4bad886f7012710e66f01e73                        1756905315394  \n",
      "1  1ddbf0f8b15bb41880cda106269dfca0                        1756905315459  \n",
      "2  7230d74ce681810042458268960c2f43                        1756905315531  \n",
      "3  182fd8efae4159e20a19f0a1ad9f95ad                        1756905315611  \n",
      "4  100f9ec4f34ab6ba5c5c792bccc076b5                        1756905315690  \n",
      "✅ Test accuracy: 0.5483870967741935\n",
      "📊 Loop 2: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:20<00:00 |  2.28it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:17 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:21<00:00 |  1.51it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_cc8656  dd17a2a0-2912-4d4c-bf7e-b3b3027b75fe   \n",
      "1  EXP_ID_a92e3b  c820437c-026e-4cf5-9e0a-1d190fa39fae   \n",
      "2  EXP_ID_50d9e5  787aa17a-da44-4c75-b4fd-cc303be0bd56   \n",
      "3  EXP_ID_10e0c2  36e38c54-b02b-4219-94d1-8da3bb5e14d9   \n",
      "4  EXP_ID_63d2ae  ce75bd3c-9abe-4e4c-aaea-3f1fa1361886   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  e2fac03a0c9e7641f5fd38322a95fb85   \n",
      "1          Billing Inquiry  d71732cd94f7e31b93f2735ef6f27339   \n",
      "2     Technical Bug Report  cb31a4ff728acf07b8947a4df2ec5c60   \n",
      "3          Billing Inquiry  cf4477d59646e553a639734cafe62393   \n",
      "4          Feature Request  0b95ab486fb01caa2872551d24f37645   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1756905344864                          1.0   \n",
      "1           1756905345869                          1.0   \n",
      "2           1756905346811                          0.0   \n",
      "3           1756905347836                          1.0   \n",
      "4           1756905348784                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                   incorrect   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: incorrect;\\n        explanation: ...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  72e6492cd609e60b796b4951b8c492f1                          1756905425193   \n",
      "1  6ba82384429c5a8b916136604dc8b5cf                          1756905425278   \n",
      "2  5e3b858a144af8625ec89dcbc25340f4                          1756905425349   \n",
      "3  c051d3544857162adb9c668fa6c24256                          1756905425431   \n",
      "4  6d96e4eaa8ad29039639f364c6d68587                          1756905425511   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  a6e039d4b513c408d4c149024bb5868c   \n",
      "1                     placeholder  9f55a4325e95dcdd1d503baaafda9f66   \n",
      "2                     placeholder  6babdc4241812a0d5de999141e4b7c37   \n",
      "3                     placeholder  6c866dc45d3fd27ed7e9feb468526bd1   \n",
      "4                     placeholder  fff334454606f0c36907361d6c86c326   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756905425195  \n",
      "1                        1756905425279  \n",
      "2                        1756905425354  \n",
      "3                        1756905425435  \n",
      "4                        1756905425515  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 01:04<00:00 |  3.81it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training accuracy: 0.7723577235772358\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:27<00:00 |  1.13it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:19 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:29<00:00 |  1.06it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:03<00:00 |  8.68it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id  \\\n",
      "0  EXP_ID_2261c4  e229198e-c98b-4dee-a5b8-a6f3f36d18e4   \n",
      "1  EXP_ID_ab4f1f  0e780f04-2457-469c-9bc9-954db24b3583   \n",
      "2  EXP_ID_8e6039  abe65d8c-297a-4759-b187-2f48cd510b3d   \n",
      "3  EXP_ID_fb7e4b  8287e65d-8fb9-470f-9bcf-cc68491f230a   \n",
      "4  EXP_ID_bd75a0  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                    Login Issues  1a967a4db8659598294fd0104bd0ec28   \n",
      "1  Subscription Upgrade/Downgrade  29c8ecdc3c251b454c145087fa4546c1   \n",
      "2                     Data Export  64781d712b854c281d8ca4a0c9684ecd   \n",
      "3                  Password Reset  b731c5b4068400b50c8d99abcd03ea31   \n",
      "4                    Login Issues  c4f9ade1c49198f8809f626afc743198   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1756905545045                        0.0   \n",
      "1           1756905546218                        0.0   \n",
      "2           1756905547009                        1.0   \n",
      "3           1756905548053                        0.0   \n",
      "4           1756905548924                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  4b20ac7e0bb0888288863dd364400417                        1756905572421  \n",
      "1  2bad86f7b224179dcbed783fa6f20f55                        1756905572502  \n",
      "2  5dc1b75aecd25f97ebb2cdffdbd4b447                        1756905572583  \n",
      "3  819e043bf3e58c54f200bc2ded9e0100                        1756905572682  \n",
      "4  9461a2fe40027bcc7c60c8856801d560                        1756905572762  \n",
      "✅ Test accuracy: 0.6451612903225806\n",
      "📊 Loop 3: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:20<00:00 |  1.26it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:21 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:21<00:00 |  1.51it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_280a6b  dd17a2a0-2912-4d4c-bf7e-b3b3027b75fe   \n",
      "1  EXP_ID_ea1bb3  c820437c-026e-4cf5-9e0a-1d190fa39fae   \n",
      "2  EXP_ID_a8d783  787aa17a-da44-4c75-b4fd-cc303be0bd56   \n",
      "3  EXP_ID_f6c57f  36e38c54-b02b-4219-94d1-8da3bb5e14d9   \n",
      "4  EXP_ID_a73937  ce75bd3c-9abe-4e4c-aaea-3f1fa1361886   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  7c504594577d69930848c19ef17cecf2   \n",
      "1          Billing Inquiry  1738f24c66f0647f8863d31efcea45a3   \n",
      "2       **Product Return**  fa46c4bb5d6b416228f04664bc8486d7   \n",
      "3          Billing Inquiry  66c31275028840e540d4977e2702e387   \n",
      "4          Feature Request  f9ff934cac9192e3d2f99c5db4a937c5   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1756905602392                          1.0   \n",
      "1           1756905603535                          1.0   \n",
      "2           1756905604424                          1.0   \n",
      "3           1756905605304                          1.0   \n",
      "4           1756905608210                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                     correct   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: correct;\\n        explanation: Th...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  b39d91cbff173390f8f8931bea520645                          1756905682784   \n",
      "1  07b64855ec3fb28b35a7f4a90584217a                          1756905682859   \n",
      "2  9010961dc83a367f4b4972d2a8520348                          1756905682937   \n",
      "3  cd5fd96232c4f7ea556501b44752d318                          1756905683016   \n",
      "4  f173fea6a3121399b1ccca2afc66d33c                          1756905683099   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  ea55a6d0bb4dbcc65d61be4e0b553623   \n",
      "1                     placeholder  28537931d9e5d0a47d30ca1f226bf6fd   \n",
      "2                     placeholder  de61b5ac773ee51e6a9eb3c9e8a95cfc   \n",
      "3                     placeholder  aef724d09f5581afd4118ba5ac109db2   \n",
      "4                     placeholder  0f556117248afe49f8b8c93122fa22e8   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756905682786  \n",
      "1                        1756905682860  \n",
      "2                        1756905682939  \n",
      "3                        1756905683021  \n",
      "4                        1756905683103  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 01:00<00:00 |  4.05it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training accuracy: 0.7967479674796748\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:27<00:00 |  1.31it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:23 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:29<00:00 |  1.05it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:03<00:00 |  8.96it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id  \\\n",
      "0  EXP_ID_da0286  e229198e-c98b-4dee-a5b8-a6f3f36d18e4   \n",
      "1  EXP_ID_285fea  0e780f04-2457-469c-9bc9-954db24b3583   \n",
      "2  EXP_ID_3e8095  abe65d8c-297a-4759-b187-2f48cd510b3d   \n",
      "3  EXP_ID_3d21a8  8287e65d-8fb9-470f-9bcf-cc68491f230a   \n",
      "4  EXP_ID_1896a9  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1   \n",
      "\n",
      "                               result                   result.trace.id  \\\n",
      "0                        Login Issues  9312b7cd948a832424c244402e261dd3   \n",
      "1  **Subscription Upgrade/Downgrade**  c74967c366f29051aacfcd73579e92f6   \n",
      "2                         Data Export  20552b811a6cf3980c971ee9a83a4cb4   \n",
      "3                      Password Reset  13ac8580fe6a1d1a970ca35d70c65688   \n",
      "4                        Login Issues  62700bb18a0bf97bbab76ab1f8706cab   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1756905795072                        0.0   \n",
      "1           1756905796068                        0.0   \n",
      "2           1756905797104                        1.0   \n",
      "3           1756905798054                        0.0   \n",
      "4           1756905798976                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  f374d1c44937abbe7ba461c80bf67141                        1756905822628  \n",
      "1  14a584517d509c6eed2264aec132e212                        1756905822710  \n",
      "2  7dd9ba6fc3cab8e743274576c5367d70                        1756905822797  \n",
      "3  e4cbb5f20e07f9b96337f6fba289cd0e                        1756905822871  \n",
      "4  4b2b73e84b1985905e1c57bccab213b0                        1756905822949  \n",
      "✅ Test accuracy: 0.4838709677419355\n",
      "📊 Loop 4: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:20<00:00 |  2.32it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:25 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:21<00:00 |  1.50it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_c13b46  dd17a2a0-2912-4d4c-bf7e-b3b3027b75fe   \n",
      "1  EXP_ID_a44f6d  c820437c-026e-4cf5-9e0a-1d190fa39fae   \n",
      "2  EXP_ID_8be281  787aa17a-da44-4c75-b4fd-cc303be0bd56   \n",
      "3  EXP_ID_4778d0  36e38c54-b02b-4219-94d1-8da3bb5e14d9   \n",
      "4  EXP_ID_921271  ce75bd3c-9abe-4e4c-aaea-3f1fa1361886   \n",
      "\n",
      "                        result                   result.trace.id  \\\n",
      "0  **Privacy Policy Question**  a3f5ca6ead158a43e56fff8bf830971b   \n",
      "1          **Billing Inquiry**  8945406b1fac08b576bcf6804309e004   \n",
      "2           **Product Return**  0d7765ab60304021fe36e8b08dca337c   \n",
      "3          **Billing Inquiry**  b43f27f77812dec7d8932dfc1a533e64   \n",
      "4          **Feature Request**  bc63716b4fffb357890a15ca9692695b   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1756905852002                          1.0   \n",
      "1           1756905853128                          1.0   \n",
      "2           1756905853973                          1.0   \n",
      "3           1756905854953                          1.0   \n",
      "4           1756905855943                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                     correct   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: correct;\\n        explanation: Th...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  b98ebc5be125af757fceac8e2e6d71d9                          1756905932819   \n",
      "1  b2c07f31c5d1c0ff44ab7900f23013b3                          1756905932903   \n",
      "2  64deef1ec5d3899ff8f61fd482bb1322                          1756905933014   \n",
      "3  0687fe3ad82bd15159bf61ba2b124053                          1756905933091   \n",
      "4  2f579d765fae5590d93371cafd11e766                          1756905933171   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        0.0                     False   \n",
      "1                        0.0                     False   \n",
      "2                        0.0                     False   \n",
      "3                        0.0                     False   \n",
      "4                        0.0                     False   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  bdbc63932481983770d242a35f1376bd   \n",
      "1                     placeholder  9faec5fb685a62b46da58dfaa507574a   \n",
      "2                     placeholder  7d6e42c53f81f0c15d6143047cdec0e4   \n",
      "3                     placeholder  559ca6f4b5baca1b2c3155709fc0c52a   \n",
      "4                     placeholder  8c7c15b360568dccecf0945deabbac98   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756905932821  \n",
      "1                        1756905932905  \n",
      "2                        1756905933019  \n",
      "3                        1756905933095  \n",
      "4                        1756905933174  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 01:03<00:00 |  3.88it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training accuracy: 0.8292682926829268\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:26<00:00 |  1.18it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:27 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:28<00:00 |  1.10it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:16<00:00 |  1.91it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id  \\\n",
      "0  EXP_ID_482a23  e229198e-c98b-4dee-a5b8-a6f3f36d18e4   \n",
      "1  EXP_ID_91969e  0e780f04-2457-469c-9bc9-954db24b3583   \n",
      "2  EXP_ID_a2c4a3  abe65d8c-297a-4759-b187-2f48cd510b3d   \n",
      "3  EXP_ID_1abacc  8287e65d-8fb9-470f-9bcf-cc68491f230a   \n",
      "4  EXP_ID_a51a15  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                    Login Issues  2f97ae33e5b4ffe9f7f50da6c5089f1a   \n",
      "1  Subscription Upgrade/Downgrade  71d71ce8ed8a3df4696bac06a9457f53   \n",
      "2                     Data Export  b9c699da6ac2b22c8a3d27ff5376d8ee   \n",
      "3                  Password Reset  2a22b58c8cbe61e9a0dd4bdddf174037   \n",
      "4                    Login Issues  d11bd95ffa2564bf782d84aadcb0e171   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1756906050492                        0.0   \n",
      "1           1756906051558                        0.0   \n",
      "2           1756906052501                        1.0   \n",
      "3           1756906054450                        0.0   \n",
      "4           1756906055347                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  92e4df8147f25f1e4bb0aa4e1c223fd5                        1756906076756  \n",
      "1  659d779b6152c5fac4394c601961b5c4                        1756906076864  \n",
      "2  0d86510a4cfebd8537973ae516a1a04b                        1756906076936  \n",
      "3  ca476acc1300a8d1928b50d1340613ae                        1756906077026  \n",
      "4  c4aa7eab5911b0dd511a39e0941079db                        1756906077109  \n",
      "✅ Test accuracy: 0.5806451612903226\n",
      "📊 Loop 5: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:20<00:00 |  2.44it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:29 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 123/123 (100.0%) | ⏳ 01:21<00:00 |  1.51it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_c7b2e6  dd17a2a0-2912-4d4c-bf7e-b3b3027b75fe   \n",
      "1  EXP_ID_456c75  c820437c-026e-4cf5-9e0a-1d190fa39fae   \n",
      "2  EXP_ID_1529e6  787aa17a-da44-4c75-b4fd-cc303be0bd56   \n",
      "3  EXP_ID_cfae9d  36e38c54-b02b-4219-94d1-8da3bb5e14d9   \n",
      "4  EXP_ID_aaa517  ce75bd3c-9abe-4e4c-aaea-3f1fa1361886   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  91b6ec2bf1eded6fb42939356b792c74   \n",
      "1          Billing Inquiry  7b65b544896bfc7ab59968096bc4014f   \n",
      "2       **Product Return**  c9e22a245f9b4e5f8a15e3a5206f4850   \n",
      "3          Billing Inquiry  39906e374314920e46969e57f89052b3   \n",
      "4          Feature Request  2cdbd3896a94862ab3b2a70f362fd3a1   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1756906106208                          1.0   \n",
      "1           1756906107240                          1.0   \n",
      "2           1756906108210                          1.0   \n",
      "3           1756906109132                          1.0   \n",
      "4           1756906110133                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                     correct   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: correct;\\n        explanation: Th...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  281724994b5b0281627f98bb5531fef0                          1756906186423   \n",
      "1  4aff9b3dcf136bcd1f27379966bda3bb                          1756906186508   \n",
      "2  ed2849d87ad2ad9d5c5bd2f1f5beed53                          1756906186586   \n",
      "3  832e8be56da6f5b740ed86d7d0a8e192                          1756906186659   \n",
      "4  a75ef3c513048e9facb9f86d4c588cb0                          1756906186750   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  7be48e1df79272e915b74dca2bac7897   \n",
      "1                     placeholder  0d9e4d7136eb396504985a728870d7fd   \n",
      "2                     placeholder  23775fcc87cc433ef34c2b4d94ddde07   \n",
      "3                     placeholder  c4e46aba3b1506771fa271ba210f58e6   \n",
      "4                     placeholder  f0874563b1e212d22dada526e463cd17   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1756906186425  \n",
      "1                        1756906186509  \n",
      "2                        1756906186590  \n",
      "3                        1756906186663  \n",
      "4                        1756906186753  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 246/246 (100.0%) | ⏳ 01:05<00:00 |  3.77it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training accuracy: 0.8130081300813008\n",
      "🔍 Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "🔧 Creating batches with 90,000 token limit\n",
      "📊 Processing 123 examples in 1 batches\n",
      "   ✅ Batch 1/1: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | 🧪 Experiment started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:27<00:00 |  1.15it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ Task runs completed.\n",
      "Tasks Summary (09/03/25 06:32 AM -0700)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 31/31 (100.0%) | ⏳ 00:29<00:00 |  1.06it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ✅ All evaluators completed.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 31/31 (100.0%) | ⏳ 00:17<00:00 |  1.81it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                            example_id  \\\n",
      "0  EXP_ID_83044b  e229198e-c98b-4dee-a5b8-a6f3f36d18e4   \n",
      "1  EXP_ID_52ef60  0e780f04-2457-469c-9bc9-954db24b3583   \n",
      "2  EXP_ID_d1e5b6  abe65d8c-297a-4759-b187-2f48cd510b3d   \n",
      "3  EXP_ID_709e32  8287e65d-8fb9-470f-9bcf-cc68491f230a   \n",
      "4  EXP_ID_69a659  8fc5bd05-f4fa-41b4-894e-2d76955a5cf1   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                    Login Issues  35bd4b72241ec8aa9db44eb730deda39   \n",
      "1  Subscription Upgrade/Downgrade  a0bd73ae0f449c51d116700e0dc7c267   \n",
      "2                     Data Export  a547a17c4d280bb4e518008f0e28a6e4   \n",
      "3                  Password Reset  e08015917c6acf5199dbe40e55d463c2   \n",
      "4                    Login Issues  2fa27b32aeee195eb139f26f3b4ee918   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1756906320028                        0.0   \n",
      "1           1756906321039                        0.0   \n",
      "2           1756906321997                        1.0   \n",
      "3           1756906322968                        0.0   \n",
      "4           1756906323983                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  6e1d473957f18fb9470bc9c36da08db9                        1756906347459  \n",
      "1  929f2526d03aac70c101f63ab3eed51c                        1756906347535  \n",
      "2  9e7fbf6dc56defae863e7a6f4d95ac8f                        1756906347630  \n",
      "3  05e28dc0576349c457bedee1541d59fd                        1756906347704  \n",
      "4  226fb91e1e1ee7e82ebdecb10f379284                        1756906347780  \n",
      "✅ Test accuracy: 0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "#prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name = \"prompt_optimizer+\" + UUID\n",
    "\n",
    "def optimize_loop(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    system_prompt,\n",
    "    feedback_columns,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "    prompt_versions=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    \"\"\"\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    raw_dfs = []\n",
    "    train_df = train_dataset\n",
    "\n",
    "    print(f\"🚀 Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"�� Initial evaluation:\")\n",
    "\n",
    "    task = generate_task(system_prompt)\n",
    "\n",
    "    initial_experiment_id, _ = client.run_experiment(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_id=test_dataset_id,\n",
    "        task=task,\n",
    "        evaluators=[test_evaluator],\n",
    "        experiment_name=\"initial_experiment_1\",\n",
    "        concurrency=10\n",
    "    )\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    initial_metric_value = compute_metric(initial_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "    print(f\"✅ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "    prompt_versions.append(system_prompt)\n",
    "\n",
    "    add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", initial_metric_value, 0)\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"🎉 Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompt_versions,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    # Initialize all feedback columns\n",
    "\n",
    "    while loops > 0:\n",
    "        print(f\"📊 Loop {curr_loop}: Optimizing prompt...\")\n",
    "        \n",
    "        task = generate_task(system_prompt)\n",
    "\n",
    "        train_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=train_dataset_id,\n",
    "            task=task,\n",
    "            evaluators=[output_evaluator, test_evaluator],\n",
    "            experiment_name=f\"train_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        train_metric = compute_metric(train_experiment_id, \"eval.output_evaluator.score\", \"accuracy\")\n",
    "        train_metrics.append(train_metric)\n",
    "        train_df = process_experiment(client, train_experiment_id, train_df, \"query\", \"output\", feedback_columns)\n",
    "        print(f\"✅ Training {scorer}: {train_metric}\")\n",
    "        \n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,  \n",
    "            model_choice=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_df,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_df,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "        prompt_versions.append(system_prompt)\n",
    "\n",
    "        \n",
    "        test_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=test_dataset_id,\n",
    "            task=generate_task(system_prompt),\n",
    "            evaluators=[test_evaluator],\n",
    "            experiment_name=f\"test_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        test_metric = compute_metric(test_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "        test_metrics.append(test_metric)\n",
    "\n",
    "        add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", test_metric, curr_loop)\n",
    "        print(f\"✅ Test {scorer}: {test_metric}\")\n",
    "\n",
    "        if test_metric >= threshold:\n",
    "            print(\"🎉 Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"prompts\": prompt_versions,\n",
    "        \"raw\": raw_dfs\n",
    "    }\n",
    "\n",
    "# Main execution - use asyncio.run() to run the async function\n",
    "evaluators = [output_evaluator]\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "result = optimize_loop(train_dataset, test_dataset, system_prompt, feedback_columns, loops=5, scorer=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7627",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd42533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Best Prompt Found:\n",
      "\n",
      "support query: {query}\n",
      "Account Creation\n",
      "Login Issues\n",
      "Password Reset\n",
      "Two-Factor Authentication\n",
      "Profile Updates\n",
      "Billing Inquiry\n",
      "Refund Request\n",
      "Subscription Upgrade/Downgrade\n",
      "Payment Method Update\n",
      "Invoice Request\n",
      "Order Status\n",
      "Shipping Delay\n",
      "Product Return\n",
      "Warranty Claim\n",
      "Technical Bug Report\n",
      "Feature Request\n",
      "Integration Help\n",
      "Data Export\n",
      "Security Concern\n",
      "Terms of Service Question\n",
      "Privacy Policy Question\n",
      "Compliance Inquiry\n",
      "Accessibility Support\n",
      "Language Support\n",
      "Mobile App Issue\n",
      "Desktop App Issue\n",
      "Email Notifications\n",
      "Marketing Preferences\n",
      "Beta Program Enrollment\n",
      "General Feedback\n",
      "\n",
      "Return just the category, no other text.\n",
      "\n",
      "🧪 Initial Test Accuracy: 0.5806451612903226\n",
      "🧪 Optimized Test Accuracy: 0.7096774193548387 (Δ 0.1290)\n"
     ]
    }
   ],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompts\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\n🔍 Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"🧪 Initial Test Accuracy: {initial_test_acc}\")\n",
    "print(f\"🧪 Optimized Test Accuracy: {best_test_acc} (Δ {best_test_acc - initial_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
