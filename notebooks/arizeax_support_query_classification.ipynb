{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166be50e",
   "metadata": {},
   "source": [
    "# Arize AX: Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"600\">\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in datasets/support_queries.csv.\n",
    "\n",
    "**Note: This notebook `arizeax_support_query_classification.ipynb` complements `support_query_classification.ipynb` by using Arize Phoenix datasets, experiments, and prompt management for Prompt Learning. It's a more end to end way for you to visualize your iterative prompt improvement and see how it performs on train/test sets, and also leverages methods for advanced features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bb715e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q arize-phoenix openai pandas arize gql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "74a2b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "openai_client = AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "ARIZE_API_KEY = getpass.getpass('ARIZE_API_KEY')\n",
    "ARIZE_SPACE_ID = getpass.getpass('ARIZE_SPACE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9d31f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3bec",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a45a44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "import pandas as pd\n",
    "\n",
    "# add your Arize API key here\n",
    "client = ArizeDatasetsClient(api_key=ARIZE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463fac",
   "metadata": {},
   "source": [
    "## **Make train/test sets**\n",
    "\n",
    "We use an 80/20 train/test split to train our prompt. The optimizer will use the training set to visualize and analyze its errors and successes, and make prompt updates based on these results. We will then test on the test set to see how that prompt performs on unseen data. \n",
    "\n",
    "We will be exporting these datasets to Arize AX. In Arize you will be able to view the experiments we run on the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acf0fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset id: RGF0YXNldDozMjM2ODM6bmZ1Qw==\n",
      "test dataset id: RGF0YXNldDozMjM2ODQ6QURmRA==\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "\n",
    "\n",
    "#train_set = data.sample(frac=0.8, random_state=42).sample(frac=0.02) #small sample test\n",
    "train_set = data.sample(frac=0.8, random_state=42) #full dataset test train 123 / test 31 examples\n",
    "test_set = data.drop(train_set.index)\n",
    "\n",
    "UUID = str(uuid1())[:8] #associate UUID with datasets and prompt name to track across runs\n",
    "TRAIN_DATASET_NAME = \"prompt_optimizer_training_data+\" + UUID\n",
    "TEST_DATASET_NAME = \"prompt_optimizer_test_data+\" + UUID\n",
    "\n",
    "train_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TRAIN_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=train_set,\n",
    "    )\n",
    "\n",
    "test_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TEST_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=test_set,\n",
    "    )\n",
    "\n",
    "# Get datasets as Dataframe\n",
    "train_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TRAIN_DATASET_NAME)\n",
    "test_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TEST_DATASET_NAME)\n",
    "\n",
    "print(\"train dataset id:\", train_dataset_id)\n",
    "print(\"test dataset id:\", test_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3184e0",
   "metadata": {},
   "source": [
    "## **Base Prompt for Optimization**\n",
    "\n",
    "This is our base prompt - our 0th iteration. This is the prompt we will be optimizing for our task.\n",
    "\n",
    "We also upload our prompt to Arize AX. Arize's Prompt Hub serves as a repository for your prompts. You will be able to view all iterations of your prompt as its optimized, along with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "902beeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q \"arize[PromptHub]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9e78fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from phoenix.client.types import PromptVersion\n",
    "# Prompt Hub docs: https://arize.com/docs/ax/reference/reference/prompt-hub-api\n",
    "\n",
    "from arize.experimental.prompt_hub import ArizePromptClient, Prompt, LLMProvider\n",
    "\n",
    "# Initialize the client with your Arize credentials\n",
    "prompt_client = ArizePromptClient(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    api_key=ARIZE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "support query: {query}\n",
    "Account Creation\n",
    "Login Issues\n",
    "Password Reset\n",
    "Two-Factor Authentication\n",
    "Profile Updates\n",
    "Billing Inquiry\n",
    "Refund Request\n",
    "Subscription Upgrade/Downgrade\n",
    "Payment Method Update\n",
    "Invoice Request\n",
    "Order Status\n",
    "Shipping Delay\n",
    "Product Return\n",
    "Warranty Claim\n",
    "Technical Bug Report\n",
    "Feature Request\n",
    "Integration Help\n",
    "Data Export\n",
    "Security Concern\n",
    "Terms of Service Question\n",
    "Privacy Policy Question\n",
    "Compliance Inquiry\n",
    "Accessibility Support\n",
    "Language Support\n",
    "Mobile App Issue\n",
    "Desktop App Issue\n",
    "Email Notifications\n",
    "Marketing Preferences\n",
    "Beta Program Enrollment\n",
    "General Feedback\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_prompt_version(system_prompt, prompt_name, model_name, test_metric, loop_number):\n",
    "    try:\n",
    "        existing_prompt = prompt_client.pull_prompt(prompt_name=prompt_name)\n",
    "        existing_prompt.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} - Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "    except:\n",
    "        existing_prompt = Prompt(\n",
    "            name=prompt_name,\n",
    "            model_name=model_name,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
    "            provider=LLMProvider.OPENAI,\n",
    "        )\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} \\n Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21e149",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "691d829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task(system_prompt):\n",
    "\n",
    "    async def output_task(dataset_row):\n",
    "        formatted_prompt = system_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return output_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "091f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(label):\n",
    "        return label.strip().strip('\"').strip(\"'\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886cda",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1be0f",
   "metadata": {},
   "source": [
    "Our evaluator leverages llm_generate once again to build these llm evals with concurrency. We use an output parser to ensure that our eval is returned in proper json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a96edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#from phoenix.experiments.types import EvaluationResult\n",
    "from arize.experimental.datasets.experiments.evaluators.base import EvaluationResult\n",
    "\n",
    "\n",
    "\n",
    "def find_attributes(output):\n",
    "    patterns = {\n",
    "        \"correctness\": r'\"correctness\":\\s*\"([^\"]*)\"',\n",
    "        \"explanation\": r'\"explanation\":\\s*\"([^\"]*)\"',\n",
    "        \"confusion_reason\": r'\"confusion_reason\":\\s*\"([^\"]*)\"',\n",
    "        \"error_type\": r'\"error_type\":\\s*\"([^\"]*)\"',\n",
    "        \"evidence_span\": r'\"evidence_span\":\\s*\"([^\"]*)\"',\n",
    "        \"prompt_fix_suggestion\": r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    }\n",
    "\n",
    "    return tuple(\n",
    "        (match := re.search(pattern, output, re.IGNORECASE)) and match.group(1)\n",
    "        for pattern in patterns.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_parser(response: str) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "\n",
    "async def output_evaluator(dataset_row, output):\n",
    "    with open(\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{ground_truth}\", dataset_row.get(\"ground_truth\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{output}\", output)\n",
    "\n",
    "    eval_result = await openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluator_prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    response = eval_result.choices[0].message.content\n",
    "    parsed_eval_result = eval_parser(response)\n",
    "    explanation=f\"\"\"correctness: {parsed_eval_result.get(\"correctness\", \"\")};\n",
    "        explanation: {parsed_eval_result.get(\"explanation\", \"\")};\n",
    "        confusion_reason: {parsed_eval_result.get(\"confusion_reason\", \"\")};\n",
    "        error_type: {parsed_eval_result.get(\"error_type\", \"\")};\n",
    "        evidence_span: {parsed_eval_result.get(\"evidence_span\", \"\")};\n",
    "        prompt_fix_suggestion: {parsed_eval_result.get(\"prompt_fix_suggestion\", \"\")};\"\"\"\n",
    "\n",
    "    score = float(parsed_eval_result.get(\"correctness\") == \"correct\")\n",
    "    label = parsed_eval_result.get(\"correctness\", \"\")\n",
    "    explanation = explanation\n",
    "\n",
    "    return EvaluationResult(\n",
    "        score=score,\n",
    "        label=label,\n",
    "        explanation=explanation,\n",
    "    )\n",
    "\n",
    "async def test_evaluator(dataset_row, output):\n",
    "    label=str(normalize(dataset_row.get(\"ground_truth\")) == normalize(output))\n",
    "    return EvaluationResult(\n",
    "        label=label,\n",
    "        score = float(label==\"True\"),\n",
    "        explanation=\"placeholder\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ca3",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba30cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "def compute_metric(experiment_id, prediction_column_name, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested classification metric from a Arize experiment.\n",
    "    \"\"\"\n",
    "    experiment_df = client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    print(experiment_df.head())\n",
    "\n",
    "    y_pred = experiment_df[prediction_column_name]\n",
    "    y_true = [1]*len(experiment_df)\n",
    "\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9be50",
   "metadata": {},
   "source": [
    "## Experiment Processor\n",
    "\n",
    "This function pulls an Arize experiment and loads the data into a pandas dataframe so it can run through the optimizer.\n",
    "\n",
    "Specifically it:\n",
    "- Pulls the experiment data from Arize\n",
    "- Adds the input column to the dataframe\n",
    "- Adds the evals to the dataframe\n",
    "- Adds the output to the dataframe\n",
    "- Returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a9dc4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(arize_client, experiment_id, train_set, input_column_name, output_column_name, feedback_columns = None):\n",
    "\n",
    "    experiment_df = arize_client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    train_set_with_experiment_results = pd.merge(train_set, experiment_df, left_on='id', right_on='example_id', how='inner')\n",
    "    \n",
    "    for column in feedback_columns:\n",
    "        train_set[column] = [None] * len(train_set)\n",
    "    \n",
    "    for idx, row in train_set_with_experiment_results.iterrows():\n",
    "        index = row[\"example_id\"]\n",
    "        eval_output = row[\"eval.output_evaluator.explanation\"]\n",
    "        if feedback_columns:\n",
    "            for item in eval_output.split(\";\"):\n",
    "                key_value = item.split(\":\")\n",
    "                if key_value[0].strip() in feedback_columns:\n",
    "                    key, value = key_value[0].strip(), key_value[1].strip()\n",
    "                    train_set.loc[train_set[\"id\"] == index, key] = value\n",
    "\n",
    "            \n",
    "\n",
    "    train_set[output_column_name] = train_set_with_experiment_results[\"result\"]\n",
    "\n",
    "    train_set.rename(columns={\"query\": input_column_name}, inplace=True)\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c953b6",
   "metadata": {},
   "source": [
    "# Optimization with Inbult Looping and Random Sampling\n",
    "- This cell demonstrates use of the new optimize_with_experiments workflow for prompt optimization,\n",
    "- including module reloading and running train/test experiments with full iterative evaluation and Prompt Hub integration.\n",
    "- This new workflow is still in a testing phase and is expected to eventually replace the previous optimize method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8bc9df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting prompt optimization with 20 iterations (scorer: accuracy, threshold: 1.0)\n",
      "\n",
      "üìä Initial evaluation:\n",
      "‚è≥ Running initial evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce78c69d971a41b7a212d5a86e083f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:54 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c5b3b7fc504bf1a934a1100d3cfcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving experiment results...\n",
      "‚úÖ Initial accuracy: 0.613\n",
      "‚è≥ Uploading initial prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "üîß Calculated max rows for context: 25 rows fit within 1,250 tokens\n",
      "üî¢ Maximum number of rows for training context: 25\n",
      "üîß EFFICIENCY SETUP: Will sample 25 examples from 123 total training examples each loop\n",
      "   üí∞ This saves ~98 evaluator calls per loop!\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 1: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f147f1b1ca441cc8b5bd3f94368e163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:55 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36330b1339b144d19d266c5f34d2b766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62263759d7d24f41b6e3ba8ee9f17d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:55 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272e28e063544485809dcfc1b9d89ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.613\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 2: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4fe9b1cd40418d83c8cf45e7b1c13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:56 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36247ceba6144a6e96020ff2e209a293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7191b9bbb0b4e72b46ba32acd1ab16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:57 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4091434e206644f590715896a6714056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.613\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 3: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70ead6082374a7db049149e996e33fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:57 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a112d7c6cb444b4b6752aedb102c6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.840\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb18df392d54e2cb259eee14817a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:58 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c84eba5def8494bba04d1cf9fc2a99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.677\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 4: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcd216745544fd3a1bcb973b8e41fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:59 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2840b0a1f64015b5be38c835fbe28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.640\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeed1089231446f196dd7b70d4393615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:59 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6c1b1adf624494be26b4838dac36f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 5: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5549176039f4574863ff91189e27ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:00 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25d9ec4eab949b897523d126434514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.680\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b79176e8cf472b919932146b35bb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:01 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7217c17708415fa0ebe573fc467e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 6: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a099728d244a499ef6b4602767bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:01 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51719ffba3f24a8fbcfe8d2a865fb855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e2f605da04a468c5d4ee0e8f2e779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:02 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525847293a4343b0be3fe9c264f24835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 7: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a48fce5b2fb414897a01651c528d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:02 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f279e3f3e449a0851d7f200e94d58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943ff85667b64fd3aaf0e630fc5e0668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:03 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70564c6baf8a46d8aef10e08451a753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 8: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff1ed0c5fb4e2b8a7f183a5206c9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:04 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c649bf4fe254af89696fc4177b6362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.640\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feabe790795847dcb43eb8c241a307f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:05 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac792e8eca7477792b38087c84fc6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.774\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 9: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5e8a1b02ff40f89f12d93ab0949ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:05 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2b816bc2b4406d8efec0dafecea653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.680\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b378d246f1344eabeece968db2bb4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:06 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f118b9adde410391116e5e0ef1bdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.774\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 10: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc782a80bcce41e2929396e1ebe93dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:07 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b18c437e3469e8677e5259856431c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.760\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaa5fa4134b416897581d920c1d7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:08 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff05202b51a74dc78a8037f80302b042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 11: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1142e7180b42aa826e0ff1e4735f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:08 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be80b09a0a34c39a5a6d5d6deee126c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.760\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b01f7ec1e4d40a3bc4a8eef79f9db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:09 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c57994b35462f87896c7f32842e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.774\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 12: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ba5e819da9463095fc88d1d6da88bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:09 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd77d4839964b52a9cee2f4a7a927f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09467bd2e1c4a14b2dfb2309e472972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:10 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72844fd7133e4cb3a207058a603f29f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 13: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409ec3d94f004fd4a2633531db889bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:11 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcccac2b16d349279e96108fc712410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.800\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d052d256514a4aae24147b4b4e2ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:12 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9a1ded0c724442a7d3bdfa6cc6437b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 14: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc8f0d988cc49cdbdb7733b2d2f240d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:12 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f22c6f10864cfe8b253efc0e30aa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.680\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ae915daf654653ba5e22d2ba30fbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:13 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4456b1222a46308685444e2b11d9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 15: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e610ce71754c2a9d6c4dca9dd367fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:14 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5db92a8f6c4a648f1c6c1e9a245ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799d4016af5749dcb49f2eaf6c2a113f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:15 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e229bf070ee44f398c935b5263dede3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 16: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa58f0bde6e433cb9a39fe77941135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:15 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329b82a807e4527815b32937d66e85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.800\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e7543ecc740d4939e71612ceefdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:16 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5f0136291f46a4afb392fec477979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 17: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24116095f7294f0a92fd2eb74340a4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:16 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e794f213c04456eb27bdb7d951083ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.840\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2584c22eb63141839c3d2acdc03f6637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:17 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4420a71a2824daf8fbe8a0aaf5b17f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.677\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 18: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f49c8ac7884820805e75a7df5ce0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:18 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43067d0767b6484690a3656cfbd06d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.680\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4012219eb21a41dab5e1eac47359a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:19 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a1292cb2dd4e8bb51cdd648e7b8819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.774\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 19: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6994cd4cd6ea4109a62d88bdb8977a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:19 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e317976e3a8042ada43a2d49fe92b744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.680\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cc042f1a1b4e38bd910f723d29542e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:20 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694ee04161d44fbb9509bc0eb0b26358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.742\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 20: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 25 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ac1858eb6b4f5b990dc4b2dc6fa890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:20 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10513d8f45347a6a4bdc5af18caac72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.720\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a205a69b5b459fa140967ecc4907c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 03:21 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca10f45b888444479ad605583096d16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.710\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                      PROMPT OPTIMIZATION SUMMARY                               ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "Optimization completed after 20 iteration(s)\n",
      "Final test accuracy: 0.710\n",
      "Improvement over baseline: +15.8%\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Loop ‚îÇ Train Acc   ‚îÇ Test Acc    ‚îÇ Train Exp ID     ‚îÇ Test Exp ID      ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ   0  ‚îÇ      -      ‚îÇ   0.613     ‚îÇ        -         ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ   0.720     ‚îÇ   0.613     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ   0.720     ‚îÇ   0.613     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ   0.840     ‚îÇ   0.677     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ   0.640     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   5  ‚îÇ   0.680     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   6  ‚îÇ   0.720     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   7  ‚îÇ   0.720     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   8  ‚îÇ   0.640     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   9  ‚îÇ   0.680     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  10  ‚îÇ   0.760     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  11  ‚îÇ   0.760     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  12  ‚îÇ   0.720     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  13  ‚îÇ   0.800     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  14  ‚îÇ   0.680     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  15  ‚îÇ   0.720     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  16  ‚îÇ   0.800     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  17  ‚îÇ   0.840     ‚îÇ   0.677     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  18  ‚îÇ   0.680     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  19  ‚îÇ   0.680     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ  20  ‚îÇ   0.720     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "All prompt versions have been uploaded to Arize Prompt Hub: prompt_optimizer+510d9e26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import optimizer_sdk.prompt_learning_optimizer\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(optimizer_sdk.prompt_learning_optimizer)\n",
    "\n",
    "# Now re-import the class\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "# Prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name = \"prompt_optimizer+\" + UUID\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = PromptLearningOptimizer(\n",
    "    prompt=system_prompt,\n",
    "    model_choice=\"gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define feedback columns\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "\n",
    "# Run the complete optimization workflow\n",
    "optimized_prompt = optimizer.optimize_with_experiments(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    train_dataset_id=train_dataset_id,\n",
    "    test_dataset_id=test_dataset_id,\n",
    "    evaluators=[output_evaluator],\n",
    "    test_evaluator=test_evaluator,\n",
    "    task_fn=generate_task,\n",
    "    arize_client=client,\n",
    "    arize_space_id=ARIZE_SPACE_ID,\n",
    "    experiment_name_prefix=f\"optimization_{UUID}\",\n",
    "    prompt_hub_client=prompt_client,\n",
    "    prompt_name=prompt_name,\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    feedback_columns=feedback_columns,\n",
    "    threshold=1.0,\n",
    "    loops=20,\n",
    "    scorer=\"accuracy\",\n",
    "    context_size_k=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3f774b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting batched prompt optimization with 4 outer loops (scorer: accuracy, threshold: 1.0)\n",
      "\n",
      "üìä Initial evaluation:\n",
      "‚è≥ Running initial evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3cd6578e6c43e0ac073792fc7ead84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:39 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c41730889a2425d81b0d189428fa033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving experiment results...\n",
      "‚úÖ Initial accuracy: 0.581\n",
      "‚è≥ Uploading initial prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "üîß Calculated max rows for context: 25 rows fit within 1,250 tokens\n",
      "üîß Calculated max rows for context: 25 rows fit within 1,250 tokens\n",
      "üî¢ Will process 123 training examples in batches of ~25 rows\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 1: Processing all training data in batches...\n",
      "================================================================================\n",
      "   üîÄ Shuffled 123 examples into 5 batches\n",
      "\n",
      "   üì¶ Batch 1/5 (rows 0-24, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1884c90fd14472cab73dd4dc495b588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:40 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d83723d7ee4a499f80d97db5e8e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.560\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6c3f8e54964dd296b2dc20195322f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:41 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93ef278c8294e32b64d334f4dc4af23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.677\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 2/5 (rows 25-49, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd102e0f34b740f8ad6fb448f40db027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:41 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4875fd1d09a40e9aab797e95844624f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.520\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a0fc0c0e834907b31a96393d0ee95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:42 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5dec7159f14bd8941db0b1fe58662b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.774\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 3/5 (rows 50-74, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529954cf0bc74cc9827ba1c40fd2bd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:42 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40284e9f2e4403d8d5cf43cb227ec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.560\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35bb47653e44bbf9f1e6719be5982ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:43 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c050eaba72485f8993a3a34084aa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 4/5 (rows 75-99, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fbb90611be46a3852d7dec12ba70f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:44 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba00fd801be497b976b1ac518a56474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.880\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30222c690efb4652b017bb3b927df8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:44 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a447184fe540ac9936b776ab54e117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.774\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 5/5 (rows 100-122, 23 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a22f85ec634a9e968d6ad427833139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/23 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:45 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          23      23         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8a555bd2bd46689d17470d5ec0b65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/46 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.565\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e2f1bdd0c484f993a18be5190282e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:46 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c9740e55f4b13932d4f669e2c1b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.774\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 2: Processing all training data in batches...\n",
      "================================================================================\n",
      "   üîÄ Shuffled 123 examples into 5 batches\n",
      "\n",
      "   üì¶ Batch 1/5 (rows 0-24, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0430bf73f949c09560020de337a234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:46 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0fdd62f7284a8f870414178e061f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.680\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de330515b8124794a992f7339bc06dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:47 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c2ec803aa74d31aa58becc55101c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 2/5 (rows 25-49, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e354ddd4cf1244a2aeccc43d09aa3542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:48 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55435070b79f4a40b3e752e249dd5ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.680\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf504b91dbe94cf38e1d265885e2c216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:48 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c359669fb1db4438822680e35c4308b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 3/5 (rows 50-74, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5993ca21230e4c259d71bd857da2090c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:49 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e84ae85ed5241e6ab83915dbb00c55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.800\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8a7f4d4e294687a1ecead4ac8b0c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:50 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e651919dc4484c981d8b0b08fb60b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.710\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 4/5 (rows 75-99, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd7e19f70d740d8ac8132a8212eb575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:50 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d31e666b91e49a8ad784a98f4ab2888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.720\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9de19c18228447d9cad0a905998fe35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:51 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea98d745f7744ef08932883f4f219c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 5/5 (rows 100-122, 23 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b708ec6cdc480f82b2a4a88615e52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/23 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:51 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          23      23         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bc5bf80cb5472ca5a9c8f17e9c01f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/46 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.696\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f27d7cc03545ffb7b48d8ebb18e5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:52 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad341d18e1924ea7ac6f36f592f9bc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.774\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 3: Processing all training data in batches...\n",
      "================================================================================\n",
      "   üîÄ Shuffled 123 examples into 5 batches\n",
      "\n",
      "   üì¶ Batch 1/5 (rows 0-24, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f197fc78123b45d5a8b8443fb8d264dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:53 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e779d53a7642528838f210a422cde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.800\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77a5bb917d44b16b33b0fcc2a6b6a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:54 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375a50ba529f48e0b284c37c47a4910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 2/5 (rows 25-49, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af77e4456c1a4266886dc1d2e33e46da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:54 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afe06f833a34bbf956c93159ff2449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.680\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249bd76c30b0493b897e62da6e0abea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:55 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fb9bccd2d2404296befdf33d8c2c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 3/5 (rows 50-74, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f96f58b6c141a58afcb672602fcca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:56 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2e08d0d6494ce99a2b0574e0e586a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.800\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abc3ad4326946c2861796facbdff279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:56 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4128c5181e4cd694499a5327a6c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 4/5 (rows 75-99, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfd8694c79b46489d65162a5339a5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:57 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bed046e79e54868a7f757394a41fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.600\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51ca62f03ef4eccb8de93c57e790a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:58 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877898ba6a234861b509e5c930fc98a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.710\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 5/5 (rows 100-122, 23 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756f475e503f44258c785d3c91080356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/23 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:58 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          23      23         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5b4274f3b949099bed720fbfa36a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/46 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.739\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bd2fb70f1c46969532b44327325082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 10:59 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3d8dcb593e415eb1e8a9411088ac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.677\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 4: Processing all training data in batches...\n",
      "================================================================================\n",
      "   üîÄ Shuffled 123 examples into 5 batches\n",
      "\n",
      "   üì¶ Batch 1/5 (rows 0-24, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c82d8cc3a14334a8468bf51706bb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:00 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574e286aa7c64722a87134c8f2ba4c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.880\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619d575ae15f4b129658508813bd19ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:01 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f02f34b3e5541e08d57d1320c326692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.645\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 2/5 (rows 25-49, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f94cbd04d645aaa69ec1e68f50ea4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:01 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b4d89686a2457b8884302701a79065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.960\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚ö†Ô∏è  API call failed (attempt 1/6): APITimeoutError: Request timed out.\n",
      "   Retrying in 1s...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02aad69137e746d1a8b08f03390bd1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:03 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b026a44454fd492ebf6e9e467d4a48fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.645\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 3/5 (rows 50-74, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce74ce3e3c294a9f9a721d91c0e35666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:03 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba312e778964ed199ea0868f0fd536a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.760\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31aaf4883f54e1e9fff6112382f2fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:04 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68dfb77ac1a4b0f9a89af85ec0188e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.645\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 4/5 (rows 75-99, 25 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358ad00e89b74f2d837b258a781052a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/25 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:05 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          25      25         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e4bfb267fb407faaf20d9d4a265dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/50 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.840\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ba4bb031604b8e9c30cc3a28349ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:06 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589a0ac29b4d46f3aab834c4bc4e13dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.742\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "   üì¶ Batch 5/5 (rows 100-122, 23 examples)\n",
      "   ‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d658b8807c4742b3a9575c079649d50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/23 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:06 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          23      23         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb94c5b19b504160baa83caa86d0facc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/46 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving training experiment results...\n",
      "   ‚úÖ Training accuracy: 0.870\n",
      "   ‚è≥ Processing experiment results and extracting feedback...\n",
      "   ‚è≥ Optimizing prompt with meta-prompt...\n",
      "   ‚úÖ Prompt optimization complete\n",
      "   ‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49d5c478555488783ac5fe55472d9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 11:07 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329bf31298a94ac785eb7c203f146f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "   ‚è≥ Retrieving test experiment results...\n",
      "   ‚úÖ Test accuracy: 0.710\n",
      "   ‚è≥ Uploading prompt version to Prompt Hub...\n",
      "   ‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                  BATCHED PROMPT OPTIMIZATION SUMMARY                           ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "Optimization completed after 20 batch(es)\n",
      "Final test accuracy: 0.710\n",
      "Improvement over baseline: +22.2%\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Loop ‚îÇ Batch   ‚îÇ Train Acc   ‚îÇ Test Acc    ‚îÇ Train Exp ID     ‚îÇ Test Exp ID      ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ   0  ‚îÇ    -    ‚îÇ      -      ‚îÇ   0.581     ‚îÇ        -         ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ  1/5    ‚îÇ   0.560     ‚îÇ   0.677     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ  2/5    ‚îÇ   0.520     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ  3/5    ‚îÇ   0.560     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ  4/5    ‚îÇ   0.880     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ  5/5    ‚îÇ   0.565     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ  1/5    ‚îÇ   0.680     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ  2/5    ‚îÇ   0.680     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ  3/5    ‚îÇ   0.800     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ  4/5    ‚îÇ   0.720     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ  5/5    ‚îÇ   0.696     ‚îÇ   0.774     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ  1/5    ‚îÇ   0.800     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ  2/5    ‚îÇ   0.680     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ  3/5    ‚îÇ   0.800     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ  4/5    ‚îÇ   0.600     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   3  ‚îÇ  5/5    ‚îÇ   0.739     ‚îÇ   0.677     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ  1/5    ‚îÇ   0.880     ‚îÇ   0.645     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ  2/5    ‚îÇ   0.960     ‚îÇ   0.645     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ  3/5    ‚îÇ   0.760     ‚îÇ   0.645     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ  4/5    ‚îÇ   0.840     ‚îÇ   0.742     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   4  ‚îÇ  5/5    ‚îÇ   0.870     ‚îÇ   0.710     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "All prompt versions have been uploaded to Arize Prompt Hub: prompt_optimizer_batched+3c2a6fb2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import optimizer_sdk.prompt_learning_optimizer\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(optimizer_sdk.prompt_learning_optimizer)\n",
    "\n",
    "# Now re-import the class\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "# Prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name_batched = \"prompt_optimizer_batched+\" + UUID\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer_batched = PromptLearningOptimizer(\n",
    "    prompt=system_prompt,\n",
    "    model_choice=\"gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define feedback columns\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "\n",
    "# Run the complete batched optimization workflow\n",
    "optimized_prompt_batched = optimizer_batched.optimize_with_experiments_batched(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    train_dataset_id=train_dataset_id,\n",
    "    test_dataset_id=test_dataset_id,\n",
    "    evaluators=[output_evaluator],\n",
    "    test_evaluator=test_evaluator,\n",
    "    task_fn=generate_task,\n",
    "    arize_client=client,\n",
    "    arize_space_id=ARIZE_SPACE_ID,\n",
    "    experiment_name_prefix=f\"optimization_batched_{UUID}\",\n",
    "    prompt_hub_client=prompt_client,\n",
    "    prompt_name=prompt_name_batched,\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    feedback_columns=feedback_columns,\n",
    "    threshold=1.0,\n",
    "    loops=4,  # Fewer loops since each loop processes ALL data\n",
    "    scorer=\"accuracy\",\n",
    "    context_size_k=1250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93dace",
   "metadata": {},
   "source": [
    "# Prompt Optimization Loop with Arize Experiments\n",
    "\n",
    "This code implements an iterative prompt optimization system that uses **Arize AX experiments** to evaluate and improve prompts based on feedback from LLM evaluators.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `optimize_loop` function automates prompt engineering by:\n",
    "\n",
    "- Evaluating prompts using Arize experiments  \n",
    "- Collecting detailed feedback from LLM evaluators  \n",
    "- Optimizing prompts via a learning-based optimizer  \n",
    "- Iterating until the performance threshold is met or the loop limit is reached  \n",
    "\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "Each of these numbers are added as comments in the code.\n",
    "\n",
    "### 1. Initialization\n",
    "\n",
    "- Set up tracking variables:\n",
    "  - `train_metrics`, `test_metrics`, `raw_dfs` for storing evaluation results\n",
    "- Convert training dataset to a DataFrame for easy updates\n",
    "\n",
    "### 2. Baseline Evaluation\n",
    "\n",
    "- Run an initial experiment using the **test set**\n",
    "- Establish a **baseline metric** (e.g., accuracy, F1) to compare against future improvements\n",
    "\n",
    "### 3. Early Exit Check\n",
    "\n",
    "- If the **initial prompt already meets the performance threshold**, skip further optimization to save time and compute\n",
    "\n",
    "### 4. Main Optimization Loop\n",
    "\n",
    "For each iteration (up to `loops`):\n",
    "\n",
    "#### 4a. Run Training Experiment\n",
    "\n",
    "- Execute the current prompt on the **training set**\n",
    "- Use LLM evaluators to generate **natural language feedback**\n",
    "\n",
    "#### 4b. Process Feedback\n",
    "\n",
    "- Extract structured information from evaluator outputs:\n",
    "  - Correctness\n",
    "  - Explanation\n",
    "  - Confusion reason\n",
    "  - Error type\n",
    "  - Prompt fix suggestions\n",
    "- Update the training DataFrame with this feedback\n",
    "\n",
    "#### 4c. Generate Learning Annotations\n",
    "\n",
    "- Convert feedback into structured annotations for the optimizer to learn from\n",
    "- This allows learning from evaluator insights in a consistent format\n",
    "\n",
    "#### 4d. Optimize the Prompt\n",
    "\n",
    "- Pass feedback to the **PromptLearningOptimizer**\n",
    "- Generate an **improved prompt** that attempts to correct issues found in the previous iteration\n",
    "\n",
    "#### 4e. Evaluate on Test Set\n",
    "\n",
    "- Evaluate the updated prompt on the **held-out test set**\n",
    "- Assess **generalization** beyond the training data\n",
    "\n",
    "#### 4f. Track Metrics\n",
    "\n",
    "- Log metrics for:\n",
    "  - Training set performance\n",
    "  - Test set performance\n",
    "- Store raw results for further analysis or visualization\n",
    "\n",
    "#### 4g. Convergence Check\n",
    "\n",
    "- If the new prompt's test metric **meets or exceeds the threshold**, exit the loop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "be67e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting prompt optimization with 5 iterations (scorer: accuracy, threshold: 1)\n",
      "ÔøΩÔøΩ Initial evaluation:\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acf00444a9d44be9ddeb4644aafd1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:30 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115b2aaa16104d3e9a997def8c316bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id           result  \\\n",
      "0  EXP_ID_2e4d1f  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8     Login Issues   \n",
      "1  EXP_ID_231ac7  ab975139-7a59-41d1-9c2d-42c6e26986b9  Billing Inquiry   \n",
      "2  EXP_ID_cea527  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d     Order Status   \n",
      "3  EXP_ID_59a802  8a3fd233-fd9d-4911-8f61-e1c99a569b29   Password Reset   \n",
      "4  EXP_ID_60bdbc  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9     Login Issues   \n",
      "\n",
      "                    result.trace.id  result.trace.timestamp  \\\n",
      "0  caf0b2a001d9aa6c60af51a0c50237b9           1761071383288   \n",
      "1  d4d8a1898cad0f2e61c63ef34945ce3f           1761071384303   \n",
      "2  a6a293787df911781449d57b8a4e5adf           1761071386242   \n",
      "3  2d01e85aa3a618226ba10103183546a8           1761071387160   \n",
      "4  cbe89e9c32786770fd333ae17feb0532           1761071388122   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        0.0                     False   \n",
      "1                        0.0                     False   \n",
      "2                        0.0                     False   \n",
      "3                        0.0                     False   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  91ef13cc83523edb2acd0eb6a54d3862   \n",
      "1                     placeholder  d267af07b91fabf12dc8783862cba995   \n",
      "2                     placeholder  ead5dee4ad698b4c688aa0ad7f066aa3   \n",
      "3                     placeholder  e4cd922e1e564c46a3177f57273d7978   \n",
      "4                     placeholder  409d7210745d381c59eb4d885366a6d0   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761071406866  \n",
      "1                        1761071406911  \n",
      "2                        1761071407019  \n",
      "3                        1761071407061  \n",
      "4                        1761071407116  \n",
      "‚úÖ Initial accuracy: 0.5483870967741935\n",
      "üìä Loop 1: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132c8ff4ea6648a2999deb711a9fdb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:31 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb657634d8a4ad6887972f7880fdf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/246 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_91fcff  0a27dd1f-1e25-4a3c-b3f6-7fd3338c7cf0   \n",
      "1  EXP_ID_ee2877  da4374f1-cd63-463c-b41a-1d322878d601   \n",
      "2  EXP_ID_81a0ac  b4eb05b2-b1eb-42aa-8c06-995c5ce7899d   \n",
      "3  EXP_ID_251c2b  79146ac7-9c1e-44b4-a663-3b5a00b3d162   \n",
      "4  EXP_ID_0b4825  75034039-708b-4641-88e0-f0f7abe4b8dc   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  d33140872fb285a7348d1e987c957ff5   \n",
      "1          Billing Inquiry  8897e59d15a3374684f9b8417acda1f8   \n",
      "2     Technical Bug Report  71b30ddcbcdb4ede41845b67bda6c854   \n",
      "3          Billing Inquiry  d27ec928f06a1b2664d04f2ebf8ebd70   \n",
      "4         General Feedback  e0e4fce02e7cfd0426cbbcb1362c0243   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1761071414877                          1.0   \n",
      "1           1761071415994                          1.0   \n",
      "2           1761071416849                          0.0   \n",
      "3           1761071417852                          1.0   \n",
      "4           1761071419694                          0.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                   incorrect   \n",
      "3                     correct   \n",
      "4                   incorrect   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: incorrect;\\n        explanation: ...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: incorrect;\\n        explanation: ...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  670d5b9bfcb71eeda24ac82741f01841                          1761071492990   \n",
      "1  e2228e8eca458c7ae33c5a6c527a88d5                          1761071493034   \n",
      "2  db1c62fd8f3c397fb27d07098b19f197                          1761071493075   \n",
      "3  abc44b9a37815d349157b0752eb98433                          1761071493121   \n",
      "4  2fe53d2545b3ec272b8e6227cd8f6c7a                          1761071493164   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        0.0                     False   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  0af40a322655ad4cda653b01ac670ebb   \n",
      "1                     placeholder  5b068894729b8bd9e7831bb64697c4c8   \n",
      "2                     placeholder  c0ffea8139f163a5a0f14fb33d54e81f   \n",
      "3                     placeholder  407b8bfbd35ab1d913d1dfb6e71faaa6   \n",
      "4                     placeholder  c3eaf77e1b47456c7473963a3ec4069b   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761071492993  \n",
      "1                        1761071493036  \n",
      "2                        1761071493077  \n",
      "3                        1761071493123  \n",
      "4                        1761071493166  \n",
      "‚úÖ Training accuracy: 0.5609756097560976\n",
      "üîç Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 4,000 token limit\n",
      "üìä Processing 123 examples in 5 batches\n",
      "   ‚úÖ Batch 1/5: Optimized\n",
      "   ‚úÖ Batch 2/5: Optimized\n",
      "   ‚úÖ Batch 3/5: Optimized\n",
      "   ‚úÖ Batch 4/5: Optimized\n",
      "   ‚úÖ Batch 5/5: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0713ac6f85b74670a78b5780eb378260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:34 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5973da76b8146b7a31447c9911ee19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_de651f  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8   \n",
      "1  EXP_ID_dd0ab1  ab975139-7a59-41d1-9c2d-42c6e26986b9   \n",
      "2  EXP_ID_b6b9df  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d   \n",
      "3  EXP_ID_37f645  8a3fd233-fd9d-4911-8f61-e1c99a569b29   \n",
      "4  EXP_ID_57d42e  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                    Login Issues  718f944097404fd6d1f55966bb8428d4   \n",
      "1  Subscription Upgrade/Downgrade  d5c81eae4960f4211d999055be236a5f   \n",
      "2                    Order Status  55d88feae29cb285af43312514cbf8f3   \n",
      "3                  Password Reset  a509ceda23842a6833d9696564808353   \n",
      "4                    Login Issues  5561959c9bd57cbb2a76d3d9b85f5611   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1761071633666                        0.0   \n",
      "1           1761071634636                        0.0   \n",
      "2           1761071635668                        0.0   \n",
      "3           1761071636574                        0.0   \n",
      "4           1761071637548                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                     False                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                     False                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  a7d375c487b74c2c26244cbf661358c6                        1761071655607  \n",
      "1  ae670fbbefd7527c9eec7ee88fdc3e05                        1761071655649  \n",
      "2  b61376fc74ee3710e864189eec6f14b4                        1761071655693  \n",
      "3  cab14f04accb84eac9c8aadc1805c087                        1761071655758  \n",
      "4  88f1e948cbec4d4e841bfeffaa0e6c15                        1761071655799  \n",
      "‚úÖ Test accuracy: 0.6451612903225806\n",
      "üìä Loop 2: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad8da1148364a638225cdd73629a8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:35 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5bf2adf90c475ca31909e2fa0a779c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/246 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_e015e4  0a27dd1f-1e25-4a3c-b3f6-7fd3338c7cf0   \n",
      "1  EXP_ID_69bdb8  da4374f1-cd63-463c-b41a-1d322878d601   \n",
      "2  EXP_ID_15094d  b4eb05b2-b1eb-42aa-8c06-995c5ce7899d   \n",
      "3  EXP_ID_66ad62  79146ac7-9c1e-44b4-a663-3b5a00b3d162   \n",
      "4  EXP_ID_29dbd2  75034039-708b-4641-88e0-f0f7abe4b8dc   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  e3a2e2f4e112609b065f45e358648cc5   \n",
      "1          Billing Inquiry  9c978ec23fa7111c6a6208b15de309b1   \n",
      "2     Technical Bug Report  a657c850e7b9288bee2ce12c08db8c06   \n",
      "3          Billing Inquiry  d9ef856d42b9433e011d5ae82a7ab8b2   \n",
      "4          Feature Request  09f464da917f2ae0c67ed49dcae77779   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1761071663357                          1.0   \n",
      "1           1761071664360                          1.0   \n",
      "2           1761071665380                          0.0   \n",
      "3           1761071666273                          1.0   \n",
      "4           1761071667245                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                   incorrect   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: incorrect;\\n        explanation: ...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  93857f9f5f0159a477506dbf227ed9a8                          1761071738642   \n",
      "1  ef6469f279065c493d255a1a5fc6f035                          1761071738695   \n",
      "2  cdef4f8a5247734c9c11876e98e41c34                          1761071738738   \n",
      "3  ded1bf4006ad83d597756d0ddddde66e                          1761071738785   \n",
      "4  667527cadac759491d4b5bdf00eeb5bd                          1761071738827   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  0cf760469e0383fae3f68c51f3545cb5   \n",
      "1                     placeholder  3a635bee84abf712d620eddadbc27190   \n",
      "2                     placeholder  ddf9fd450525e854a66498d01a861b9b   \n",
      "3                     placeholder  f866a28a3824bb33a50755914e0a0810   \n",
      "4                     placeholder  0e06141696531bc88fceb50c6ba20eb4   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761071738646  \n",
      "1                        1761071738696  \n",
      "2                        1761071738740  \n",
      "3                        1761071738787  \n",
      "4                        1761071738828  \n",
      "‚úÖ Training accuracy: 0.6991869918699187\n",
      "üîç Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 4,000 token limit\n",
      "üìä Processing 123 examples in 5 batches\n",
      "   ‚úÖ Batch 1/5: Optimized\n",
      "   ‚úÖ Batch 2/5: Optimized\n",
      "   ‚úÖ Batch 3/5: Optimized\n",
      "   ‚úÖ Batch 4/5: Optimized\n",
      "‚ö†Ô∏è  API call failed (attempt 1/6): APITimeoutError: Request timed out.\n",
      "   Retrying in 1s...\n",
      "   ‚úÖ Batch 5/5: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a4ed246a3466bbab56b7f7f53762e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:38 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa16bf6fb344b9ab6b1675ced3ff69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id           result  \\\n",
      "0  EXP_ID_5f52a5  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8     Login Issues   \n",
      "1  EXP_ID_373a96  ab975139-7a59-41d1-9c2d-42c6e26986b9  Billing Inquiry   \n",
      "2  EXP_ID_000b79  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d  Invoice Request   \n",
      "3  EXP_ID_7edb07  8a3fd233-fd9d-4911-8f61-e1c99a569b29   Password Reset   \n",
      "4  EXP_ID_e065df  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9     Login Issues   \n",
      "\n",
      "                    result.trace.id  result.trace.timestamp  \\\n",
      "0  01fc88ddc043b5bf7a4dc747ec5374a3           1761071895538   \n",
      "1  9b442ee8a3401970387566e7a11f963a           1761071896545   \n",
      "2  c9a161a87cf949a829e72465faffb5f9           1761071897533   \n",
      "3  1440c6634bfab6838176a71da7898339           1761071898516   \n",
      "4  8df59a4a72857228b0781af4a587bfa1           1761071900356   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        0.0                     False   \n",
      "1                        0.0                     False   \n",
      "2                        0.0                     False   \n",
      "3                        0.0                     False   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  2532963cfbfa2fcafdf2121a27f3a4a0   \n",
      "1                     placeholder  7296fcdd1f4f67c9f2e503ba60b1750c   \n",
      "2                     placeholder  74ac99c3612458003f2eab78aca898bb   \n",
      "3                     placeholder  5b4ebfbcf5bbef52224684d262df32d2   \n",
      "4                     placeholder  055113c31238cc1da6cf28a759047413   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761071919128  \n",
      "1                        1761071919172  \n",
      "2                        1761071919212  \n",
      "3                        1761071919254  \n",
      "4                        1761071919297  \n",
      "‚úÖ Test accuracy: 0.5806451612903226\n",
      "üìä Loop 3: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a065e4263a4a6993c545fd4132a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:40 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16abe762027f41d99ea67bdff47335b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/246 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_691399  0a27dd1f-1e25-4a3c-b3f6-7fd3338c7cf0   \n",
      "1  EXP_ID_4c0c86  da4374f1-cd63-463c-b41a-1d322878d601   \n",
      "2  EXP_ID_99bce7  b4eb05b2-b1eb-42aa-8c06-995c5ce7899d   \n",
      "3  EXP_ID_cee869  79146ac7-9c1e-44b4-a663-3b5a00b3d162   \n",
      "4  EXP_ID_07a805  75034039-708b-4641-88e0-f0f7abe4b8dc   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  110a816911aa092812fde89ce87415fc   \n",
      "1          Billing Inquiry  afa608e87f5944df324bce61f86e80ba   \n",
      "2           Product Return  6231ee69fe15dfe7101661bc7874fcb3   \n",
      "3          Billing Inquiry  73a9e08a52e477bb62246d6de5ed2c72   \n",
      "4          Feature Request  2072c107187e5a5efaada6b997882089   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1761071926556                          1.0   \n",
      "1           1761071927582                          1.0   \n",
      "2           1761071930420                          1.0   \n",
      "3           1761071931380                          1.0   \n",
      "4           1761071932333                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                     correct   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: correct;\\n        explanation: Th...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  bf7aff1071a9611034348541b38263b0                          1761072001836   \n",
      "1  01e3e44d1a1531a8474adc6363f9ee82                          1761072001882   \n",
      "2  a9b0e60d28e47e68f2877848d166e81c                          1761072001924   \n",
      "3  9c97e5c8ed80d2c84122f6e215b541e3                          1761072001968   \n",
      "4  f0fe3490e4e78a1c006d4df76c5eaef9                          1761072002008   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        1.0                      True   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  5858b47c4ca013912a87fe312c328989   \n",
      "1                     placeholder  3b80672c6f1a140277d76265808fce1d   \n",
      "2                     placeholder  e71d4f4cf1dfad082c113924735c50d9   \n",
      "3                     placeholder  cc2938cbdde54d17c48782b95f8faa3f   \n",
      "4                     placeholder  153ed1fa9707778660600871ea0070cf   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761072001839  \n",
      "1                        1761072001884  \n",
      "2                        1761072001925  \n",
      "3                        1761072001969  \n",
      "4                        1761072002009  \n",
      "‚úÖ Training accuracy: 0.7479674796747967\n",
      "üîç Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 4,000 token limit\n",
      "üìä Processing 123 examples in 5 batches\n",
      "   ‚úÖ Batch 1/5: Optimized\n",
      "‚ö†Ô∏è  API call failed (attempt 1/6): APITimeoutError: Request timed out.\n",
      "   Retrying in 1s...\n",
      "   ‚úÖ Batch 2/5: Optimized\n",
      "   ‚úÖ Batch 3/5: Optimized\n",
      "   ‚úÖ Batch 4/5: Optimized\n",
      "   ‚úÖ Batch 5/5: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fd41e5f97a430cbcb645904f9fd0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:43 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e42f4b509f4c5184083cb4d3cf14c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id            result  \\\n",
      "0  EXP_ID_5c232e  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8  Account Creation   \n",
      "1  EXP_ID_84822b  ab975139-7a59-41d1-9c2d-42c6e26986b9   Billing Inquiry   \n",
      "2  EXP_ID_e8e1ee  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d   Invoice Request   \n",
      "3  EXP_ID_95a23b  8a3fd233-fd9d-4911-8f61-e1c99a569b29    Password Reset   \n",
      "4  EXP_ID_cad77f  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9      Login Issues   \n",
      "\n",
      "                    result.trace.id  result.trace.timestamp  \\\n",
      "0  571d595e645321f2a00f8642734694c7           1761072161429   \n",
      "1  b5dba7ccdce9fec4aae2f2fb165f7973           1761072162475   \n",
      "2  67978ce1252521ffa8c64a11a29fad3e           1761072163422   \n",
      "3  e6c505e4a3bcc6983e2b7704d540d996           1761072164355   \n",
      "4  5ed51a89519ed1ba8a11fcc9bab9ebf5           1761072165371   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        0.0                     False   \n",
      "2                        0.0                     False   \n",
      "3                        0.0                     False   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  60bdb877b83ee8cc59d66319b9f1f65c   \n",
      "1                     placeholder  95f10a6ad054c0ef7432f3afa1475351   \n",
      "2                     placeholder  6ecca7725b500c2b064bc65018bbfac0   \n",
      "3                     placeholder  6c7c37db5ab5b766cb2f112938e2d245   \n",
      "4                     placeholder  477bb9a51e0e864d739876f2b177196b   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761072184930  \n",
      "1                        1761072184972  \n",
      "2                        1761072185016  \n",
      "3                        1761072185059  \n",
      "4                        1761072185106  \n",
      "‚úÖ Test accuracy: 0.6451612903225806\n",
      "üìä Loop 4: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d107dd70e2234eca8fa1014c97ecba49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:44 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56131d89e84d7fb42ec4399b735f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/246 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_18398d  0a27dd1f-1e25-4a3c-b3f6-7fd3338c7cf0   \n",
      "1  EXP_ID_1ca3cd  da4374f1-cd63-463c-b41a-1d322878d601   \n",
      "2  EXP_ID_a6bea8  b4eb05b2-b1eb-42aa-8c06-995c5ce7899d   \n",
      "3  EXP_ID_7495b5  79146ac7-9c1e-44b4-a663-3b5a00b3d162   \n",
      "4  EXP_ID_4e7d3d  75034039-708b-4641-88e0-f0f7abe4b8dc   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  4de08171bcbdd4082db6c6c9ca661af3   \n",
      "1          Billing Inquiry  ae7bc0c40deba79f7f6d49ec1a13d633   \n",
      "2           Product Return  ebcdf0763d4847b4160e1fc740ce829b   \n",
      "3          Billing Inquiry  7ca1768c6227846957ba3b90263ba58a   \n",
      "4          Feature Request  db2d83721d4a334ae4f03d4e407ba3d4   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1761072192569                          1.0   \n",
      "1           1761072193575                          1.0   \n",
      "2           1761072194514                          1.0   \n",
      "3           1761072195537                          1.0   \n",
      "4           1761072196451                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                     correct   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: correct;\\n        explanation: Th...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  5338a6eda26fa0d1c2dae8ac099b25fd                          1761072271403   \n",
      "1  741acff820e9f0ab4c62615b7c28e58b                          1761072271446   \n",
      "2  1447a3056e6e7bb621f68d223bc620cd                          1761072271487   \n",
      "3  b1fb9dd70f2d753dfdff12b107e11329                          1761072271531   \n",
      "4  3a2883aaab617e0b03ab1a9bbc7627ec                          1761072271572   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        1.0                      True   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  58c232f4c4411d08fc986ac0c6a654e5   \n",
      "1                     placeholder  7a5896a274288cefdae98b03bcbacf24   \n",
      "2                     placeholder  a671c3b8e815572263d9da91ccd0b330   \n",
      "3                     placeholder  03c7845ebdf4033f70a5287f36192565   \n",
      "4                     placeholder  88182b10beb05b0acddc6640dae52138   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761072271406  \n",
      "1                        1761072271448  \n",
      "2                        1761072271488  \n",
      "3                        1761072271532  \n",
      "4                        1761072271575  \n",
      "‚úÖ Training accuracy: 0.7723577235772358\n",
      "üîç Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 4,000 token limit\n",
      "üìä Processing 123 examples in 4 batches\n",
      "   ‚úÖ Batch 1/4: Optimized\n",
      "   ‚úÖ Batch 2/4: Optimized\n",
      "   ‚úÖ Batch 3/4: Optimized\n",
      "   ‚úÖ Batch 4/4: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2596b4ae4a834f369272d334e81903c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:47 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e60a9d30ca54f96880da8a1a1a359db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id            result  \\\n",
      "0  EXP_ID_4b97a5  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8  Account Creation   \n",
      "1  EXP_ID_252713  ab975139-7a59-41d1-9c2d-42c6e26986b9   Billing Inquiry   \n",
      "2  EXP_ID_6d504e  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d       Data Export   \n",
      "3  EXP_ID_905e72  8a3fd233-fd9d-4911-8f61-e1c99a569b29    Password Reset   \n",
      "4  EXP_ID_45b551  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9      Login Issues   \n",
      "\n",
      "                    result.trace.id  result.trace.timestamp  \\\n",
      "0  268a0e04be12aa507f59d64a3796a495           1761072404907   \n",
      "1  769e514ccb43659cf895ae65c03f6125           1761072405928   \n",
      "2  9e7cc80f3e9fae1454002ee9a55fd563           1761072406926   \n",
      "3  f51f51dd09eb9cc65d6b6f219c6dd139           1761072407811   \n",
      "4  36fd0d26d22e6141f546d21f42b92954           1761072408787   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        0.0                     False   \n",
      "2                        1.0                      True   \n",
      "3                        0.0                     False   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  365f502ab57d2a5b9153b8ce070dde91   \n",
      "1                     placeholder  0c59ba0e3a73d7fd3db77744e98763e7   \n",
      "2                     placeholder  ba473a3913bc262013dfe3fd0363b127   \n",
      "3                     placeholder  b5b1f1fe624b42e4243128cde3dedb02   \n",
      "4                     placeholder  2c91efb1394f454a35978aa73c039a45   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761072427099  \n",
      "1                        1761072427148  \n",
      "2                        1761072427189  \n",
      "3                        1761072427238  \n",
      "4                        1761072427282  \n",
      "‚úÖ Test accuracy: 0.6774193548387096\n",
      "üìä Loop 5: Optimizing prompt...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75436d56d0f143cc8ed565a8183a3a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/123 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:48 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         123     123         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbe5dba88f84fdc803856f202e18c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/246 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_b39b0a  0a27dd1f-1e25-4a3c-b3f6-7fd3338c7cf0   \n",
      "1  EXP_ID_14384a  da4374f1-cd63-463c-b41a-1d322878d601   \n",
      "2  EXP_ID_8bf94a  b4eb05b2-b1eb-42aa-8c06-995c5ce7899d   \n",
      "3  EXP_ID_41d4d4  79146ac7-9c1e-44b4-a663-3b5a00b3d162   \n",
      "4  EXP_ID_1a024e  75034039-708b-4641-88e0-f0f7abe4b8dc   \n",
      "\n",
      "                    result                   result.trace.id  \\\n",
      "0  Privacy Policy Question  6a4cf5f8b2a6730f2a89a310e2bdc41b   \n",
      "1          Billing Inquiry  7083000e2795fe28cd49b47d0e207bec   \n",
      "2     Technical Bug Report  c213fc391868077c5387ffcf0fdb0c51   \n",
      "3          Billing Inquiry  9cc360fc5d0ec59bf1dd7f1e15e047c3   \n",
      "4          Feature Request  3b94d12b405dcd44a907743bd5cfb391   \n",
      "\n",
      "   result.trace.timestamp  eval.output_evaluator.score  \\\n",
      "0           1761072434760                          1.0   \n",
      "1           1761072435722                          1.0   \n",
      "2           1761072437694                          0.0   \n",
      "3           1761072439658                          1.0   \n",
      "4           1761072440538                          1.0   \n",
      "\n",
      "  eval.output_evaluator.label  \\\n",
      "0                     correct   \n",
      "1                     correct   \n",
      "2                   incorrect   \n",
      "3                     correct   \n",
      "4                     correct   \n",
      "\n",
      "                   eval.output_evaluator.explanation  \\\n",
      "0  correctness: correct;\\n        explanation: Th...   \n",
      "1  correctness: correct;\\n        explanation: Th...   \n",
      "2  correctness: incorrect;\\n        explanation: ...   \n",
      "3  correctness: correct;\\n        explanation: Th...   \n",
      "4  correctness: correct;\\n        explanation: Th...   \n",
      "\n",
      "     eval.output_evaluator.trace.id  eval.output_evaluator.trace.timestamp  \\\n",
      "0  fa9dbf1b575c8c24aee565787e613d7d                          1761072512799   \n",
      "1  38f74275e4e11e3d873d85156b9f6db8                          1761072512845   \n",
      "2  725ce2a0796fd26b6d993ab4621a4e44                          1761072512886   \n",
      "3  9e775719a7199def753b430a6e30c646                          1761072512929   \n",
      "4  86b205c1f3ec4591f42cd648806fc8c2                          1761072512972   \n",
      "\n",
      "   eval.test_evaluator.score eval.test_evaluator.label  \\\n",
      "0                        1.0                      True   \n",
      "1                        1.0                      True   \n",
      "2                        0.0                     False   \n",
      "3                        1.0                      True   \n",
      "4                        1.0                      True   \n",
      "\n",
      "  eval.test_evaluator.explanation      eval.test_evaluator.trace.id  \\\n",
      "0                     placeholder  2213f2b4dc554b8bdc6aa9cb4df0f599   \n",
      "1                     placeholder  ce6fad6cfadb993bb9544399e0cee790   \n",
      "2                     placeholder  4ac473e783a1d17a73ae7bedf9045bd1   \n",
      "3                     placeholder  1620b2521ef463b8d2537b82099300d2   \n",
      "4                     placeholder  34d21f05455a02365459161d40c1fff4   \n",
      "\n",
      "   eval.test_evaluator.trace.timestamp  \n",
      "0                        1761072512801  \n",
      "1                        1761072512846  \n",
      "2                        1761072512887  \n",
      "3                        1761072512930  \n",
      "4                        1761072512974  \n",
      "‚úÖ Training accuracy: 0.7642276422764228\n",
      "üîç Running annotator...\n",
      "['query', 'ground_truth', 'created_at', 'updated_at', 'id', '__index_level_0__', 'correctness', 'explanation', 'confusion_reason', 'error_type', 'evidence_span', 'prompt_fix_suggestion', 'output']\n",
      "\n",
      "üîß Creating batches with 4,000 token limit\n",
      "üìä Processing 123 examples in 4 batches\n",
      "   ‚úÖ Batch 1/4: Optimized\n",
      "   ‚úÖ Batch 2/4: Optimized\n",
      "   ‚úÖ Batch 3/4: Optimized\n",
      "   ‚úÖ Batch 4/4: Optimized\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f1cfd92c6b4a17a1327d6d840d9c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/21/25 02:51 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dbdce7eaad4d7197219c8c7b45d2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "              id                            example_id  \\\n",
      "0  EXP_ID_db5fcf  1ef6ac7f-4a25-41cb-b622-9f1890faeeb8   \n",
      "1  EXP_ID_169556  ab975139-7a59-41d1-9c2d-42c6e26986b9   \n",
      "2  EXP_ID_f04349  f71e3c70-3601-4f2a-8cbe-da3bcabe7c0d   \n",
      "3  EXP_ID_07a89b  8a3fd233-fd9d-4911-8f61-e1c99a569b29   \n",
      "4  EXP_ID_a9d8f3  a6f0aa89-3f62-40e0-a3a3-ae5f0d7e37a9   \n",
      "\n",
      "                           result                   result.trace.id  \\\n",
      "0                Account Creation  8c0cb2e862e15079540acb0f0297b44b   \n",
      "1  Subscription Upgrade/Downgrade  6bd1583623d3ba888e3e18421fdd6d30   \n",
      "2                     Data Export  a33def5ddd8cc6442c1ef27d755a313a   \n",
      "3                  Password Reset  68c9c52382d0a506b6e4c815a9543db1   \n",
      "4                    Login Issues  fc9d3ad54c67db72b0d7df007fd3bf24   \n",
      "\n",
      "   result.trace.timestamp  eval.test_evaluator.score  \\\n",
      "0           1761072654599                        1.0   \n",
      "1           1761072655635                        0.0   \n",
      "2           1761072656624                        1.0   \n",
      "3           1761072657507                        0.0   \n",
      "4           1761072658490                        1.0   \n",
      "\n",
      "  eval.test_evaluator.label eval.test_evaluator.explanation  \\\n",
      "0                      True                     placeholder   \n",
      "1                     False                     placeholder   \n",
      "2                      True                     placeholder   \n",
      "3                     False                     placeholder   \n",
      "4                      True                     placeholder   \n",
      "\n",
      "       eval.test_evaluator.trace.id  eval.test_evaluator.trace.timestamp  \n",
      "0  e54fa681de7b3982589b70a0f2ea5fa1                        1761072680690  \n",
      "1  b05805c4bc9a479d3f96334f5c212a6f                        1761072680732  \n",
      "2  d1c6d0be1ecb4e1994a1aa39175ebe63                        1761072680777  \n",
      "3  db70a234640cb97dfe3cf257362423c3                        1761072680823  \n",
      "4  8ee52019e08940dd71bda339f3bfd2cf                        1761072680863  \n",
      "‚úÖ Test accuracy: 0.6774193548387096\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import optimizer_sdk.prompt_learning_optimizer\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(optimizer_sdk.prompt_learning_optimizer)\n",
    "\n",
    "# Now re-import the class\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "#prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name = \"prompt_optimizer+\" + UUID\n",
    "\n",
    "def optimize_loop(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    system_prompt,\n",
    "    feedback_columns,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "    prompt_versions=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    \"\"\"\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    raw_dfs = []\n",
    "    train_df = train_dataset\n",
    "\n",
    "    print(f\"üöÄ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"ÔøΩÔøΩ Initial evaluation:\")\n",
    "\n",
    "    task = generate_task(system_prompt)\n",
    "\n",
    "    initial_experiment_id, _ = client.run_experiment(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_id=test_dataset_id,\n",
    "        task=task,\n",
    "        evaluators=[test_evaluator],\n",
    "        experiment_name=\"initial_experiment_1\",\n",
    "        concurrency=10\n",
    "    )\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    initial_metric_value = compute_metric(initial_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "    print(f\"‚úÖ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "    prompt_versions.append(system_prompt)\n",
    "\n",
    "    add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", initial_metric_value, 0)\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"üéâ Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompt_versions,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    # Initialize all feedback columns\n",
    "\n",
    "    while loops > 0:\n",
    "        print(f\"üìä Loop {curr_loop}: Optimizing prompt...\")\n",
    "        \n",
    "        task = generate_task(system_prompt)\n",
    "\n",
    "        train_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=train_dataset_id,\n",
    "            task=task,\n",
    "            evaluators=[output_evaluator, test_evaluator],\n",
    "            experiment_name=f\"train_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        train_metric = compute_metric(train_experiment_id, \"eval.output_evaluator.score\", \"accuracy\")\n",
    "        train_metrics.append(train_metric)\n",
    "        train_df = process_experiment(client, train_experiment_id, train_df, \"query\", \"output\", feedback_columns)\n",
    "        print(f\"‚úÖ Training {scorer}: {train_metric}\")\n",
    "        \n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,  \n",
    "            model_choice=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_df,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_df,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=4000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "        prompt_versions.append(system_prompt)\n",
    "\n",
    "        \n",
    "        test_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=test_dataset_id,\n",
    "            task=generate_task(system_prompt),\n",
    "            evaluators=[test_evaluator],\n",
    "            experiment_name=f\"test_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        test_metric = compute_metric(test_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "        test_metrics.append(test_metric)\n",
    "\n",
    "        add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", test_metric, curr_loop)\n",
    "        print(f\"‚úÖ Test {scorer}: {test_metric}\")\n",
    "\n",
    "        if test_metric >= threshold:\n",
    "            print(\"üéâ Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"prompts\": prompt_versions,\n",
    "        \"raw\": raw_dfs\n",
    "    }\n",
    "\n",
    "# Main execution - use asyncio.run() to run the async function\n",
    "evaluators = [output_evaluator]\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "result = optimize_loop(train_dataset, test_dataset, system_prompt, feedback_columns, loops=5, scorer=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7627",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd42533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompts\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüîç Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"üß™ Initial Test Accuracy: {initial_test_acc}\")\n",
    "print(f\"üß™ Optimized Test Accuracy: {best_test_acc} (Œî {best_test_acc - initial_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
