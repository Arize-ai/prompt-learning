{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166be50e",
   "metadata": {},
   "source": [
    "# Arize AX: Improving Classification with LLMs using Prompt Learning\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/arize-logo-white.jpg\" width=\"600\">\n",
    "\n",
    "In this notebook we will leverage the PromptLearningOptimizer developed here at Arize to improve upon the accuracy of LLMs on classification tasks. Specifically we will be classifying support queries into 30 different classes, including\n",
    "\n",
    "Account Creation\n",
    "\n",
    "Login Issues\n",
    "\n",
    "Password Reset\n",
    "\n",
    "Two-Factor Authentication\n",
    "\n",
    "Profile Updates\n",
    "\n",
    "Billing Inquiry\n",
    "\n",
    "Refund Request\n",
    "\n",
    "and 24 more. \n",
    "\n",
    "You can view the dataset in datasets/support_queries.csv.\n",
    "\n",
    "**Note: This notebook `arizeax_support_query_classification.ipynb` complements `support_query_classification.ipynb` by using Arize Phoenix datasets, experiments, and prompt management for Prompt Learning. It's a more end to end way for you to visualize your iterative prompt improvement and see how it performs on train/test sets, and also leverages methods for advanced features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb715e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q arize-phoenix openai pandas arize gql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a2b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, getpass\n",
    "import openai\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI\n",
    "import re\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7ef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "openai_client = AsyncOpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "ARIZE_API_KEY = getpass.getpass('ARIZE_API_KEY')\n",
    "ARIZE_SPACE_ID = getpass.getpass('ARIZE_SPACE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d31f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee3bec",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45a44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "import pandas as pd\n",
    "\n",
    "# add your Arize API key here\n",
    "client = ArizeDatasetsClient(api_key=ARIZE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463fac",
   "metadata": {},
   "source": [
    "## **Make train/test sets**\n",
    "\n",
    "We use an 80/20 train/test split to train our prompt. The optimizer will use the training set to visualize and analyze its errors and successes, and make prompt updates based on these results. We will then test on the test set to see how that prompt performs on unseen data. \n",
    "\n",
    "We will be exporting these datasets to Arize AX. In Arize you will be able to view the experiments we run on the train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf0fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset id: RGF0YXNldDozMjM1NjE6M0NOTg==\n",
      "test dataset id: RGF0YXNldDozMjM1NjI6SXc5dA==\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from uuid import uuid1\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../datasets/support_queries.csv\")\n",
    "\n",
    "\n",
    "#train_set = data.sample(frac=0.8, random_state=42).sample(frac=0.02) #small sample test\n",
    "train_set = data.sample(frac=0.8, random_state=42) #full dataset test train 123 / test 31 examples\n",
    "test_set = data.drop(train_set.index)\n",
    "\n",
    "UUID = str(uuid1())[:8] #associate UUID with datasets and prompt name to track across runs\n",
    "TRAIN_DATASET_NAME = \"prompt_optimizer_training_data+\" + UUID\n",
    "TEST_DATASET_NAME = \"prompt_optimizer_test_data+\" + UUID\n",
    "\n",
    "train_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TRAIN_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=train_set,\n",
    "    )\n",
    "\n",
    "test_dataset_id = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_name=TEST_DATASET_NAME,\n",
    "        dataset_type=GENERATIVE,\n",
    "        data=test_set,\n",
    "    )\n",
    "\n",
    "# Get datasets as Dataframe\n",
    "train_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TRAIN_DATASET_NAME)\n",
    "test_dataset = client.get_dataset(space_id=ARIZE_SPACE_ID, dataset_name=TEST_DATASET_NAME)\n",
    "\n",
    "print(\"train dataset id:\", train_dataset_id)\n",
    "print(\"test dataset id:\", test_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3184e0",
   "metadata": {},
   "source": [
    "## **Base Prompt for Optimization**\n",
    "\n",
    "This is our base prompt - our 0th iteration. This is the prompt we will be optimizing for our task.\n",
    "\n",
    "We also upload our prompt to Arize AX. Arize's Prompt Hub serves as a repository for your prompts. You will be able to view all iterations of your prompt as its optimized, along with some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902beeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q \"arize[PromptHub]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e78fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gautamrangarajan/Desktop/workspace/prompt-learning/prompt_learning_venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from phoenix.client.types import PromptVersion\n",
    "# Prompt Hub docs: https://arize.com/docs/ax/reference/reference/prompt-hub-api\n",
    "\n",
    "from arize.experimental.prompt_hub import ArizePromptClient, Prompt, LLMProvider\n",
    "\n",
    "# Initialize the client with your Arize credentials\n",
    "prompt_client = ArizePromptClient(\n",
    "    space_id=ARIZE_SPACE_ID,\n",
    "    api_key=ARIZE_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "support query: {query}\n",
    "Account Creation\n",
    "Login Issues\n",
    "Password Reset\n",
    "Two-Factor Authentication\n",
    "Profile Updates\n",
    "Billing Inquiry\n",
    "Refund Request\n",
    "Subscription Upgrade/Downgrade\n",
    "Payment Method Update\n",
    "Invoice Request\n",
    "Order Status\n",
    "Shipping Delay\n",
    "Product Return\n",
    "Warranty Claim\n",
    "Technical Bug Report\n",
    "Feature Request\n",
    "Integration Help\n",
    "Data Export\n",
    "Security Concern\n",
    "Terms of Service Question\n",
    "Privacy Policy Question\n",
    "Compliance Inquiry\n",
    "Accessibility Support\n",
    "Language Support\n",
    "Mobile App Issue\n",
    "Desktop App Issue\n",
    "Email Notifications\n",
    "Marketing Preferences\n",
    "Beta Program Enrollment\n",
    "General Feedback\n",
    "\n",
    "Return just the category, no other text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def add_prompt_version(system_prompt, prompt_name, model_name, test_metric, loop_number):\n",
    "    try:\n",
    "        existing_prompt = prompt_client.pull_prompt(prompt_name=prompt_name)\n",
    "        existing_prompt.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} - Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "    except:\n",
    "        existing_prompt = Prompt(\n",
    "            name=prompt_name,\n",
    "            model_name=model_name,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}],\n",
    "            provider=LLMProvider.OPENAI,\n",
    "        )\n",
    "        existing_prompt.commit_message = f\"Loop {loop_number} \\n Test Metric: {test_metric}\"\n",
    "        prompt_client.push_prompt(existing_prompt, commit_message = existing_prompt.commit_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21e149",
   "metadata": {},
   "source": [
    "## **Output Generator**\n",
    "\n",
    "This function calls OpenAI with our prompt on every row of our dataset to generate outputs. It leverages llm_generate, a Phoenix function, for concurrency in calling LLMs. \n",
    "\n",
    "We return the output column, which contains outputs for every row of our dataset, or every support query in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "691d829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task(system_prompt):\n",
    "\n",
    "    async def output_task(dataset_row):\n",
    "        formatted_prompt = system_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "        response = await openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    return output_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091f4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(label):\n",
    "        return label.strip().strip('\"').strip(\"'\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84886cda",
   "metadata": {},
   "source": [
    "## **Evaluator**\n",
    "\n",
    "In this section we define our LLM-as-judge eval. \n",
    "\n",
    "Prompt Learning works by generating natural language evaluations on your outputs. These evaluations help guide the prompt optimizer towards building an optimized prompt. \n",
    "\n",
    "You should spend time thinking about how to write an informative eval. Your eval makes or breaks this prompt optimizer. With helpful feedback, our prompt optimizer will be able to generate a stronger optimized prompt much more effectively than with sparse or unhelpful feedback. \n",
    "\n",
    "Below is a great example for building a strong eval. You can see that we return many evaluations, including\n",
    "- **correctness**: correct/incorrect - whether the support query was classified correctly or incorrectly.\n",
    "\n",
    "-  **explanation**: Brief explanation of why the predicted classification is correct or incorrect, referencing the correct label if relevant.\n",
    "\n",
    "-  **confusion_reason**: If incorrect, explains why the model may have made this choice instead of the correct classification. Focuses on likely sources of confusion. If correct, 'no confusion'.\n",
    "\n",
    "-  **error_type**: One of: 'broad_vs_specific', 'keyword_bias', 'multi_intent_confusion', 'ambiguous_query', 'off_topic', 'paraphrase_gap', 'other'. Use 'none' if correct. Include the definition of the chosen error type, which are passed into the evaluator's prompt. \n",
    "\n",
    "-  **evidence_span**: Exact phrase(s) from the query that strongly indicate the correct classification.\n",
    "\n",
    "-  **prompt_fix_suggestion**: One clear instruction to add to the classifier prompt to prevent this error.\n",
    "\n",
    "**Take a look at support_query_classification/evaluator_prompt.txt for the full prompt!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1be0f",
   "metadata": {},
   "source": [
    "Our evaluator leverages llm_generate once again to build these llm evals with concurrency. We use an output parser to ensure that our eval is returned in proper json format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#from phoenix.experiments.types import EvaluationResult\n",
    "from arize.experimental.datasets.experiments.evaluators.base import EvaluationResult\n",
    "\n",
    "\n",
    "\n",
    "def find_attributes(output):\n",
    "    patterns = {\n",
    "        \"correctness\": r'\"correctness\":\\s*\"([^\"]*)\"',\n",
    "        \"explanation\": r'\"explanation\":\\s*\"([^\"]*)\"',\n",
    "        \"confusion_reason\": r'\"confusion_reason\":\\s*\"([^\"]*)\"',\n",
    "        \"error_type\": r'\"error_type\":\\s*\"([^\"]*)\"',\n",
    "        \"evidence_span\": r'\"evidence_span\":\\s*\"([^\"]*)\"',\n",
    "        \"prompt_fix_suggestion\": r'\"prompt_fix_suggestion\":\\s*\"([^\"]*)\"'\n",
    "    }\n",
    "\n",
    "    return tuple(\n",
    "        (match := re.search(pattern, output, re.IGNORECASE)) and match.group(1)\n",
    "        for pattern in patterns.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def eval_parser(response: str) -> dict:\n",
    "    correctness, explanation, confusion_reason, error_type, evidence_span, prompt_fix_suggestion = find_attributes(response)\n",
    "    return {\n",
    "        \"correctness\": correctness,\n",
    "        \"explanation\": explanation,\n",
    "        \"confusion_reason\": confusion_reason,\n",
    "        \"error_type\": error_type,\n",
    "        \"evidence_span\": evidence_span,\n",
    "        \"prompt_fix_suggestion\": prompt_fix_suggestion\n",
    "    }\n",
    "\n",
    "\n",
    "async def output_evaluator(dataset_row, output):\n",
    "    with open(\"../prompts/support_query_classification/evaluator_prompt.txt\", \"r\") as file:\n",
    "        evaluator_prompt = file.read()\n",
    "\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{query}\", dataset_row.get(\"query\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{ground_truth}\", dataset_row.get(\"ground_truth\"))\n",
    "    evaluator_prompt = evaluator_prompt.replace(\"{output}\", output)\n",
    "\n",
    "    eval_result = await openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": evaluator_prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "\n",
    "    response = eval_result.choices[0].message.content\n",
    "    parsed_eval_result = eval_parser(response)\n",
    "    explanation=f\"\"\"correctness: {parsed_eval_result.get(\"correctness\", \"\")};\n",
    "        explanation: {parsed_eval_result.get(\"explanation\", \"\")};\n",
    "        confusion_reason: {parsed_eval_result.get(\"confusion_reason\", \"\")};\n",
    "        error_type: {parsed_eval_result.get(\"error_type\", \"\")};\n",
    "        evidence_span: {parsed_eval_result.get(\"evidence_span\", \"\")};\n",
    "        prompt_fix_suggestion: {parsed_eval_result.get(\"prompt_fix_suggestion\", \"\")};\"\"\"\n",
    "\n",
    "    score = float(parsed_eval_result.get(\"correctness\") == \"correct\")\n",
    "    label = parsed_eval_result.get(\"correctness\", \"\")\n",
    "    explanation = explanation\n",
    "\n",
    "    return EvaluationResult(\n",
    "        score=score,\n",
    "        label=label,\n",
    "        explanation=explanation,\n",
    "    )\n",
    "\n",
    "async def test_evaluator(dataset_row, output):\n",
    "    label=str(normalize(dataset_row.get(\"ground_truth\")) == normalize(output))\n",
    "    return EvaluationResult(\n",
    "        label=label,\n",
    "        score = float(label==\"True\"),\n",
    "        explanation=\"placeholder\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa25ca3",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Below we define some metrics that will compute on each iteration of prompt optimization. It will help us measure how our classifier with the current iteration's prompt performs.\n",
    "\n",
    "Specifically we use scikit learn for precision, recall, f1 score, and simple accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba30cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "def compute_metric(experiment_id, prediction_column_name, scorer=\"accuracy\", average=\"macro\"):\n",
    "    \"\"\"\n",
    "    Compute the requested classification metric from a Arize experiment.\n",
    "    \"\"\"\n",
    "    experiment_df = client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    print(experiment_df.head())\n",
    "\n",
    "    y_pred = experiment_df[prediction_column_name]\n",
    "    y_true = [1]*len(experiment_df)\n",
    "\n",
    "    if scorer == \"accuracy\":\n",
    "        return accuracy_score(y_true, y_pred)\n",
    "    elif scorer == \"f1\":\n",
    "        return f1_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"precision\":\n",
    "        return precision_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    elif scorer == \"recall\":\n",
    "        return recall_score(y_true, y_pred, zero_division=0, average=average)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scorer: {scorer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9be50",
   "metadata": {},
   "source": [
    "## Experiment Processor\n",
    "\n",
    "This function pulls an Arize experiment and loads the data into a pandas dataframe so it can run through the optimizer.\n",
    "\n",
    "Specifically it:\n",
    "- Pulls the experiment data from Arize\n",
    "- Adds the input column to the dataframe\n",
    "- Adds the evals to the dataframe\n",
    "- Adds the output to the dataframe\n",
    "- Returns the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9dc4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_experiment(arize_client, experiment_id, train_set, input_column_name, output_column_name, feedback_columns = None):\n",
    "\n",
    "    experiment_df = arize_client.get_experiment(ARIZE_SPACE_ID, experiment_id=experiment_id)\n",
    "\n",
    "    train_set_with_experiment_results = pd.merge(train_set, experiment_df, left_on='id', right_on='example_id', how='inner')\n",
    "    \n",
    "    for column in feedback_columns:\n",
    "        train_set[column] = [None] * len(train_set)\n",
    "    \n",
    "    for idx, row in train_set_with_experiment_results.iterrows():\n",
    "        index = row[\"example_id\"]\n",
    "        eval_output = row[\"eval.output_evaluator.explanation\"]\n",
    "        if feedback_columns:\n",
    "            for item in eval_output.split(\";\"):\n",
    "                key_value = item.split(\":\")\n",
    "                if key_value[0].strip() in feedback_columns:\n",
    "                    key, value = key_value[0].strip(), key_value[1].strip()\n",
    "                    train_set.loc[train_set[\"id\"] == index, key] = value\n",
    "\n",
    "            \n",
    "\n",
    "    train_set[output_column_name] = train_set_with_experiment_results[\"result\"]\n",
    "\n",
    "    train_set.rename(columns={\"query\": input_column_name}, inplace=True)\n",
    "    \n",
    "    return train_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c953b6",
   "metadata": {},
   "source": [
    "# Optimization with Inbult Looping and Random Sampling\n",
    "- This cell demonstrates use of the new optimize_with_experiments workflow for prompt optimization,\n",
    "- including module reloading and running train/test experiments with full iterative evaluation and Prompt Hub integration.\n",
    "- This new workflow is still in a testing phase and is expected to eventually replace the previous optimize method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9df66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting prompt optimization with 2 iterations (scorer: accuracy, threshold: 1.0)\n",
      "\n",
      "üìä Initial evaluation:\n",
      "‚è≥ Running initial evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa4d8af2b6b466587d4f0c09fb3158f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/19/25 11:55 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913cb1836ab04dcca75be1aa643a9f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving experiment results...\n",
      "‚úÖ Initial accuracy: 0.548\n",
      "‚è≥ Uploading initial prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "üîß Calculated max rows for context: 19 rows fit within 1,000 tokens\n",
      "üî¢ Maximum number of rows for training context: 19\n",
      "üîß EFFICIENCY SETUP: Will sample 19 examples from 123 total training examples each loop\n",
      "   üí∞ This saves ~104 evaluator calls per loop!\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 1: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 19 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7444afc514bf4d5ea85bf178725a5820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/19 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/19/25 11:56 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          19      19         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c2e79812fb4a35ad66fae417ef657a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/38 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.737\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea98fe19828e495b8055fe6087b99dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/19/25 11:57 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfeaea49e704070ac9578e168b32777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.581\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "================================================================================\n",
      "üìä Loop 2: Optimizing prompt...\n",
      "================================================================================\n",
      "   üé≤ Sampled 19 examples for this loop\n",
      "‚è≥ Running training experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055e67dfe79746728eca9a290d69c318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/19 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/19/25 11:57 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          19      19         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca0332428474b3bbc12bd81e6a8afc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/38 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving training experiment results...\n",
      "‚úÖ Training accuracy: 0.526\n",
      "‚è≥ Processing experiment results and extracting feedback...\n",
      "‚è≥ Optimizing prompt with meta-prompt...\n",
      "‚úÖ Prompt optimization complete\n",
      "‚è≥ Running test evaluation experiment...\n",
      "\u001b[38;21m  arize.utils.logging | INFO | üß™ Experiment started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a1bfd71de040ae8ca3b9459a1bb716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ Task runs completed.\n",
      "Tasks Summary (10/19/25 11:58 PM -0400)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          31      31         0\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef70a8ff686f4f34ba4a16559c984d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/31 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;21m  arize.utils.logging | INFO | ‚úÖ All evaluators completed.\u001b[0m\n",
      "‚è≥ Retrieving test experiment results...\n",
      "‚úÖ Test accuracy: 0.581\n",
      "‚è≥ Uploading prompt version to Prompt Hub...\n",
      "‚úÖ Uploaded to Prompt Hub\n",
      "\n",
      "\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë                      PROMPT OPTIMIZATION SUMMARY                               ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "Optimization completed after 2 iteration(s)\n",
      "Final test accuracy: 0.581\n",
      "Improvement over baseline: +5.9%\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ Loop ‚îÇ Train Acc   ‚îÇ Test Acc    ‚îÇ Train Exp ID     ‚îÇ Test Exp ID      ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ   0  ‚îÇ      -      ‚îÇ   0.548     ‚îÇ        -         ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   1  ‚îÇ   0.737     ‚îÇ   0.581     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îÇ   2  ‚îÇ   0.526     ‚îÇ   0.581     ‚îÇ RXhwZXJpbWVud... ‚îÇ RXhwZXJpbWVud... ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "All prompt versions have been uploaded to Arize Prompt Hub: prompt_optimizer+09f3d914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import optimizer_sdk.prompt_learning_optimizer\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(optimizer_sdk.prompt_learning_optimizer)\n",
    "\n",
    "# Now re-import the class\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "# Prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name = \"prompt_optimizer+\" + UUID\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = PromptLearningOptimizer(\n",
    "    prompt=system_prompt,\n",
    "    model_choice=\"gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Define feedback columns\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "\n",
    "# Run the complete optimization workflow\n",
    "optimized_prompt = optimizer.optimize_with_experiments(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    train_dataset_id=train_dataset_id,\n",
    "    test_dataset_id=test_dataset_id,\n",
    "    evaluators=[output_evaluator],\n",
    "    test_evaluator=test_evaluator,\n",
    "    task_fn=generate_task,\n",
    "    arize_client=client,\n",
    "    arize_space_id=ARIZE_SPACE_ID,\n",
    "    experiment_name_prefix=f\"optimization_{UUID}_3\",\n",
    "    prompt_hub_client=prompt_client,\n",
    "    prompt_name=prompt_name,\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",\n",
    "    feedback_columns=feedback_columns,\n",
    "    threshold=1.0,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "    context_size_k=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93dace",
   "metadata": {},
   "source": [
    "# Prompt Optimization Loop with Arize Experiments\n",
    "\n",
    "This code implements an iterative prompt optimization system that uses **Arize AX experiments** to evaluate and improve prompts based on feedback from LLM evaluators.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `optimize_loop` function automates prompt engineering by:\n",
    "\n",
    "- Evaluating prompts using Arize experiments  \n",
    "- Collecting detailed feedback from LLM evaluators  \n",
    "- Optimizing prompts via a learning-based optimizer  \n",
    "- Iterating until the performance threshold is met or the loop limit is reached  \n",
    "\n",
    "\n",
    "## Step-by-Step Breakdown\n",
    "\n",
    "Each of these numbers are added as comments in the code.\n",
    "\n",
    "### 1. Initialization\n",
    "\n",
    "- Set up tracking variables:\n",
    "  - `train_metrics`, `test_metrics`, `raw_dfs` for storing evaluation results\n",
    "- Convert training dataset to a DataFrame for easy updates\n",
    "\n",
    "### 2. Baseline Evaluation\n",
    "\n",
    "- Run an initial experiment using the **test set**\n",
    "- Establish a **baseline metric** (e.g., accuracy, F1) to compare against future improvements\n",
    "\n",
    "### 3. Early Exit Check\n",
    "\n",
    "- If the **initial prompt already meets the performance threshold**, skip further optimization to save time and compute\n",
    "\n",
    "### 4. Main Optimization Loop\n",
    "\n",
    "For each iteration (up to `loops`):\n",
    "\n",
    "#### 4a. Run Training Experiment\n",
    "\n",
    "- Execute the current prompt on the **training set**\n",
    "- Use LLM evaluators to generate **natural language feedback**\n",
    "\n",
    "#### 4b. Process Feedback\n",
    "\n",
    "- Extract structured information from evaluator outputs:\n",
    "  - Correctness\n",
    "  - Explanation\n",
    "  - Confusion reason\n",
    "  - Error type\n",
    "  - Prompt fix suggestions\n",
    "- Update the training DataFrame with this feedback\n",
    "\n",
    "#### 4c. Generate Learning Annotations\n",
    "\n",
    "- Convert feedback into structured annotations for the optimizer to learn from\n",
    "- This allows learning from evaluator insights in a consistent format\n",
    "\n",
    "#### 4d. Optimize the Prompt\n",
    "\n",
    "- Pass feedback to the **PromptLearningOptimizer**\n",
    "- Generate an **improved prompt** that attempts to correct issues found in the previous iteration\n",
    "\n",
    "#### 4e. Evaluate on Test Set\n",
    "\n",
    "- Evaluate the updated prompt on the **held-out test set**\n",
    "- Assess **generalization** beyond the training data\n",
    "\n",
    "#### 4f. Track Metrics\n",
    "\n",
    "- Log metrics for:\n",
    "  - Training set performance\n",
    "  - Test set performance\n",
    "- Store raw results for further analysis or visualization\n",
    "\n",
    "#### 4g. Convergence Check\n",
    "\n",
    "- If the new prompt's test metric **meets or exceeds the threshold**, exit the loop early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "import time\n",
    "\n",
    "import copy\n",
    "\n",
    "#prompt name that will show up in Arize Prompt Hub\n",
    "prompt_name = \"prompt_optimizer+\" + UUID\n",
    "\n",
    "def optimize_loop(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    system_prompt,\n",
    "    feedback_columns,\n",
    "    threshold=1,\n",
    "    loops=5,\n",
    "    scorer=\"accuracy\",\n",
    "    prompt_versions=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    scorer: one of \"accuracy\", \"f1\", \"precision\", \"recall\"\n",
    "    \"\"\"\n",
    "    curr_loop = 1\n",
    "    train_metrics = []\n",
    "    test_metrics = []\n",
    "    raw_dfs = []\n",
    "    train_df = train_dataset\n",
    "\n",
    "    print(f\"üöÄ Starting prompt optimization with {loops} iterations (scorer: {scorer}, threshold: {threshold})\")\n",
    "    \n",
    "    print(f\"ÔøΩÔøΩ Initial evaluation:\")\n",
    "\n",
    "    task = generate_task(system_prompt)\n",
    "\n",
    "    initial_experiment_id, _ = client.run_experiment(\n",
    "        space_id=ARIZE_SPACE_ID,\n",
    "        dataset_id=test_dataset_id,\n",
    "        task=task,\n",
    "        evaluators=[test_evaluator],\n",
    "        experiment_name=\"initial_experiment_1\",\n",
    "        concurrency=10\n",
    "    )\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    initial_metric_value = compute_metric(initial_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "    print(f\"‚úÖ Initial {scorer}: {initial_metric_value}\")\n",
    "\n",
    "    test_metrics.append(initial_metric_value)\n",
    "    raw_dfs.append(copy.deepcopy(test_set))\n",
    "    prompt_versions.append(system_prompt)\n",
    "\n",
    "    add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", initial_metric_value, 0)\n",
    "    if initial_metric_value >= threshold:\n",
    "        print(\"üéâ Initial prompt already meets threshold!\")\n",
    "        return {\n",
    "            \"train\": train_metrics,\n",
    "            \"test\": test_metrics,\n",
    "            \"prompt\": prompt_versions,\n",
    "            \"raw\": raw_dfs\n",
    "        }\n",
    "    \n",
    "    # Initialize all feedback columns\n",
    "\n",
    "    while loops > 0:\n",
    "        print(f\"üìä Loop {curr_loop}: Optimizing prompt...\")\n",
    "        \n",
    "        task = generate_task(system_prompt)\n",
    "\n",
    "        train_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=train_dataset_id,\n",
    "            task=task,\n",
    "            evaluators=[output_evaluator, test_evaluator],\n",
    "            experiment_name=f\"train_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        train_metric = compute_metric(train_experiment_id, \"eval.output_evaluator.score\", \"accuracy\")\n",
    "        train_metrics.append(train_metric)\n",
    "        train_df = process_experiment(client, train_experiment_id, train_df, \"query\", \"output\", feedback_columns)\n",
    "        print(f\"‚úÖ Training {scorer}: {train_metric}\")\n",
    "        \n",
    "        optimizer = PromptLearningOptimizer(\n",
    "            prompt=system_prompt,  \n",
    "            model_choice=\"gpt-4o\",\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "        with open(\"../prompts/support_query_classification/annotations_prompt.txt\", \"r\") as file:\n",
    "            annotations_prompt = file.read()\n",
    "\n",
    "        annotations = optimizer.create_annotation(\n",
    "            system_prompt,\n",
    "            [\"query\"],\n",
    "            train_df,\n",
    "            feedback_columns,\n",
    "            [annotations_prompt],\n",
    "            \"output\",\n",
    "            \"ground_truth\"\n",
    "        )\n",
    "\n",
    "        system_prompt = optimizer.optimize(\n",
    "            train_df,\n",
    "            \"output\",\n",
    "            feedback_columns=feedback_columns,\n",
    "            context_size_k=90000,\n",
    "            annotations=annotations,\n",
    "        )\n",
    "        prompt_versions.append(system_prompt)\n",
    "\n",
    "        \n",
    "        test_experiment_id, _ = client.run_experiment(\n",
    "            space_id=ARIZE_SPACE_ID,\n",
    "            dataset_id=test_dataset_id,\n",
    "            task=generate_task(system_prompt),\n",
    "            evaluators=[test_evaluator],\n",
    "            experiment_name=f\"test_experiment_{curr_loop}\",\n",
    "            concurrency=10\n",
    "        )\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        test_metric = compute_metric(test_experiment_id, \"eval.test_evaluator.score\", scorer)\n",
    "        test_metrics.append(test_metric)\n",
    "\n",
    "        add_prompt_version(system_prompt, prompt_name, \"gpt-4o-mini-2024-07-18\", test_metric, curr_loop)\n",
    "        print(f\"‚úÖ Test {scorer}: {test_metric}\")\n",
    "\n",
    "        if test_metric >= threshold:\n",
    "            print(\"üéâ Prompt optimization met threshold!\")\n",
    "            break\n",
    "\n",
    "        loops -= 1\n",
    "        curr_loop += 1\n",
    "\n",
    "    return {\n",
    "        \"train\": train_metrics,\n",
    "        \"test\": test_metrics,\n",
    "        \"prompts\": prompt_versions,\n",
    "        \"raw\": raw_dfs\n",
    "    }\n",
    "\n",
    "# Main execution - use asyncio.run() to run the async function\n",
    "evaluators = [output_evaluator]\n",
    "feedback_columns = [\"correctness\", \"explanation\", \"confusion_reason\", \"error_type\", \"evidence_span\", \"prompt_fix_suggestion\"]\n",
    "result = optimize_loop(train_dataset, test_dataset, system_prompt, feedback_columns, loops=5, scorer=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d7627",
   "metadata": {},
   "source": [
    "# Prompt Optimized!\n",
    "\n",
    "The code below picks the prompt with the highest score on the test set, and displays the training/test metrics and delta for that prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd42533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best index based on highest test accuracy\n",
    "best_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n",
    "\n",
    "# Retrieve values\n",
    "best_prompt = result[\"prompts\"][best_idx - 1]\n",
    "best_test_acc = result[\"test\"][best_idx]\n",
    "best_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\n",
    "initial_test_acc = result[\"test\"][0]\n",
    "initial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüîç Best Prompt Found:\")\n",
    "print(best_prompt)\n",
    "print(f\"üß™ Initial Test Accuracy: {initial_test_acc}\")\n",
    "print(f\"üß™ Optimized Test Accuracy: {best_test_acc} (Œî {best_test_acc - initial_test_acc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt_learning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
