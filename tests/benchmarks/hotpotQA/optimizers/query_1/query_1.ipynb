{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e29cee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hover-benchmark/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "hotpot_root = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(hotpot_root))\n",
    "from application import generate_output, answer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a12263",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prompt = \"\"\"\n",
    "You are an expert in multi-hop question-answering and prompt optimization. \n",
    "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
    "\n",
    "====================\n",
    "üèóÔ∏è SYSTEM DESCRIPTION\n",
    "====================\n",
    "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
    "It operates as follows:\n",
    "\n",
    "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
    "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
    "3. **Retrieval (BM25-based Search):**\n",
    "   The search tool is a static BM25 retriever implemented as:\n",
    "\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
    "corpus = retriever.corpus\n",
    "\n",
    "def search(query: str, k: int) -> list[dict]:\n",
    "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
    "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
    "formatted_results = []\n",
    "for doc in results[0]:\n",
    "text = doc['text']\n",
    "if \" | \" not in text:\n",
    "return []\n",
    "title, content = text.split(\" | \", 1)\n",
    "formatted_results.append({\"title\": title, \"content\": content})\n",
    "return formatted_results\n",
    "\n",
    "- This search function retrieves top-{k} Wikipedia abstracts.\n",
    "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
    "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
    "\n",
    "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
    "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
    "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
    "\n",
    "====================\n",
    "üéØ OPTIMIZATION GOAL\n",
    "====================\n",
    "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
    "\n",
    "In particular:\n",
    "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
    "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
    "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
    "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
    "\n",
    "====================\n",
    "üìÑ YOUR INPUTS\n",
    "====================\n",
    "Below are the baseline prompts currently being used, along with example runs and feedback. \n",
    "Use these to identify weaknesses and propose improvements.\n",
    "\n",
    "************* start prompts *************\n",
    "{baseline_prompt}\n",
    "************* end prompts *************\n",
    "\n",
    "************* start example data *************\n",
    "{examples}\n",
    "************* end example data *************\n",
    "\n",
    "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
    "{annotations}\n",
    "\n",
    "====================\n",
    "üîß FINAL INSTRUCTIONS\n",
    "====================\n",
    "Iterate on the baseline prompts to produce a **new, improved prompts** that:\n",
    "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
    "- Produces clearer, more factually grounded reasoning and retrieval.\n",
    "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
    "- Remains faithful to the output schema and return format from the original prompt.\n",
    "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
    "- Return the prompts in the same formatting that they were given.\n",
    "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
    "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
    "\n",
    "NEW PROMPTS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c700eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"\n",
    "You are evaluating a multi-hop question-answering system composed of several reasoning and retrieval components.\n",
    "\n",
    "Below is the full execution trace of the system for one example.\n",
    "\n",
    "=====================\n",
    "üü© INPUT QUESTION\n",
    "{question}\n",
    "\n",
    "=====================\n",
    "üü¶ STEP 1 ‚Äî FIRST QUERY & PASSAGES\n",
    "Query #1:\n",
    "{query_1}\n",
    "\n",
    "Retrieved Passages (Hop 1):\n",
    "{passages_1}\n",
    "\n",
    "Summary #1 (based on the above passages and question):\n",
    "{summary_1}\n",
    "\n",
    "=====================\n",
    "üü™ STEP 2 ‚Äî SECOND QUERY & PASSAGES\n",
    "Query #2:\n",
    "{query_2}\n",
    "\n",
    "Retrieved Passages (Hop 2):\n",
    "{passages_2}\n",
    "\n",
    "Summary #2 (based on the above passages, question, and previous summary):\n",
    "{summary_2}\n",
    "\n",
    "=====================\n",
    "üü® FINAL ANSWER\n",
    "{final_answer}\n",
    "\n",
    "=====================\n",
    "üü• GROUND TRUTH\n",
    "Supporting Facts (Wikipedia titles): {supporting_facts}\n",
    "Gold Answer: {gold_answer}\n",
    "\n",
    "=====================\n",
    "üß† EVALUATION TASK\n",
    "\n",
    "Your task is to carefully analyze the full reasoning chain and provide *diagnostic feedback* about this system‚Äôs performance.\n",
    "\n",
    "Please:\n",
    "1. **Assess correctness** ‚Äî Is the final answer correct relative to the gold answer and evidence?\n",
    "2. **Explain reasoning quality** ‚Äî Did the system logically connect relevant facts across both hops?\n",
    "3. **Identify failure points** ‚Äî If wrong, where did the error arise (query formulation, retrieval precision/recall, summarization accuracy, or synthesis quality)?\n",
    "4. **Propose actionable improvements** ‚Äî Offer *specific, constructive* feedback for each editable module:\n",
    "   - **Query Generation:** Clarity, completeness, relevance, or missing entities/relations.\n",
    "   - **Passage Retrieval:** Recall or precision gaps (even if retrieval itself is static, point out how better queries could fix it).\n",
    "   - **Summary Generation:** Information loss, factual errors, or misinterpretations.\n",
    "   - **Final Answer Generation:** Completeness, factual grounding, consistency with summaries.\n",
    "\n",
    "Output your response strictly in this JSON-like format (no markdown, no extra text):\n",
    "\"correctness\" :\"<whether the final answer and passage retrieval is correct or incorrect>\",\n",
    "\"explanation\": \"<A detailed, structured analysis of why the system was correct or incorrect, highlighting causal chains and reasoning quality.>\",\n",
    "\"suggestions\": \"<Actionable improvement ideas for each component, concise but specific enough for an optimizer to learn from.>\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96747ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parents[3]\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "# Paths\n",
    "hotpot_root = Path.cwd().parents[1]  # .../hotpotqa\n",
    "sys.path.insert(0, str(hotpot_root))\n",
    "questions_path = Path.cwd() / \"questions_train_150.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c927650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import hotpot_evaluate_v1\n",
    "importlib.reload(hotpot_evaluate_v1)\n",
    "import hotpot_evaluate_v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from hotpot_evaluate_v1 import eval\n",
    "\n",
    "def run_train(prompts) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    runs prompts on training set. returns results and result metrics\n",
    "    \"\"\"\n",
    "    hotpot_root = Path.cwd().parents[1]\n",
    "    train_data = pd.read_json(hotpot_root / \"data/hotpot_train_v1.json\")\n",
    "    train_dataset = train_data.sample(150, random_state=42)\n",
    "    train_dataset.to_json(hotpot_root / \"data/hotpot_train_sample_150.json\", orient=\"records\")    \n",
    "\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    results, predictions = answer_dataset(train_dataset,\n",
    "                                                client,\n",
    "                                                prompts[\"create_query_1_prompt\"],\n",
    "                                                prompts[\"summarize_1_prompt\"],\n",
    "                                                prompts[\"create_query_2_prompt\"],\n",
    "                                                prompts[\"summarize_2_prompt\"],\n",
    "                                                prompts[\"final_answer_prompt\"]\n",
    "    )\n",
    "\n",
    "    with open(hotpot_root / \"predictions/predictions_train_150.json\", \"w\") as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "\n",
    "    return results, eval(hotpot_root / \"predictions/predictions_train_150.json\", hotpot_root / \"data/hotpot_train_sample_150.json\")\n",
    "\n",
    "def run_dev(prompts, dev_dataset, dev_path) -> dict:\n",
    "    \"\"\"\n",
    "    runs prompts on dev set. returns result metrics\n",
    "    \"\"\"\n",
    "    dev_dataset.to_json(dev_path, orient=\"records\")\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    questions, predictions = answer_dataset(dev_dataset,\n",
    "                                                client,\n",
    "                                                prompts[\"create_query_1_prompt\"],\n",
    "                                                prompts[\"summarize_1_prompt\"],\n",
    "                                                prompts[\"create_query_2_prompt\"],\n",
    "                                                prompts[\"summarize_2_prompt\"],\n",
    "                                                prompts[\"final_answer_prompt\"]\n",
    "    )\n",
    "    \n",
    "    with open(hotpot_root / \"predictions/predictions_dev_300.json\", \"w\") as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "\n",
    "    return eval(hotpot_root / \"predictions/predictions_dev_300.json\", dev_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89ce12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_generate\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "\n",
    "def evaluate_optimize(questions_df, prompts):\n",
    "    model = OpenAIModel(\n",
    "        model=\"gpt-4.1\",\n",
    "    )\n",
    "\n",
    "    eval_results = llm_generate(\n",
    "        dataframe=questions_df,\n",
    "        template=eval_prompt,\n",
    "        model=model,\n",
    "        concurrency=40,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    questions_df[\"evals\"] = eval_results[\"output\"]\n",
    "\n",
    "    prompts = f\"\"\"\n",
    "        create_query_1_prompt = {prompts[\"create_query_1_prompt\"]}\n",
    "        summarize_1_prompt = {prompts[\"summarize_1_prompt\"]}\n",
    "        create_query_2_prompt = {prompts[\"create_query_2_prompt\"]}\n",
    "        summarize_2_prompt = {prompts[\"summarize_2_prompt\"]}\n",
    "        final_answer_prompt = {prompts[\"final_answer_prompt\"]}\n",
    "    \"\"\"\n",
    "    questions_df[\"queries\"] = questions_df[\"query_1\"] + questions_df[\"query_2\"]\n",
    "\n",
    "    optimizer = PromptLearningOptimizer(\n",
    "        prompt=prompts,\n",
    "        model_choice=\"gpt-5\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        meta_prompt=meta_prompt\n",
    "    )\n",
    "\n",
    "    optimized_prompt = optimizer.optimize(\n",
    "        dataset=questions_df,\n",
    "        output_column=\"final_answer\",\n",
    "        feedback_columns=[\"query_1\", \"query_2\", \"passages_1\", \"passages_2\", \"evals\"],\n",
    "        context_size_k=100000\n",
    "    )\n",
    "\n",
    "    print(optimized_prompt)\n",
    "    return optimized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ac3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, textwrap\n",
    "\n",
    "def parse_prompts(s: str, keys=None):\n",
    "    if keys is None:\n",
    "        keys = [\n",
    "            \"create_query_1_prompt\",\n",
    "            \"summarize_1_prompt\",\n",
    "            \"create_query_2_prompt\",\n",
    "            \"summarize_2_prompt\",\n",
    "            \"final_answer_prompt\",\n",
    "        ]\n",
    "    key_group = \"|\".join(map(re.escape, keys))\n",
    "    pattern = rf'(?sm)^\\s*({key_group})\\s*=\\s*(.*?)(?=^\\s*(?:{key_group})\\s*=|\\Z)'\n",
    "    out = {}\n",
    "    for key, block in re.findall(pattern, s):\n",
    "        b = block.strip()\n",
    "        if (b.startswith(('\"\"\"',\"'''\")) and b.endswith(('\"\"\"',\"'''\"))):\n",
    "            b = b[3:-3]\n",
    "        elif (b.startswith('\"') and b.endswith('\"')) or (b.startswith(\"'\") and b.endswith(\"'\")):\n",
    "            b = b[1:-1]\n",
    "        out[key] = textwrap.dedent(b).strip()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d022f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4859a308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:39<00:00 |  3.77it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:17<00:00 |  8.79it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.60it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:25<00:00 |  5.97it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:32<00:00 |  4.57it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.658888222888223, em: 0.5866666666666667, prec: 0.6722433862433863, recall: 0.6628232323232323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:34<00:00 |  4.32it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'question_id', 'gold_answer', 'supporting_facts', 'query_1', 'passages_1', 'summary_1', 'query_2', 'passages_2', 'summary_2', 'final_answer', 'evals', 'queries']\n",
      "\n",
      "üîß Creating batches with 100,000 token limit\n",
      "üìä Processing 150 examples in 3 batches\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 1/3: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 2/3: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 3/3: Optimized\n",
      "NEW PROMPTS:\n",
      "\n",
      "create_query_1_prompt = You are generating a single keyword-style search query for a BM25 retriever over Wikipedia abstracts. Use {question}.\n",
      "- Identify: (a) target entity/answer type (person/date/number/title/place/list/yes-no), (b) key entities and relations (author-of, member-of, birthplace, cast/role, founded year), (c) any constraints (timeframe, medium/type like song/album/film/novel/city/league, country).\n",
      "- Disambiguate aggressively:\n",
      "  - Put exact titles in quotes; add medium/type words and years if implied (e.g., \"The Other Side\" song vs album; \"A Death in Vienna\" novel).\n",
      "  - For people with common names, add role or affiliation (e.g., actor Tombstone, boxer).\n",
      "  - For places, add state/country or league/division.\n",
      "- Multi-hop awareness:\n",
      "  - If the question asks about a property of an entity that must first be identified (e.g., author of a work, team of a player), include both the known entity and the relation in one query (e.g., '\"A Death in Vienna\" novel author').\n",
      "  - For comparisons or ‚Äúwhich/what‚Äù between multiple entities, include all entities plus the property in one query.\n",
      "- Use relation keywords: author, creator, cast, role, birth date, birthplace, population 2010 census, founded year, member count, creators, owners, address.\n",
      "- Prefer franchise or continuity disambiguators when relevant (e.g., Disney, sequel, series, league).\n",
      "- Do not output code or pseudo-code. Output exactly one plain-text query string only (no extra punctuation besides quotes).\n",
      "Guidelines (examples, do not output these):\n",
      "- Q: Who wrote the novel that the film City of Ember is based on? -> Query: \"City of Ember\" film novel author Jeanne DuPrau\n",
      "- Q: Which magazine was published first, Harpies and Quines or Harper's Bazaar? -> Query: \"Harpies and Quines\" first issue year vs \"Harper's Bazaar\" 1867 magazine\n",
      "- Q: The Other Side was released by the band founded in what year? -> Query: \"The Other Side\" song band founding year OR \"The Other Side\" album 1927 band founding year (include medium)\n",
      "- Q: How many novels has the author of \"A Death in Vienna\" written? -> Query: \"A Death in Vienna\" novel author bibliography number of novels\n",
      "- Q: Which actor from 'Tombstone' was born May 17, 1955? -> Query: \"Tombstone\" film cast actor born May 17, 1955\n",
      "- Q: Which brothel originally known as Mustang Bridge Ranch is in Clark, Nevada? -> Query: Clark Nevada brothel \"Mustang Bridge Ranch\" Mustang Ranch\n",
      "- Q: What is the address of the theatre near the Old Post Office Block in Manchester? -> Query: Palace Theatre Manchester New Hampshire address \"Hanover Street\"\n",
      "\n",
      "summarize_1_prompt = Read {passages_1} and extract only facts that help answer {question}.\n",
      "- Write concise bullets as subject‚Äìverb‚Äìobject triples. Prefix each bullet with the source page title in brackets. Include medium/type and years where relevant, e.g., [The Other Side (1927 album)] is a 1990 album by 1927 (band).\n",
      "- Preserve exact titles, names, dates, numbers; include aliases if present.\n",
      "- Explicitly note ambiguities or multiple candidates and how they differ (e.g., song vs album; different authors with same title).\n",
      "- Prefer authoritative sources (page title exactly matches the entity) but list all candidates if uncertain.\n",
      "- If the question implies constraints (timeframe, franchise/continuity, jurisdiction), add a bullet:\n",
      "  - Question constraints: <state any constraints inferred from the question (e.g., medium, year, timeframe like ‚Äúduring 1927, 1930‚Äù, franchise like Disney, census year 2010)>\n",
      "- End with two lines:\n",
      "  - Known entities: <exact titles/names identified so far (include medium/type)>\n",
      "  - Next-hop focus: <specific missing link to look up next (e.g., \"identify author of 'A Death in Vienna' (novel)\", \"Boston Red Sox World Series wins during 1927 or 1930\", \"Baloo second Disney film production year\", \"address of Palace Theatre Manchester NH\")>\n",
      "- Do not speculate or add unrelated background.\n",
      "\n",
      "create_query_2_prompt = Use {question} and {summary_1} to generate a single improved search query.\n",
      "- Use exact strings from Known entities; directly target the gap in Next-hop focus.\n",
      "- Add strong disambiguators: medium/type (song/album/film/novel/league/city), franchise (Disney), roles (author/cast), locations (city/state/country), and years/timeframes mentioned under Question constraints.\n",
      "- For comparisons, include all entities plus the property (e.g., \"first issue year\", \"older than\").\n",
      "- For canonical properties (member count, founding year, population 2010), query the exact entity page plus the property keywords (e.g., \"Boston Red Sox\" World Series championships; \"Ivy League\" number of schools).\n",
      "- If ambiguity remains, include alternative titles/aliases in quotes in the same query.\n",
      "- Do not output code/SQL. Output exactly one plain-text query string only.\n",
      "Guidelines (examples, do not output these):\n",
      "- Next-hop: EastEnders creators -> Query: \"EastEnders\" creators Julia Smith Tony Holland\n",
      "- Next-hop: identify '71 actor and Peaky Blinders role -> Query: \"'71\" film cast Peaky Blinders role Arthur Shelby Paul Anderson\n",
      "- Next-hop: Baloo second Disney movie production year -> Query: Baloo character Disney \"The Jungle Book 2\" 2003\n",
      "- Next-hop: owner/operator of Las Vegas Convention Center -> Query: \"Las Vegas Convention Center\" owner operator \"Las Vegas Convention and Visitors Authority\"\n",
      "- Next-hop: author of \"A Death in Vienna\" (novel) -> Query: \"A Death in Vienna\" novel author Daniel Silva bibliography number of novels\n",
      "\n",
      "summarize_2_prompt = Read {passages_2} in light of {question} and {summary_1}.\n",
      "- Compile final, question-focused bullets with exact names/dates/titles/numbers. Prefix each bullet with the source page title in brackets.\n",
      "- Resolve ambiguity by preferring the most specific, authoritative passage (exact entity page or the page for the property itself). Note conflicts briefly and state which source is most direct/specific and why (e.g., city page for population, film page for cast/role, award page for recipients).\n",
      "- Ensure constraints from Question constraints are satisfied (e.g., timeframe during a player‚Äôs tenure, Disney continuity, census year).\n",
      "- If multiple candidates remain, list them with distinguishing details.\n",
      "- End with one line: Answer candidate(s): <verbatim minimal candidate value(s) complying with constraints>.\n",
      "- Produce only the summary.\n",
      "\n",
      "final_answer_prompt = Given {question}, {summary_1}, {summary_2}, output the minimal, precise answer only.\n",
      "- Determine the answer type (yes/no, person, date, number, title, place, list) and extract it verbatim from the summaries.\n",
      "- Answer only if the exact value appears explicitly and matches any constraints (timeframe, medium/franchise, location). If evidence conflicts or is missing after two hops, output \"Unknown\".\n",
      "- When multiple candidates exist but one is best supported by the most authoritative, most specific source (per summarize_2), choose that one.\n",
      "- For yes/no, output \"yes\" or \"no\" only. For names, dates, numbers, titles, or lists, output only that content (comma-separated for lists) with original capitalization/spelling.\n",
      "- Prefer exact phrasing expected by the question (e.g., \"Mustang Ranch\", \"August 25, 1867\"). For counts, output the numeral as stated in the selected passage that satisfies constraints.\n",
      "- Do not include any extra words, punctuation, or explanations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 (99.7%) | ‚è≥ 00:42<00:00 | 10.44it/s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:38<00:00 |  7.82it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:27<00:00 | 10.74it/s\n",
      "                                                                       \n",
      "                                                                       \n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 02:33<00:00 | 10.44it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.52it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5788519791755087, em: 0.47333333333333333, prec: 0.5767152657284238, recall: 0.6198631553631554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 03:10<00:00 |  1.58it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 01:16<00:00 |  3.94it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.67it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:18<00:00 |  8.24it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:19<00:00 |  7.75it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:21<00:00 |  6.99it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.61it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.6935193325193325, em: 0.58, prec: 0.6891111111111111, recall: 0.7247222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:36<00:00 |  4.10it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'question_id', 'gold_answer', 'supporting_facts', 'query_1', 'passages_1', 'summary_1', 'query_2', 'passages_2', 'summary_2', 'final_answer', 'evals', 'queries']\n",
      "\n",
      "üîß Creating batches with 100,000 token limit\n",
      "üìä Processing 150 examples in 3 batches\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 1/3: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 2/3: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 3/3: Optimized\n",
      "create_query_1_prompt = You are generating a single high-recall, keyword-style search query for a BM25 retriever over Wikipedia abstracts. Use {question}.\n",
      "- Objective: write one entity-rich query that surfaces the core entities AND the likely pivot pages (exact entity page, list/filmography/discography/roster/awards pages, cast/role pages, founding/formation pages) needed to answer the question.\n",
      "- First, identify: (a) answer type (person/date/number/title/place/list/yes-no/category/comparison), (b) all key entities and relations (author-of, creator-of, member-of, birthplace, award/award category, founding/charter year, publication/formation year, cast/role, nickname/alias, original/birth name), (c) constraints (timeframe/year, medium/type such as film/novel/album/magazine/comic/video game/city/league, jurisdiction/country/state/council, franchise/continuity).\n",
      "- Disambiguate aggressively with exact strings and type words:\n",
      "  - Put exact titles/names in quotes; add medium/type in parentheses where canonical (e.g., Editors (band), \"Der Wildsch√ºtz\" opera, \"Peak FM (North Derbyshire)\" radio station).\n",
      "  - For ambiguous names or punctuation-heavy titles, include normalized OR-variants in quotes within the same query: e.g., '\"OK! (magazine)\" OR \"OK magazine\"'; include accented/unaccented or punctuation-stripped variants when helpful.\n",
      "  - For people with common names, add role/affiliation (actor, footballer, DJ, cosmonaut) and one notable work in quotes.\n",
      "  - For works with common titles, add medium and artist/franchise (e.g., '\"The Other Side\" song OR single artist band').\n",
      "- Recall-first multi-hop strategy in one query:\n",
      "  - If the answer depends on a property of a second entity, include both the pivot entity and the property, and target overview/list pages: discography, filmography, roster, list of X, award pages, founding pages.\n",
      "  - Include strong synonyms and near-variants to avoid over-constraining: album/compilation/EP/single; founded/established/formation year; birthplace/place of birth/hometown; publisher/owner/operator; first issue year/publication year/release date; headquarters city/head office.\n",
      "  - For comparisons (‚Äúwhich/what ‚Ä¶ first/older/founded‚Äù), include all entities plus the property and comparison keywords: earlier than/older than/first issue year/published first.\n",
      "  - For timelines (‚Äúworked before/joined from‚Äù), include the person + ‚Äúprevious club‚Äù OR ‚Äúprior to‚Äù OR ‚Äúbefore‚Äù + the year/season + role (coach/manager).\n",
      "  - For counts/how many, include the canonical entity and ‚Äúnumber of‚Äù or ‚Äúhow many‚Äù plus the container entity keyword (e.g., schools, albums, members).\n",
      "  - For location questions (‚Äúwhere/which city/country‚Äù), include the entity name plus ‚Äúcountry‚Äù OR ‚Äúlocated in‚Äù OR ‚Äúheadquarters city‚Äù.\n",
      "- Authority targeting and breadth control:\n",
      "  - Prefer exact entity pages, then official list/award/roster pages, then overview pages. If ambiguity remains, include multiple plausible candidates joined by OR in a single query.\n",
      "- Do not output code or explanations. Output exactly one plain-text query string only (no trailing punctuation beyond quotes).\n",
      "Guidelines (examples, do not output these):\n",
      "- Q: Which was published first, The Fader or OK!? -> Query: '\"The Fader\" magazine first issue year OR founding year vs \"OK! (magazine)\" OR \"OK magazine\" first issue year'\n",
      "- Q: Who was born first, Alex Segal or Mika Kaurism√§ki? -> Query: '\"Alex Segal\" birth year OR born vs \"Mika Kaurism√§ki\" born 1955 birth year'\n",
      "- Q: The Other Side was released by the band founded in what year? -> Query: '\"The Other Side\" song OR single artist band founding year formation year'\n",
      "- Q: Which actor from '71 also played Arthur Shelby in Peaky Blinders? -> Query: '\"\\'71\" (film) cast actor \"Arthur Shelby\" \"Peaky Blinders\" Paul Anderson'\n",
      "- Q: Panathinaikos F.C.‚Äôs new coach worked where before? -> Query: '\"Henk ten Cate\" managerial career previous club prior to 2008 Panathinaikos coach before 2008'\n",
      "\n",
      "\n",
      "summarize_1_prompt = Read {passages_1} and extract only facts that help answer {question}.\n",
      "- Write concise bullets as subject‚Äìverb‚Äìobject triples. Prefix each bullet with the source page title in brackets. Include medium/type and years where relevant (e.g., [The Other Side (1927 album)] is a 1990 album by 1927 (band)).\n",
      "- Preserve exact titles, names, dates, numbers; include aliases if present (birth/original names, nicknames). Include both punctuated and normalized forms where relevant (e.g., OK! (magazine) / OK magazine).\n",
      "- For each bullet, indicate authority in parentheses at the end: (exact entity page / official list/award/roster page / overview page / unrelated).\n",
      "- Explicitly note ambiguities, conflicts, or multiple candidates and how they differ (song vs album; magazine vs manga; city vs council; studio vs compilation; standard vs extended measurements).\n",
      "- If the question is a comparison (older/first/which), list the deciding property for each entity side-by-side (e.g., first issue year for both magazines; birth years for both people).\n",
      "- If a key entity implied by the question is missing (e.g., second item in a comparison; the pivot entity like an author or owner; the specific cast/role), add a bullet:\n",
      "  - Missing evidence: <state the missing entity/page/property we still need>.\n",
      "- If the question implies constraints (timeframe, franchise/continuity, jurisdiction, revived/relaunched title, award category), add a bullet:\n",
      "  - Question constraints: <state constraints verbatim>.\n",
      "- Add an ‚ÄúEntity map‚Äù bullet to link roles across entities when helpful (work -> author; actor -> role; team -> league; suburb -> council -> establishment date).\n",
      "- End with two lines:\n",
      "  - Known entities: <exact titles/names identified so far (include medium/type) and authoritative variants (Editors (band), \"Peak FM (North Derbyshire)\", OK! (magazine) / OK magazine)>\n",
      "  - Next-hop focus: <precise gap to look up next (e.g., \"publication years for both The Fader and OK! (magazine)\", \"Henk ten Cate previous club before June 2008 (managerial timeline)\", \"is 'The Other Side' the Aerosmith or Pendulum song (work page)\", \"owner of Peak FM and owner HQ city\", \"formation year of Editors (band)\")>\n",
      "- Do not speculate or add unrelated background. If no relevant facts exist in the passages, state Missing evidence accordingly.\n",
      "\n",
      "\n",
      "create_query_2_prompt = Use {question} and {summary_1} to generate a single improved search query.\n",
      "- Target the exact gap in Next-hop focus using exact strings from Known entities; include disambiguators (medium/type qualifiers in parentheses), roles (author/cast/composer/librettist/coach), and jurisdictions/years from Question constraints.\n",
      "- Include normalized OR-variants for ambiguous or punctuated names directly in the same query (e.g., '\"OK! (magazine)\" OR \"OK magazine\"').\n",
      "- Calibrate breadth:\n",
      "  - If Missing evidence indicates we lack the key pivot entity (author/publisher/owner/actor/artist), query directly for that entity and the property (e.g., '\"A Death in Vienna\" novel author'; '\"Peak FM (North Derbyshire)\" owner headquarters city').\n",
      "  - If the first hop over-constrained (e.g., only promo/compilation surfaced), broaden with synonyms and list pages: discography/filmography/roster/‚Äúlist of‚Äù/awards/career timeline.\n",
      "  - For timeline questions, include prior-to/previous/before terms plus the year/season (e.g., '\"Henk ten Cate\" previous club before 2008 Panathinaikos').\n",
      "- For comparisons, include all entities plus the property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublished first‚Äù, ‚Äúolder than‚Äù, ‚Äúbirth year‚Äù).\n",
      "- For canonical properties (member count, founding/charter year, population 2010, address), query the exact entity page plus property keywords (e.g., '\"Ivy League\" number of schools', '\"Florida Panthers\" founded 1993 founding year').\n",
      "- For lists or set membership, include ‚Äúlist of‚Äù, ‚Äúcomplete list‚Äù, or the container entity (e.g., 'list of suburbs \"City of Port Adelaide Enfield\" north-western').\n",
      "- Avoid special-case extensions unless asked (e.g., for river lengths prefer standard ‚Äúlength‚Äù rather than tributary-extended totals).\n",
      "- Do not output code/SQL. Output exactly one plain-text query string only.\n",
      "Guidelines (examples, do not output these):\n",
      "- Next-hop: publication years for The Fader and OK! -> Query: '\"The Fader\" magazine first issue year OR founding year vs \"OK! (magazine)\" OR \"OK magazine\" first issue year'\n",
      "- Next-hop: identify '71 actor and Peaky Blinders role -> Query: '\"\\'71\" (film) cast actor \"Arthur Shelby\" \"Peaky Blinders\" Paul Anderson'\n",
      "- Next-hop: Red Hot Chili Peppers 1989 album on EMI (studio vs compilation) -> Query: '\"Red Hot Chili Peppers\" discography 1989 album \"Mother\\'s Milk\" EMI studio album'\n",
      "- Next-hop: owner of Peak FM and owner HQ city -> Query: '\"Peak FM (North Derbyshire)\" owner \"Wireless Group\" headquarters city Belfast'\n",
      "- Next-hop: who was born first -> Query: '\"Alex Segal\" birth year OR born vs \"Mika Kaurism√§ki\" birth year born 1955'\n",
      "\n",
      "\n",
      "summarize_2_prompt = Read {passages_2} in light of {question} and {summary_1}.\n",
      "- Compile final, question-focused bullets with exact names/dates/titles/numbers. Prefix each bullet with the source page title in brackets and indicate authority (exact entity/list/overview/unrelated).\n",
      "- Resolve ambiguity by preferring the most specific, authoritative passage (exact entity page, official list/roster/award page, or the page for the property itself). Note conflicts briefly and state which source is most direct/specific and why (e.g., main entity page for standard length; film page for cast/role; award page for recipients).\n",
      "- Ensure all constraints from Question constraints are satisfied (timeframe during a player‚Äôs tenure, correct medium/franchise, revived/relaunched year, chartered year, correct jurisdiction/council).\n",
      "- For comparisons, explicitly compute which satisfies the comparison and state the deciding facts side-by-side (e.g., first issue year 1867 vs 1994 ‚Üí Harper‚Äôs Bazaar first; birth years 1926 vs 1955 ‚Üí 1926 is older). Add a ‚ÄúComputation‚Äù bullet showing the decision.\n",
      "- For category/slot-fill questions (e.g., ‚Äúare a breed of what?‚Äù, ‚ÄúJohn Smith ________ in Tadcaster‚Äù), include both the general class (dog, brewery) and any specific subgroup (working dog) if present; prefer the most general class when the question asks ‚Äúwhat kind of/what‚Äù.\n",
      "- For location ‚Äúwhere‚Äù questions, return the place (city/region/country), not distances or directions.\n",
      "- If the first hop lacked a key entity and it still does not appear, add a bullet:\n",
      "  - Remaining gap: <state missing entity/property>.\n",
      "- For measurements with multiple reported values, prefer the standard/conventional value from the canonical entity page unless the question specifies an alternative (e.g., do not include tributary-extended river lengths unless asked).\n",
      "- End with one line: Answer candidate(s): <verbatim minimal candidate value(s) complying with constraints>. If truly unresolved, include ‚ÄúUnknown‚Äù.\n",
      "- Produce only the summary.\n",
      "\n",
      "\n",
      "final_answer_prompt = Given {question}, {summary_1}, {summary_2}, output the minimal, precise answer only.\n",
      "- Determine the answer type (yes/no, person, date, number, title, place, list, category/comparison) and extract it verbatim from the summaries. For comparisons, output the single winning entity/value computed in summarize_2.\n",
      "- Output an answer only if the exact value appears explicitly in the summaries (or is a direct comparison between explicit values) and matches all constraints (timeframe, medium/franchise, location, chartered year). Do not use outside knowledge.\n",
      "- If summaries flag Missing evidence/Remaining gap, or if one side of a required comparison is absent, or if the value is not explicitly supported, output \"Unknown\".\n",
      "- When multiple candidates exist but one is best supported by the most authoritative, most specific source (per summarize_2), choose that one; if multiple equal candidates remain for a singular ‚Äúwhich/what‚Äù question, output \"Unknown\".\n",
      "- For names or titles that include nicknames/aliases in the selected passage, include them as shown (e.g., Alan \"Sparrow\" Taylor).\n",
      "- For fill-in-the-blank/type questions (‚ÄúJohn Smith ________ in Tadcaster‚Äù), return the class/type (e.g., brewery) rather than a full proper name if that best fills the blank and is explicitly stated in the summaries.\n",
      "- For ‚Äúhow many‚Äù questions, output the numeral only. For comparisons (‚Äúwhich published first/older than‚Äù), output the single winning entity/value only.\n",
      "- Do not include any extra words, punctuation, or explanations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.34it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:32<00:00 |  9.33it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:27<00:00 | 10.72it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:32<00:00 |  9.34it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:27<00:00 | 10.82it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5163326036392959, em: 0.42, prec: 0.5122939421689422, recall: 0.5511450216450218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:16<00:00 |  9.25it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:25<00:00 |  5.82it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:16<00:00 |  9.36it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:22<00:00 |  6.64it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.49it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.6146190476190476, em: 0.54, prec: 0.6160331890331892, recall: 0.6360555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 149/150 (99.3%) | ‚è≥ 00:54<00:02 |  2.30s/it "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 01:00<00:00 |  5.90s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'question_id', 'gold_answer', 'supporting_facts', 'query_1', 'passages_1', 'summary_1', 'query_2', 'passages_2', 'summary_2', 'final_answer', 'evals', 'queries']\n",
      "\n",
      "üîß Creating batches with 100,000 token limit\n",
      "üìä Processing 150 examples in 4 batches\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 1/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 2/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 3/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 4/4: Optimized\n",
      "************* start prompts *************\n",
      "************* start prompts *************\n",
      "************* start prompts *************\n",
      "************* start prompts *************\n",
      "\n",
      "create_query_1_prompt = You are generating a single high-recall, keyword-style search query for a BM25 retriever over Wikipedia abstracts. Use {question}.\n",
      "- Objective: write one entity-rich query that surfaces the core entities AND the likely pivot pages (exact entity page; list/filmography/discography/roster/awards pages; cast/role pages; formation/founding/establishment pages; acquisition/ownership pages) needed to answer the question.\n",
      "- First, identify: (a) answer type (person/date/year/number/title/place/list/yes-no/category/comparison), (b) all key entities and relations (author/screenwriter; composer/librettist; member-of; cast/role; birthplace/nationality vs country; award/award category; founding/formation year vs first season; publication/release year; owner/acquirer; list/roster membership), (c) constraints (timeframe/year; medium/type such as film/novel/album/song/TV series/magazine/board game/video game/city/league; jurisdiction/country/state/council; birth/death date filters; ‚Äúmore than/older than/first/which supports more‚Äù comparisons).\n",
      "- Disambiguate aggressively with exact strings and type words:\n",
      "  - Put exact titles/names in quotes; add medium/type and year where canonical: e.g., \"Tombstone\" (1993 film), \"OK! (magazine)\".\n",
      "  - For ambiguous titles, include medium + year + creator/artist OR add multiple plausible variants joined by OR in one query: e.g., '\"The Other Side\" song OR single OR track'.\n",
      "  - For people with common names, add role/affiliation and one notable work in quotes (actor; footballer; DJ; astronaut; composer).\n",
      "  - Include normalized OR-variants for punctuated/named entities (OK! (magazine) OR \"OK magazine\"; \"Ain't\" OR \"Aint\").\n",
      "- Recall-first multi-hop strategy in one query:\n",
      "  - If the answer depends on a property of a second entity, include both the pivot entity and the property, and target overview/list pages: filmography/discography/roster, cast pages, award pages, founding/formation pages, acquisition/ownership pages.\n",
      "  - For role-bridging questions, include: '\"X\" (year/medium) cast OR actors' plus filmography keywords (filmography, appeared in) and property synonyms (e.g., extraterrestrial OR alien OR UFO; owner OR acquired by; number of players).\n",
      "  - For comparisons, include all entities plus the deciding property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublished first‚Äù, ‚Äúolder than‚Äù, ‚Äúbirth year‚Äù, ‚Äúnumber of players‚Äù, ‚Äúplayer count‚Äù).\n",
      "  - For formation/founding, include the entity page + property keywords: founded OR formed OR established OR \"first season\" OR \"joined [league]\".\n",
      "  - For screenwriter/composer/librettist, include entity and role keywords: writer OR screenwriter OR \"written by\"; composer OR \"music by\"; librettist.\n",
      "  - For list/set membership (suburbs within a council; games that support more than two players), include ‚Äúlist of‚Äù or ‚Äúroster‚Äù or the container entity (league/council/city) when relevant.\n",
      "  - For nationality vs country, include ‚Äúnationality‚Äù OR ‚Äúborn‚Äù OR ‚Äúborn in‚Äù and the person‚Äôs exact name.\n",
      "- Precision controls for BM25:\n",
      "  - Prefer exact entity pages and specific property keywords (‚Äúcast‚Äù, ‚Äúscreenwriter‚Äù, ‚Äúplayer count‚Äù, ‚Äúfirst issue‚Äù, ‚Äúfounded‚Äù, ‚Äúowner‚Äù, ‚Äúacquired by‚Äù, ‚Äúplatforms‚Äù, ‚Äúrelease‚Äù).\n",
      "  - Avoid injecting unrelated brand/manufacturer/platform family names unless they are the property being asked (e.g., don‚Äôt add ‚ÄúSony Interactive Entertainment‚Äù unless ownership/brand is the focus).\n",
      "  - Prefer property words over vague comparison words alone.\n",
      "- Geographic and naming disambiguation:\n",
      "  - If a place or team name could refer to multiple things, include qualifiers (city/state/county/league) and nearby anchors; for sports teams include ‚ÄúNHL‚Äù/‚ÄúNBA‚Äù/‚ÄúMLS‚Äù/‚ÄúPremier League‚Äù as appropriate.\n",
      "- Do not output code or explanations. Output exactly one plain-text query string only (no trailing punctuation beyond quotes).\n",
      "\n",
      "Guidelines (examples, do not output these):\n",
      "- Q: Which was published first, The Fader or OK!? -> Query: '\"The Fader\" magazine first issue year OR founding year vs \"OK! (magazine)\" OR \"OK magazine\" first issue year'\n",
      "- Q: Which actor from Tombstone was born on May 17, 1955? -> Query: '\"Tombstone\" (1993 film) cast actor born \"May 17, 1955\"'\n",
      "- Q: Which game can be played by more than 2 players at a time, Game of the Generals or Ludo? -> Query: '\"Game of the Generals\" number of players OR \"player count\" vs \"Ludo\" board game players OR \"player count\"'\n",
      "- Q: What extraterrestrial creature is featured in a film in which the \"Happy New Year\" actor also appears? -> Query: '\"Happy New Year\" (2014 film) cast actor filmography extraterrestrial OR alien creature'\n",
      "- Q: The Other Side was released by the band founded in what year? -> Query: '\"The Other Side\" song OR single artist band founding year OR formation year discography'\n",
      "\n",
      "summarize_1_prompt = Read {passages_1} and extract only facts that help answer {question}.\n",
      "- Write concise bullets as subject‚Äìverb‚Äìobject triples. Prefix each bullet with the source page title in brackets. Include medium/type and years where relevant (e.g., [The Other Side (1927 album)] is a 1990 album by 1927 (band)).\n",
      "- Preserve exact titles, names, dates, numbers; include aliases if present (birth/original names, nicknames). Include punctuated and normalized forms where relevant (OK! (magazine) / OK magazine).\n",
      "- After each bullet, mark source authority in parentheses: (exact entity page / official list/award/roster/cast/property page / overview page / unrelated).\n",
      "- Explicitly note ambiguities, conflicts, multiple candidates, or homonymous titles (song vs album; magazine vs manga; 1983 film vs 2009 film) and how they differ (year, medium, country, creator).\n",
      "- For comparison questions (older/first/which), list the deciding property for each entity side-by-side with the entity names.\n",
      "- If a key entity implied by the question is missing (second item in a comparison; the pivot entity like an author, owner, screenwriter/composer/librettist; a specific cast/role; the team‚Äôs name), add:\n",
      "  - Missing evidence: <state the missing entity/page/property we still need>.\n",
      "- If the question implies constraints (timeframe, franchise/continuity, jurisdiction/council/county, revived/relaunched title, canonical vs first season dates, category vs subtype, ‚Äúmore than 2 players‚Äù), add:\n",
      "  - Question constraints: <state constraints verbatim>.\n",
      "- Add an ‚ÄúEntity map‚Äù bullet to link roles across entities when helpful (work -> author/screenwriter/composer/librettist; actor -> role; team -> league; suburb -> council; game -> player count; person -> nationality/birthplace).\n",
      "- Prefer canonical/standard values, but record all distinct values with their sources if conflicts appear.\n",
      "- End with two lines:\n",
      "  - Known entities: <exact titles/names identified so far (include medium/type) and authoritative variants>\n",
      "  - Next-hop focus: <precise gap to look up next (e.g., \"publication years for The Fader and OK! (magazine)\", \"identify actor from Tombstone born May 17, 1955\", \"player counts for Game of the Generals and Ludo\", \"screenwriter of Dirty Dancing (1987 film)\", \"owner and HQ city of Peak FM (North Derbyshire)\", \"traditional length of River Thames (miles)\")>\n",
      "- Do not speculate or add unrelated background; mark any non-pertinent passage as (unrelated). If no relevant facts exist in the passages, state Missing evidence accordingly.\n",
      "\n",
      "create_query_2_prompt = Use {question} and {summary_1} to generate a single improved search query.\n",
      "- Target the exact gap in Next-hop focus using exact strings from Known entities; include disambiguators (medium/type qualifiers in parentheses), roles (author/screenwriter/cast/composer/librettist/coach/owner), and jurisdictions/years from Question constraints.\n",
      "- Include normalized OR-variants for ambiguous or punctuated names directly in the same query (e.g., '\"OK! (magazine)\" OR \"OK magazine\"').\n",
      "- Calibrate breadth:\n",
      "  - If Missing evidence indicates we lack the key pivot entity (author/publisher/owner/actor/artist/composer/librettist/screenwriter/team name), query directly for that entity and the property (e.g., '\"A Death in Vienna\" novel author'; '\"Dirty Dancing\" (1987 film) writer OR screenwriter \"written by\"').\n",
      "  - If the first hop over-constrained or hit the wrong homonym, broaden or correct with year/medium/country and synonyms, and include multiple plausible variants joined by OR.\n",
      "  - For comparisons, include all entities plus the deciding property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublished first‚Äù, ‚Äúolder than‚Äù, ‚Äúbirth year‚Äù, ‚Äúnumber of players‚Äù, ‚Äúplayer count‚Äù).\n",
      "  - For lists or set membership (suburbs within a council; magazines someone was featured in), include ‚Äúlist of‚Äù, ‚Äúcomplete list‚Äù, or the container entity (e.g., 'list of suburbs \"City of Port Adelaide Enfield\" north-western').\n",
      "  - For acquisition/ownership history, include ‚Äúacquired by‚Äù OR ‚Äúpurchased by‚Äù OR ‚Äúowner‚Äù and the organization/work name; if a person was featured in unspecified magazines first, query '<Person> featured in OR appeared in magazine' to get the magazine, then acquisition.\n",
      "  - For canonical properties (member count, founding/charter year vs first season, population 2010, traditional length; screenwriter of a film; composer/librettist of an opera; platforms of a game), query the exact entity page plus property keywords.\n",
      "- Precision controls:\n",
      "  - Prefer property words (‚Äúcast‚Äù, ‚Äúscreenwriter‚Äù, ‚Äúfirst issue‚Äù, ‚Äúfounded‚Äù, ‚Äúowner‚Äù, ‚Äúplayer count‚Äù, ‚Äúplatforms‚Äù, ‚Äúrelease year‚Äù, ‚Äúbirth‚Äù) over vague comparison words alone.\n",
      "  - Avoid adding off-topic brand/manufacturer names unless they are central to the property being queried.\n",
      "- Geographic/name-change handling: if Next-hop suggests alias or renamed entities (e.g., ‚ÄúThe AXIS‚Äù now ‚ÄúZappos Theater‚Äù), include both names with OR.\n",
      "- Avoid special-case code/SQL. Output exactly one plain-text query string only.\n",
      "\n",
      "Guidelines (examples, do not output these):\n",
      "- Next-hop: player counts for Game of the Generals and Ludo -> Query: '\"Game of the Generals\" number of players OR \"player count\" vs \"Ludo\" board game players OR \"player count\"'\n",
      "- Next-hop: identify '71 actor and Peaky Blinders role -> Query: '\"\\'71\" (2014 film) cast actor \"Arthur Shelby\" \"Peaky Blinders\" Paul Anderson'\n",
      "- Next-hop: actor from Tombstone born May 17, 1955 -> Query: '\"Tombstone\" (1993 film) cast actor born \"May 17, 1955\"'\n",
      "- Next-hop: screenwriter of Dirty Dancing -> Query: '\"Dirty Dancing\" (1987 film) writer OR screenwriter \"written by\"'\n",
      "- Next-hop: founding/first season year of Florida Panthers -> Query: '\"Florida Panthers\" founded year OR \"first season\" OR joined NHL 1993'\n",
      "- Next-hop: platforms of Rise of Mana (other than PS Vita) -> Query: '\"Rise of Mana\" platforms OR released for iOS OR Android'\n",
      "\n",
      "summarize_2_prompt = Read {passages_2} in light of {question} and {summary_1}.\n",
      "- Compile final, question-focused bullets with exact names/dates/titles/numbers. Prefix each bullet with the source page title in brackets and indicate authority (exact entity/list/overview/unrelated/property).\n",
      "- Resolve ambiguity by preferring the most specific, authoritative passage (exact entity page; official list/roster/cast/award page; property page; or the page for the property itself). Note conflicts briefly and state which source is most direct/specific and why (e.g., main entity page for standard length; film page for screenwriter/cast/role; award page for recipients; official roster/list for members).\n",
      "- Ensure all constraints from Question constraints are satisfied (timeframe during a player‚Äôs tenure; correct medium/franchise; revived/relaunched year; chartered vs first season; correct jurisdiction/council/county; correct film year vs similarly named works).\n",
      "- For comparisons, explicitly compute which satisfies the comparison and state the deciding facts side-by-side (e.g., player count: 2 vs 4; first issue year 1867 vs 1994 ‚Üí Harper‚Äôs Bazaar first; birth years 1926 vs 1955 ‚Üí 1926 is older). Add a ‚ÄúComputation‚Äù bullet showing the decision.\n",
      "- For category/slot-fill questions (e.g., ‚Äúare a breed of what?‚Äù, ‚Äúwhat kind of work‚Äù), include the minimal explicit class stated in the source. Prefer the broader species/category (‚ÄúDogs‚Äù) over subtypes unless the question asks for a subtype.\n",
      "- For location ‚Äúwhere‚Äù questions, return the place (city/region/country), not distances or directions.\n",
      "- If the first hop lacked a key entity and it still does not appear, add:\n",
      "  - Remaining gap: <state missing entity/property>.\n",
      "- For measurements or dates with multiple reported values, prefer the conventional/canonical value from the main entity page unless the question specifies otherwise. For sports teams ‚Äúfounded‚Äù with conflicting date vs year (e.g., exact award date vs first season), prefer the simple year on the main team page unless the question asks for the exact date.\n",
      "- End with one line: Answer candidate(s): <verbatim minimal candidate value(s) complying with constraints>. If truly unresolved or one side of a required comparison is absent, include ‚ÄúUnknown‚Äù.\n",
      "- Produce only the summary.\n",
      "\n",
      "final_answer_prompt = Given {question}, {summary_1}, {summary_2}, output the minimal, precise answer only.\n",
      "- Determine the answer type (yes/no, person, date, year, number, title, place, list, category/comparison, nationality vs country). Output exactly the value that matches the question.\n",
      "  - If the question asks ‚Äúwhat nationality,‚Äù return the demonym (e.g., Czech). If it asks ‚Äúfrom which country,‚Äù return the country name (e.g., Czech Republic).\n",
      "  - If the question asks ‚Äúa breed of what?‚Äù return the broad species/category (e.g., Dogs), not subtypes (e.g., herding dog).\n",
      "- Output an answer only if the exact value appears explicitly in the summaries (or is a direct comparison between explicit values) and matches all constraints (timeframe, medium/franchise, location/council/county, chartered vs first season, standard vs extended measurement).\n",
      "- If summaries flag Missing evidence/Remaining gap, or if one side of a required comparison is absent, or if the value is not explicitly supported, output \"Unknown\".\n",
      "- When multiple candidates exist, choose the one best supported by the most authoritative, most specific source (per summarize_2). If equal candidates remain for a singular ‚Äúwhich/what‚Äù question, output \"Unknown\".\n",
      "- Preserve names/titles exactly as shown in the selected passage (include nicknames/aliases if present, e.g., Alan \"Sparrow\" Taylor).\n",
      "- For ‚Äúhow many‚Äù questions, output the numeral only. For comparisons (‚Äúwhich published first/older than/which supports more players‚Äù), output the single winning entity/value only.\n",
      "- For sports teams ‚Äúfounded,‚Äù if both a precise charter/award date and a first season year are present, prefer the conventional year from the main team page unless the question requests the exact date.\n",
      "- Do not include any extra words, punctuation, or explanations.\n",
      "\n",
      "************* end prompts *************\n",
      "************* end prompts *************\n",
      "************* end prompts *************\n",
      "************* end prompts *************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.62it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:33<00:00 |  9.02it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:30<00:00 |  9.79it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:31<00:00 |  9.40it/s\n",
      "\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 12:26<00:00 |  5.90s/it \n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 12:26<00:00 |  5.90s/it "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5420433112884726, em: 0.4533333333333333, prec: 0.5433691678691679, recall: 0.5714175084175086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 12:29<00:00 |  5.00s/it\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:53<00:00 |  5.62it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:16<00:00 |  9.08it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 149/150 (99.3%) | ‚è≥ 00:39<00:00 |  1.51it/s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.46it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:21<00:00 |  6.83it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.71it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.642608309990663, em: 0.5666666666666667, prec: 0.6423174603174603, recall: 0.6632777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 02:30<00:00 |  4.98s/it \n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 02:30<00:00 |  4.98s/it "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'question_id', 'gold_answer', 'supporting_facts', 'query_1', 'passages_1', 'summary_1', 'query_2', 'passages_2', 'summary_2', 'final_answer', 'evals', 'queries']\n",
      "\n",
      "üîß Creating batches with 100,000 token limit\n",
      "üìä Processing 150 examples in 4 batches\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 1/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 2/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 3/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 4/4: Optimized\n",
      "create_query_1_prompt = You are generating a single high-recall, keyword-style search query for a BM25 retriever over Wikipedia abstracts. Use {question}.\n",
      "- Objective: write one entity-rich query that surfaces (1) all core entities and aliases, (2) the exact pivot pages (persons/films/albums/TV series; list/filmography/discography/roster/awards pages; ownership/parent company/founding/HQ pages), and (3) property keywords needed to answer the question.\n",
      "- From the question, explicitly extract:\n",
      "  (a) answer type (person/date/year/number/title/place/list/category/yes-no/comparison),\n",
      "  (b) all key entities with disambiguators (medium, year, country, league, club/organization),\n",
      "  (c) relations (cast/role/author/screenwriter/composer/librettist/owner/parent company/founding/founded by/head coach OR manager/HQ city/first issue/publication year/birth year/‚Äúrevived‚Äù OR ‚Äúrelaunched‚Äù OR ‚Äúsequel‚Äù/player count/platforms OR systems OR devices),\n",
      "  (d) constraints (timeframe/season/year; jurisdiction/city/county/state/country; renamed/alias; cross-entity bridging like ‚Äúthe singer of X‚Äù, ‚Äúthe actor from Y also appears in Z‚Äù; exclusion patterns like ‚Äúother than/besides/except X‚Äù to guide later filtering).\n",
      "- Disambiguation and aliases:\n",
      "  - Add medium and year where canonical: \"Tombstone\" (1993 film), \"OK! (magazine)\", '\"71\" (2014 film)'.\n",
      "  - Include creator/league/club when helpful: 'Panathinaikos F.C. head coach OR manager 2008 OR 2009'.\n",
      "  - Include renamed venues/entities with OR: 'The AXIS OR \"Zappos Theater\"'.\n",
      "  - Include normalized punctuation/diacritics variants with OR: \"OK! (magazine)\" OR \"OK magazine\"; \"Ain't\" OR \"Aint\"; \"Koi... Mil Gaya\" OR \"Koi Mil Gaya\".\n",
      "- Recall-first multi-hop strategy (compose one rich query):\n",
      "  - If the question references an entity by role (‚Äúsinger of X‚Äù, ‚Äúauthor of Y‚Äù, ‚Äúteam that drafted Z‚Äù), include both the work and the role: '\"Work Hard, Play Harder\" singer OR artist' with deciding property words (debut/year) if needed.\n",
      "  - For comparisons (‚Äúwhich published first/older than/more players‚Äù), include all entities plus decisive property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublication year‚Äù, ‚Äúplayer count‚Äù, ‚Äúbirth year‚Äù).\n",
      "  - For ownership/HQ questions, include the organization + 'owner OR parent company OR headquarters OR HQ city OR based in'.\n",
      "  - For coaching/manager changes, include club + 'head coach OR manager' + season/year window.\n",
      "  - For revival/relaunch, include 'revived OR relaunch OR revival' + year + title names.\n",
      "  - For actor-bridging into themed works, include cast/filmography + theme terms (alien OR extraterrestrial; vampire; robot).\n",
      "- Precision controls for BM25:\n",
      "  - Prefer exact entity pages + property words (‚Äúcast‚Äù, ‚Äúwriter/screenwriter‚Äù, ‚Äúcomposer‚Äù, ‚Äúlibrettist‚Äù, ‚Äúowner‚Äù, ‚Äúparent company‚Äù, ‚Äúfounded‚Äù, ‚Äúfirst issue‚Äù, ‚Äúpublication year‚Äù, ‚Äúbirth‚Äù, ‚Äúhead coach OR manager‚Äù, ‚Äúcounty‚Äù, ‚Äúheadquarters‚Äù, ‚Äúplayer count‚Äù, ‚Äúplatforms‚Äù OR ‚Äúsystems‚Äù OR ‚Äúdevices‚Äù, ‚Äúrelease year‚Äù).\n",
      "  - Add list/filmography/discography/roster/awards pages when they are the most likely source of the property.\n",
      "  - Avoid off-topic brands/platforms unless explicitly relevant.\n",
      "- Geographic/name disambiguation:\n",
      "  - If a place/team/council is ambiguous, add qualifiers (city/state/county/league/country) and nearby anchors.\n",
      "- Temporal scope:\n",
      "  - If the question asks ‚Äúhow many has team X won‚Äù, target all-time totals unless a timeframe is specified (then include the dates).\n",
      "- Output exactly one plain-text query string only (no trailing punctuation beyond quotes). No code or explanations.\n",
      "\n",
      "Guidelines (examples, do not output these):\n",
      "- Q: Which was published first, The Fader or OK!? -> Query: '\"The Fader\" magazine first issue year OR founding year vs \"OK! (magazine)\" OR \"OK magazine\" first issue year'\n",
      "- Q: Which actor from Tombstone was born on May 17, 1955? -> Query: '\"Tombstone\" (1993 film) cast actor born \"May 17, 1955\"'\n",
      "- Q: The Other Side was released by the band founded in what year? -> Query: '\"The Other Side\" song OR single artist band founding year OR formation year discography'\n",
      "- Q: Where did Panathinaikos F.C.'s new coach work before? -> Query: '\"Panathinaikos F.C.\" head coach OR manager appointed OR new coach 2008 OR 2009 previous club OR worked before'\n",
      "- Q: Which DC title revived in 2007 features Donna Troy? -> Query: '\"The Brave and the Bold\" (2007 comic) revived OR relaunch characters OR features Donna Troy'\n",
      "- Q: The Japanese action role-playing game Rise of Mana was developed for PlayStation Vita and what other devices? -> Query: '\"Rise of Mana\" video game platforms OR systems OR devices \"released for\" OR \"available on\" iOS OR Android'\n",
      "\n",
      "\n",
      "summarize_1_prompt = Read {passages_1} and extract only facts that help answer {question}.\n",
      "- Produce concise bullets as subject‚Äìverb‚Äìobject triples. Prefix each bullet with the source page title in brackets. Include medium/type and years where relevant.\n",
      "- Preserve exact titles, names, dates, numbers; include aliases if present (original names, nicknames, former names/renames).\n",
      "- Identify and record the entity type for each named item (film, TV series, album, song, comic, magazine, opera, video game, person, team/club, council, company, venue). Use this to resolve homonyms.\n",
      "- Note temporal scope explicitly: whether a fact is all-time, by season/year, or at a point in time.\n",
      "- Mark authority after each bullet: (exact entity page / official list/award/roster/cast/property page / overview page / unrelated).\n",
      "- Explicitly note ambiguities, conflicts, multiple candidates, or homonymous titles (song vs album; magazine vs manga; 1983 vs 2009 film) and how they differ (year, medium, country).\n",
      "- For comparison/ordering questions, list the deciding property for each entity side-by-side.\n",
      "- If a key pivot or property is missing (e.g., second item in a comparison; the person/owner/publisher/author/composer/librettist/actor; HQ city; coach/manager; revived title), add:\n",
      "  - Missing evidence: <state the missing entity/page/property we still need>.\n",
      "- If the question implies constraints (timeframe/season, jurisdiction/city/county/state/country, revived/relaunched year, alias/rename, exclusion patterns like ‚Äúother than/besides/except X‚Äù), add:\n",
      "  - Question constraints: <state constraints verbatim and list any explicit exclusions to be filtered from the final answer>.\n",
      "- Add an ‚ÄúEntity map‚Äù bullet to link roles across entities when helpful (work -> author/screenwriter/composer/librettist; song -> artist; actor -> role; team -> league; owner -> HQ city; venue -> hotel/casino/parent company; person -> occupation; club -> head coach/manager).\n",
      "- Prefer explicit phrases from sources over labels when the question asks ‚Äúbelieved in what/what kind of/are a breed of what?‚Äù (quote the defining phrase if present).\n",
      "- If a descriptive modifier in the question appears unverified or possibly inaccurate, still record the decisive fact and flag the descriptor as unverified.\n",
      "- End with two lines:\n",
      "  - Known entities: <exact titles/names identified so far (include medium/type) and authoritative variants>\n",
      "  - Next-hop focus: <precise gap to look up next (e.g., \"publication years for The Fader and OK! (magazine)\", \"identify actor from Tombstone born May 17, 1955\", \"Panathinaikos F.C. current/new head coach name + previous club\", \"which HNY (2014 film) cast member appears in a film featuring an extraterrestrial creature + name of that creature\", \"owner and HQ city of Peak FM‚Äôs parent (Wireless Group)\", \"revived DC titles in 2007 including The Brave and the Bold (2007 comic)\")>\n",
      "- Do not speculate or add unrelated background; mark any non-pertinent passage as (unrelated). If no relevant facts exist in the passages, state Missing evidence accordingly.\n",
      "\n",
      "\n",
      "create_query_2_prompt = Use {question} and {summary_1} to generate a single improved search query.\n",
      "- Target exactly the gap in Next-hop focus using exact strings from Known entities; include disambiguators (medium/type qualifiers in parentheses), roles (author/screenwriter/cast/composer/librettist/coach/head coach/manager/owner), organizations (parent company/headquarters), and jurisdictions/years from Question constraints.\n",
      "- Include normalized OR-variants for ambiguous or punctuated names directly in the same query (e.g., '\"OK! (magazine)\" OR \"OK magazine\"').\n",
      "- Calibrate breadth and correct homonyms:\n",
      "  - If Missing evidence indicates a key pivot entity is absent (author/publisher/owner/parent company/actor/artist/composer/librettist/screenwriter/team name/coach OR manager/HQ city), query directly for that entity and the property (e.g., '\"Panathinaikos F.C.\" head coach OR manager appointed 2008 OR 2009 previous club').\n",
      "  - If the first hop over-constrained or hit the wrong homonym, broaden or correct with year/medium/country and synonyms; include multiple plausible variants joined by OR (e.g., '\"The AXIS\" OR \"Zappos Theater\" location hotel casino owner OR parent company').\n",
      "  - For comparisons, include all entities plus deciding property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublished first‚Äù, ‚Äúolder than‚Äù, ‚Äúbirth year‚Äù, ‚Äúnumber of players‚Äù, ‚Äúplayer count‚Äù, ‚Äúcomposer AND librettist‚Äù).\n",
      "  - For revival/relaunch questions, include 'revived OR relaunch OR revival' + year + title names.\n",
      "  - For actor-bridging to themed works, include the cast list entity and the theme keywords (e.g., 'alien OR extraterrestrial'), and, if known, likely crossover film titles with OR.\n",
      "  - For ownership/HQ follow-ups, include owner name + 'headquarters OR based in OR HQ city'.\n",
      "- Precision controls:\n",
      "  - Prefer property words (‚Äúcast‚Äù, ‚Äúwriter/screenwriter‚Äù, ‚Äúcomposer‚Äù, ‚Äúlibrettist‚Äù, ‚Äúfirst issue‚Äù, ‚Äúfounded‚Äù, ‚Äúowner‚Äù, ‚Äúparent company‚Äù, ‚Äúheadquarters‚Äù, ‚Äúplayer count‚Äù, ‚Äúplatforms‚Äù OR ‚Äúsystems‚Äù OR ‚Äúdevices‚Äù, ‚Äúplatforms list‚Äù, ‚Äúrelease year‚Äù, ‚Äúbirth‚Äù, ‚Äúhead coach OR manager‚Äù, ‚Äúcounty‚Äù, ‚Äúsuburb‚Äù).\n",
      "  - Include ‚Äúlist of‚Äù or filmography/discography/roster pages if that‚Äôs the likely authoritative source.\n",
      "  - Avoid adding off-topic brands or platforms unless central to the property.\n",
      "- Temporal scope:\n",
      "  - If the question is about franchise totals (‚Äúhow many World Series has the team won‚Äù), default to all-time totals unless a timeframe constraint is present; include ‚Äúall-time‚Äù related list pages when helpful.\n",
      "- Geographic/name-change handling: if Next-hop suggests alias or renamed entities (e.g., ‚ÄúThe AXIS‚Äù now ‚ÄúZappos Theater‚Äù), include both names with OR.\n",
      "- Exclusion-awareness: for questions with ‚Äúother than/besides/except/apart from,‚Äù you cannot enforce NOT in BM25; still include the pivot entity and property keywords so the exact entity page surfaces, and rely on summaries/final step to filter the excluded item.\n",
      "- If Next-hop focus is fully satisfied by {summary_1}, generate a narrow confirmation query to the most authoritative exact entity or property/list page.\n",
      "- Output exactly one plain-text query string only. No code or explanations.\n",
      "\n",
      "\n",
      "summarize_2_prompt = Read {passages_2} in light of {question} and {summary_1}.\n",
      "- Compile final, question-focused bullets with exact names/dates/titles/numbers. Prefix each bullet with the source page title in brackets and indicate authority (exact entity/list/overview/unrelated/property).\n",
      "- Resolve ambiguity by preferring the most specific, authoritative passage (exact entity page; official list/roster/cast/award page; primary property page). Note conflicts briefly and state which source is most direct/specific and why.\n",
      "- Ensure all constraints from Question constraints are satisfied (timeframe/season; correct medium/franchise; revived/relaunched year; jurisdiction/council/county; alias/rename handling; standard measurement vs extended variants). If the question uses ‚Äúother than/besides/except/apart from,‚Äù explicitly filter out the excluded item(s) when listing candidates.\n",
      "- For comparisons, explicitly compute which satisfies the comparison and state the deciding facts side-by-side. Add a ‚ÄúComputation‚Äù bullet with the decision.\n",
      "- For category/slot-fill questions (‚Äúare a breed of what?‚Äù, ‚Äúwhat kind of work‚Äù, ‚Äúeither X or what?‚Äù), return the minimal explicit class or missing alternative. Prefer the broader class unless a subtype is explicitly requested.\n",
      "- For location ‚Äúwhere‚Äù questions, return the place (city/region/country), not distances or directions.\n",
      "- Prefer explicit defining phrases over labels when the question asks ‚Äúbelieved in what/what kind of‚Äù (quote the source phrase if present).\n",
      "- If a first-hop descriptor in the question was unverified, but the decisive identity/property is supported, state the decisive fact and note: Descriptor unverified/not required for answer.\n",
      "- If a key entity/property remains absent, add:\n",
      "  - Remaining gap: <state missing entity/property>.\n",
      "- For totals versus time-bounded counts (e.g., championships), default to all-time totals unless the question limits the timeframe; cite list/record pages if used.\n",
      "- End with one line: Answer candidate(s): <verbatim minimal candidate value(s) complying with constraints>. If unresolved or one side of a comparison is absent, include ‚ÄúUnknown‚Äù.\n",
      "- Produce only the summary.\n",
      "\n",
      "\n",
      "final_answer_prompt = Given {question}, {summary_1}, {summary_2}, output the minimal, precise answer only.\n",
      "- Determine the answer type (yes/no, person, date, year, number, title, place, list, category/comparison/format, nationality vs country).\n",
      "  - If the question asks ‚Äúwhat nationality,‚Äù return the demonym (e.g., Czech). If it asks ‚Äúfrom which country,‚Äù return the country name (e.g., Czech Republic).\n",
      "  - If it asks ‚Äúa breed of what?‚Äù or ‚Äúwhat kind of,‚Äù return the broad class/category (e.g., Dogs; magazine; opera), not subtypes.\n",
      "  - If the question has the pattern ‚Äúeither X or what?‚Äù or ‚ÄúX and what other Y?‚Äù or contains ‚Äúother than/besides/except/apart from,‚Äù output only the alternative(s) to X and exclude any entity explicitly mentioned in the question.\n",
      "  - For franchise totals (e.g., titles won), return all-time totals unless a timeframe is explicitly specified.\n",
      "- Output an answer only if the value appears explicitly in the summaries or is a direct comparison/computation between explicit values and matches all constraints (timeframe, medium/franchise, location/council/county, standard measurement).\n",
      "- Prefer explicit phrases from sources over labels when the question asks for beliefs or definitions; if both exist, output the explicit defining phrase.\n",
      "- If summaries flag Missing evidence/Remaining gap, if one side of a required comparison is absent, or if the value is not explicitly supported, output \"Unknown\".\n",
      "- When multiple candidates exist, choose the one best supported by the most authoritative, most specific source (per summarize_2). If equal candidates remain for a singular ‚Äúwhich/what‚Äù question, output \"Unknown\".\n",
      "- Preserve names/titles exactly as shown in the selected passage (include nicknames/aliases if present). Preserve numerals and punctuation as in source when possible.\n",
      "- For ‚Äúhow many‚Äù questions, output the numeral only. For comparisons (‚Äúwhich published first/older than/which supports more players/which had more contributors‚Äù), output the single winning entity/value only.\n",
      "- For sports teams ‚Äúfounded,‚Äù if both a precise date and a first season year are present, prefer the conventional year from the main team page unless the question requests the exact date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.61it/s\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:32<00:00 |  9.19it/s\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:27<00:00 | 10.72it/s\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:31<00:00 |  9.45it/s\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.60it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.48325742446144165, em: 0.37666666666666665, prec: 0.480700176865618, recall: 0.5227323232323234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 12:45<00:00 |  5.10s/it\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 11:06<00:00 |  4.45s/it\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:16<00:00 |  9.29it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:23<00:00 |  6.49it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:16<00:00 |  9.19it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:25<00:00 |  5.84it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:15<00:00 |  9.64it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5967907647907649, em: 0.5, prec: 0.5957293806411453, recall: 0.6321666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 149/150 (99.3%) | ‚è≥ 00:50<00:00 |  1.31it/s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APITimeoutError('Request timed out.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 00:58<00:00 |  7.55s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question', 'question_id', 'gold_answer', 'supporting_facts', 'query_1', 'passages_1', 'summary_1', 'query_2', 'passages_2', 'summary_2', 'final_answer', 'evals', 'queries']\n",
      "\n",
      "üîß Creating batches with 100,000 token limit\n",
      "üìä Processing 150 examples in 4 batches\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 1/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 2/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 3/4: Optimized\n",
      "\n",
      "You are an expert in multi-hop question-answering and prompt optimization. \n",
      "Your job is to improve a question-answering system that operates on the HotpotQA benchmark ‚Äî a dataset that requires multi-hop reasoning across multiple Wikipedia articles.\n",
      "\n",
      "====================\n",
      "üèóÔ∏è SYSTEM DESCRIPTION\n",
      "====================\n",
      "The system answers complex factual questions by chaining together multiple reasoning and retrieval steps. \n",
      "It operates as follows:\n",
      "\n",
      "1. **Input Question:** The system begins with a natural language question from the HotpotQA dataset.\n",
      "2. **Query Generation:** The system generates a first query ({query_1}) to retrieve relevant documents from a fixed corpus.\n",
      "3. **Retrieval (BM25-based Search):**\n",
      "   The search tool is a static BM25 retriever implemented as:\n",
      "\n",
      "stemmer = Stemmer.Stemmer(\"english\")\n",
      "retriever = bm25s.BM25.load(\"/Users/priyanjindal/prompt-learning/benchmarks/hotpotQA/wiki17_abstracts\", corpus_name=\"wiki17_abstracts_corpus.jsonl\", load_corpus=True)\n",
      "corpus = retriever.corpus\n",
      "\n",
      "def search(query: str, k: int) -> list[dict]:\n",
      "tokens = bm25s.tokenize(query, stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
      "results, scores = retriever.retrieve(tokens, k=k, n_threads=1, show_progress=False)\n",
      "formatted_results = []\n",
      "for doc in results[0]:\n",
      "text = doc['text']\n",
      "if \" | \" not in text:\n",
      "return []\n",
      "title, content = text.split(\" | \", 1)\n",
      "formatted_results.append({\"title\": title, \"content\": content})\n",
      "return formatted_results\n",
      "\n",
      "- This search function retrieves top-{k} Wikipedia abstracts.\n",
      "- Each document has a `title` (the Wikipedia page title) and `content` (the text of the abstract).\n",
      "- The retriever is *static* ‚Äî meaning only the **queries** can be improved, not the retrieval algorithm itself.\n",
      "\n",
      "4. **Summarization:** The retrieved passages are summarized to highlight key facts ({summary_1}, {summary_2}).\n",
      "5. **Second-Hop Query Generation:** The system generates a follow-up query ({query_2}) based on the question and first summary to gather additional evidence.\n",
      "6. **Final Answer Generation:** The model combines all retrieved evidence to produce a final answer ({final_answer}).\n",
      "\n",
      "====================\n",
      "üéØ OPTIMIZATION GOAL\n",
      "====================\n",
      "Your task is to optimize the *prompts* used in each reasoning component (query generation, summarization, and answer synthesis) so that the system retrieves the correct evidence and produces accurate final answers. \n",
      "\n",
      "In particular:\n",
      "- The search component cannot change ‚Äî only the *language and structure of the queries* affect retrieval quality.\n",
      "- Good prompts guide the model to generate **precise, entity-rich, multi-hop-aware queries** that retrieve all *supporting facts*.\n",
      "- Summaries should **preserve factual links** across hops, not just paraphrase content.\n",
      "- The final answer prompt should encourage **faithful synthesis** of retrieved information.\n",
      "\n",
      "====================\n",
      "üìÑ YOUR INPUTS\n",
      "====================\n",
      "Below is the baseline prompt currently being used, along with example runs and feedback. \n",
      "Use these to identify weaknesses and propose improvements.\n",
      "\n",
      "************* start prompts *************\n",
      "{baseline_prompt}\n",
      "************* end prompts *************\n",
      "\n",
      "************* start example data *************\n",
      "{examples}\n",
      "************* end example data *************\n",
      "\n",
      "HERE ARE SOME ANNOTATIONS THAT MAY BE HELPFUL:\n",
      "{annotations}\n",
      "\n",
      "====================\n",
      "üîß FINAL INSTRUCTIONS\n",
      "====================\n",
      "Iterate on the baseline prompt to produce a **new, improved prompts** that:\n",
      "- Retains all variable placeholders (e.g., {question}, {query_1}, {summary_1}, etc.).\n",
      "- Produces clearer, more factually grounded reasoning and retrieval.\n",
      "- Encourages entity completeness (e.g., names, dates, titles, relations) and multi-hop connections.\n",
      "- Remains faithful to the output schema and return format from the original prompt.\n",
      "- Includes short, high-quality few-shot examples or guidelines if relevant.\n",
      "- Return the prompts in the same formatting that they were given.\n",
      "Note: Make sure to include the variables from the original prompt, which are wrapped in either single brackets or double brackets (e.g.\n",
      "{var}). If you fail to include these variables, the LLM will not be able to access the required data.\n",
      "\n",
      "NEW PROMPTS:\n",
      "\n",
      "   ‚úÖ Batch 4/4: Optimized\n",
      "NEW PROMPTS:\n",
      "\n",
      "        create_query_1_prompt = You are generating a single high-recall, keyword-style search query for a BM25 retriever over Wikipedia abstracts. Use {question}.\n",
      "- Objective: write one entity-complete query that surfaces (1) all core entities with exact Wikipedia page strings and plausible aliases/variants (add medium/type and year disambiguators), (2) the likely pivot pages needed to bridge hops (author/screenwriter/composer/librettist/owner/parent company/founding/HQ, film‚Üícast/character, team‚Üíleague/season/titles, station‚Üíowner), and (3) decisive property keywords required to answer the question (‚Äúbirth‚Äù, ‚Äúborn‚Äù, ‚Äúbirthplace‚Äù, ‚Äúnationality‚Äù, ‚Äúfirst issue year‚Äù, ‚Äúfounded/founded by‚Äù, ‚Äúheadquarters/HQ‚Äù, ‚Äúowner/parent company‚Äù, ‚Äúcast/actor/role‚Äù, ‚ÄúWorld Series championships‚Äù, ‚Äúdebut year‚Äù, ‚Äúplatforms/systems/devices‚Äù).\n",
      "- Before composing the query, internally determine: answer type (yes/no, person, date/year, number, title, place, list, comparison), key entities (with medium/year), relations (author/screenwriter/composer/librettist/owner/parent company/founded/founded by/HQ city/coach OR head coach OR manager/first issue/publication year/birth year/nationality/revival/character), and constraints (timeframe/season/year; jurisdiction/city/county/state/country; renames/aliases; exclusions like ‚Äúother than/besides/except‚Äù).\n",
      "- Disambiguation and aliases:\n",
      "  - Add medium/year where canonical: '\"Tombstone\" (1993 film)', '\"OK! (magazine)\"', '\"71\" (2014 film)', '\"The Brave and the Bold\" (2007 comic)'.\n",
      "  - Include creator/league/club when helpful: 'Panathinaikos F.C. head coach OR manager 2008 OR 2009'.\n",
      "  - Include renamed entities with OR: '\"The AXIS\" OR \"Zappos Theater\"'.\n",
      "  - Include punctuation/diacritic variants with OR: '\"OK! (magazine)\" OR \"OK magazine\"', '\"Who\\'s\" OR \"Whos\"', '\"Koi... Mil Gaya\" OR \"Koi Mil Gaya\"'.\n",
      "  - For ambiguous personal names, include role qualifiers with OR: '\"Randy Jackson\" musician OR producer OR \"American Idol\" OR bassist', and when known use parenthetical disambiguators (e.g., '\"Randy Jackson (musician)\"').\n",
      "- Multi-hop recall strategy (compose one rich query):\n",
      "  - For title‚Üíauthor/artist: include the work and author/artist pivot keywords: '\"A Death in Vienna\" novel author OR writer Daniel Silva'.\n",
      "  - Bridge roles explicitly: if the question references a role (‚Äúsinger of X‚Äù, ‚Äúactor from Y born on [date]‚Äù), include the work + role + property words: '\"Tombstone\" (1993 film) cast actor born \"May 17, 1955\"'.\n",
      "  - For team/person‚Üítitles: include person + played for OR team + championships keywords: '\"Frank Bushey\" baseball player played for team roster OR biography World Series championships OR titles'.\n",
      "  - For comparisons/ordering (‚Äúwhich published first/older than/more players/formed same year‚Äù), include all entities plus deciding property keywords and country qualifiers if disambiguating.\n",
      "  - For ownership/HQ/private‚Äìpublic: include the organization + 'owner OR parent company OR headquarters OR HQ city OR private OR public'.\n",
      "  - For nationality/same-country yes/no, include both people + 'born OR birthplace OR birth place OR nationality OR citizen' and common demonyms (e.g., American, British).\n",
      "  - Numbers-as-titles: include year/medium to resolve bare numerals: '\"71\" (2014 film) cast'.\n",
      "- Precision controls for BM25:\n",
      "  - Put exact entity page strings in quotes; add property words: ‚Äúcast‚Äù, ‚Äúactor‚Äù, ‚Äúwriter/screenwriter‚Äù, ‚Äúcomposer‚Äù, ‚Äúlibrettist‚Äù, ‚Äúowner‚Äù, ‚Äúparent company‚Äù, ‚Äúfounded‚Äù, ‚Äúheadquarters/HQ‚Äù, ‚Äúfirst issue‚Äù, ‚Äúbirth‚Äù, ‚Äúborn‚Äù, ‚Äúbirthplace‚Äù, ‚Äúnationality‚Äù, ‚Äúhead coach OR manager‚Äù, ‚Äúformation year‚Äù, ‚Äúplayer count‚Äù, ‚Äúplatforms OR systems OR devices‚Äù, ‚Äúdebut year‚Äù.\n",
      "  - Add authoritative list/property pages when helpful: 'cast list', 'discography', 'filmography', 'list of [team] players', 'list of [team] seasons', 'World Series championships'.\n",
      "  - Do not use site: filters or NOT/minus operators. Never include AND; use spaces and quoted phrases; use OR only as a literal token to include variants.\n",
      "- Geographic/name disambiguation: If a place/team/council is ambiguous, add qualifiers (city/state/county/country) and nearby anchors.\n",
      "- Temporal scope: For totals (‚Äúhow many has team X won‚Äù), default to all-time unless a timeframe is specified; include ‚Äúall-time‚Äù when helpful.\n",
      "- Output exactly one plain-text query string only (quotes around exact titles, no trailing punctuation beyond quotes). No code or explanations.\n",
      "\n",
      "Guidelines (examples, do not output these):\n",
      "- Are Belinda Carlisle and Randy Jackson from the same country? -> '\"Belinda Carlisle\" born OR birthplace OR nationality American vs \"Randy Jackson\" musician OR producer born OR birthplace OR nationality'\n",
      "- Which was published first, The Fader or OK!? -> '\"The Fader\" magazine \"first issue\" OR founding publication year US vs \"OK! (magazine)\" OR \"OK magazine\" \"first issue\" OR founding publication year UK'\n",
      "- Which actor from Tombstone was born on May 17, 1955? -> '\"Tombstone\" (1993 film) cast actor born \"May 17, 1955\"'\n",
      "- ‚Äô71 stars an actor who played what character in Peaky Blinders? -> '\"71\" (2014 film) cast \"Peaky Blinders\" (TV series) actor role character name'\n",
      "- How many World Series has the team Frank Bushey played for won? -> '\"Frank Bushey\" baseball played for team roster OR biography World Series championships OR titles'\n",
      "- Peak FM is owned and operated by a broadcasting company based in what city? -> '\"Peak FM\" radio station owner OR parent company \"Wireless Group\" headquarters OR HQ city Belfast'\n",
      "- The singer of \"Work Hard, Play Harder\" made her debut when? -> '\"Work Hard, Play Harder\" song singer OR artist Gretchen Wilson debut year OR career began'\n",
      "\n",
      "\n",
      "        summarize_1_prompt = Read {passages_1} and extract only facts that help answer {question}.\n",
      "- Produce concise bullets as subject‚Äìverb‚Äìobject triples. Prefix each bullet with the source page title in brackets. Include medium/type and years where relevant.\n",
      "- Preserve exact titles, names, dates, numbers, punctuation; include aliases if present (original names, nicknames, former names/renames).\n",
      "- Identify and record the entity type for each named item (film, TV series, album, song, comic, magazine, opera, video game, person, team/club, council, company, venue, university type private/public).\n",
      "- Note temporal scope explicitly: all-time vs by season/year vs point-in-time; include units (mi/km, years, counts).\n",
      "- Mark authority after each bullet: (exact entity page / official list/award/roster/cast/property page / overview page / unrelated).\n",
      "- Explicitly record:\n",
      "  - Pivots discovered (e.g., author/artist/singer; person/owner/parent company; composer/librettist; cast member; team played for; the second entity in a comparison).\n",
      "  - Negative findings (e.g., ‚Äúno HQ city stated‚Äù, ‚Äúno singer stated for the song‚Äù, ‚Äúno team identified for the player‚Äù).\n",
      "- Entity consistency check for ambiguous titles:\n",
      "  - If a title is ambiguous (e.g., ‚ÄúA Death in Vienna‚Äù), add a bullet clarifying the correct work+author before using properties.\n",
      "- For comparison/ordering questions, list the deciding property for each entity side-by-side with values and dates.\n",
      "- For cloze/relational prompts (‚Äú‚Ä¶ southwest of ?‚Äù), extract both the subject‚Äôs location and the referenced anchor place and label which is the answer target.\n",
      "- If a key pivot or property is missing (e.g., singer/author/owner/parent company/actor/HQ city/coach/private vs public/birthplace/nationality/team played for/World Series titles), add:\n",
      "  - Missing evidence: <state the missing entity/page/property we still need>.\n",
      "- If the question implies constraints (timeframe/season, jurisdiction/city/county/state/country, revived/relaunched year, alias/rename, numeric-title disambiguation, exclusions like ‚Äúother than/besides/except‚Äù), add:\n",
      "  - Question constraints: <state constraints verbatim and list any explicit exclusions to be filtered later>.\n",
      "- Add:\n",
      "  - Answer type: <person/date/year/number/title/place/list/category/yes-no/comparison>.\n",
      "  - Entity map: <link roles across entities (work -> author/singer; film -> cast; person -> team; team -> championships; organization -> owner -> HQ city; person -> nationality/birthplace etc.)>.\n",
      "- End with two lines:\n",
      "  - Known entities: <exact titles/names identified so far (include medium/type) and authoritative variants>\n",
      "  - Next-hop focus: <precise gap to look up next (e.g., \"nationality or birthplace for Belinda Carlisle and Randy Jackson\", \"identify team for Frank Bushey then team World Series championships\", \"publication years for The Fader and OK! (magazine)\", \"identify actor from '71' and their Peaky Blinders character\", \"Wireless Group headquarters city\")>\n",
      "- Do not speculate or add unrelated background; mark any non-pertinent passage as (unrelated). If no relevant facts exist in the passages, state Missing evidence accordingly and make Next-hop focus target the exact entity pages needed.\n",
      "\n",
      "\n",
      "        create_query_2_prompt = Use {question} and {summary_1} to generate a single improved search query.\n",
      "- Target exactly the gap in Next-hop focus using exact strings from Known entities; include disambiguators (medium/type qualifiers in parentheses), roles (author/screenwriter/cast/composer/librettist/coach/head coach/manager/owner), organizations (parent company/headquarters), and jurisdictions/years from Question constraints.\n",
      "- Include normalized OR-variants for ambiguous, misspelled, punctuated, or renamed names directly in the same query (e.g., '\"OK! (magazine)\" OR \"OK magazine\"', '\"The AXIS\" OR \"Zappos Theater\"').\n",
      "- Calibrate breadth and fix homonyms:\n",
      "  - If Missing evidence indicates a pivot entity is absent (author/singer/owner/parent company/actor/artist/composer/librettist/screenwriter/team/coach OR manager/HQ/private OR public/birthplace/nationality), query directly for that entity + the property (e.g., '\"Belinda Carlisle\" born OR birthplace OR nationality'; '\"Frank Bushey\" baseball played for team roster OR biography').\n",
      "  - If the first hop over-constrained or picked the wrong homonym, broaden/correct with year/medium/country and synonyms; include multiple plausible variants joined by OR (e.g., '\"Randy Jackson (musician)\" OR \"Randy Jackson\" musician OR producer').\n",
      "  - For comparisons, include all entities plus deciding property keywords (‚Äúfirst issue year‚Äù, ‚Äúpublished first‚Äù, ‚Äúolder than‚Äù, ‚Äúbirth year‚Äù, ‚Äúformation year‚Äù, ‚Äúplayer count‚Äù, ‚ÄúWorld Series championships‚Äù).\n",
      "  - For revival/relaunch questions, include 'revived OR relaunch OR revival' + year + title names and notable character names.\n",
      "  - For ownership/HQ/private‚Äìpublic, include the entity name + 'owner OR parent company OR headquarters OR HQ city'.\n",
      "- Precision controls:\n",
      "  - Prefer property words: ‚Äúcast‚Äù, ‚Äúactor‚Äù, ‚Äúwriter/screenwriter‚Äù, ‚Äúcomposer‚Äù, ‚Äúlibrettist‚Äù, ‚Äúfirst issue‚Äù, ‚Äúfounded‚Äù, ‚Äúowner‚Äù, ‚Äúparent company‚Äù, ‚Äúheadquarters/HQ‚Äù, ‚Äúformation year‚Äù, ‚Äúplayer count‚Äù, ‚Äúplatforms OR systems OR devices‚Äù, ‚Äúrelease year‚Äù, ‚Äúbirth‚Äù, ‚Äúborn‚Äù, ‚Äúbirthplace‚Äù, ‚Äúnationality‚Äù, ‚Äúhead coach OR manager‚Äù, ‚ÄúWorld Series championships OR titles‚Äù.\n",
      "  - Include ‚Äúlist of‚Äù / filmography / discography / roster pages when authoritative; include ‚Äúcast list‚Äù when targeting film actors; include ‚Äúlist of [team] seasons‚Äù or ‚ÄúWorld Series championships‚Äù for MLB titles.\n",
      "  - Do not use site: filters or NOT/minus operators. Never include AND; use spaces and quoted phrases; use OR only for variants.\n",
      "- Temporal scope:\n",
      "  - For totals or histories (e.g., titles won), default to all-time unless limited; include ‚Äúall-time‚Äù when helpful.\n",
      "- Geographic/name-change handling: if Next-hop suggests alias or renamed entities, include both names with OR.\n",
      "- Exclusion-awareness: for ‚Äúother than/besides/except/apart from,‚Äù still include the pivot entity and property keywords; filtering happens in later steps.\n",
      "- If Next-hop focus is fully satisfied by {summary_1}, generate a narrow confirmation query to the most authoritative exact entity or property/list page (e.g., 'cast list', 'infobox', 'publication history').\n",
      "- Output exactly one plain-text query string only. No code or explanations.\n",
      "\n",
      "Guidelines (examples, do not output these):\n",
      "- ‚ÄúA Death in Vienna‚Äù novel author and count novels -> '\"A Death in Vienna\" novel author Daniel Silva bibliography list of novels total'\n",
      "- Tombstone actor born May 17, 1955 -> '\"Tombstone\" (1993 film) cast list actor born \"May 17, 1955\" Bill Paxton birth date'\n",
      "- The Fader vs OK! first issue -> '\"The Fader\" magazine \"first issue\" OR founding year vs \"OK! (magazine)\" OR \"OK magazine\" \"first issue\" OR founding year'\n",
      "- Singer of \"Work Hard, Play Harder\" debut -> '\"Work Hard, Play Harder\" song singer OR artist Gretchen Wilson debut year OR career began'\n",
      "- Same-country check for Belinda Carlisle and Randy Jackson -> '\"Belinda Carlisle\" born OR birthplace OR nationality vs \"Randy Jackson\" musician OR producer born OR birthplace OR nationality'\n",
      "- Frank Bushey team then titles -> '\"Frank Bushey\" baseball played for team roster OR biography World Series championships OR titles \"Boston Red Sox\" OR \"Chicago Cubs\" (if discovered)'\n",
      "\n",
      "\n",
      "        summarize_2_prompt = Read {passages_2} in light of {question} and {summary_1}.\n",
      "- Compile final, question-focused bullets with exact names/dates/titles/numbers. Prefix each bullet with the source page title in brackets and indicate authority (exact entity/list/overview/unrelated/property).\n",
      "- Resolve ambiguity by preferring the most specific, authoritative passage (exact entity page; official list/roster/cast/award/championship page; primary property page). Note conflicts briefly and state which source is most direct and why.\n",
      "- Ensure all constraints from Question constraints are satisfied (timeframe/season; medium/franchise; revived/relaunched year; jurisdiction/council/county; alias/rename handling; numeric-title entity year/medium).\n",
      "- For comparisons, list the deciding facts side-by-side and add a ‚ÄúComputation‚Äù bullet stating the decision; for yes/no ‚Äúsame country‚Äù comparisons, normalize demonyms to countries and compute.\n",
      "- Prefer the specific named sub-event/entity over a broader umbrella event when both appear (e.g., choose ‚ÄúNorthern Rock crisis‚Äù over ‚Äúlate-2000s financial crisis‚Äù if the scoop was specifically Northern Rock).\n",
      "- For category/slot-fill questions, return the minimal explicit class or missing alternative. Prefer broader class unless a subtype is explicitly requested.\n",
      "- For ‚Äúwhere‚Äù questions, return the place (city/region/country) only; for cloze locations, return the referenced anchor place.\n",
      "- If a descriptor in the question was unverified but the decisive identity/property is supported, state the decisive fact and note: Descriptor unverified/not required for answer.\n",
      "- If a key entity/property remains absent, add:\n",
      "  - Remaining gap: <state missing entity/property>.\n",
      "- For totals vs time-bounded counts, default to all-time unless limited; cite list/record pages if used.\n",
      "- End with one line: Answer candidate(s): <verbatim minimal candidate value(s) complying with constraints or ‚ÄúUnknown‚Äù if unresolved>.\n",
      "- Produce only the summary.\n",
      "\n",
      "\n",
      "        final_answer_prompt = Given {question}, {summary_1}, {summary_2}, output the minimal, precise answer only.\n",
      "- Determine the answer type (yes/no, person, date, year, number, title, place, list, category/comparison, nationality vs country, address-only).\n",
      "  - If ‚Äúwhat nationality,‚Äù return the demonym (e.g., Czech). If ‚Äúfrom which country,‚Äù return the country name (e.g., Czech Republic).\n",
      "  - If ‚Äúa breed of what?‚Äù or ‚Äúwhat kind of,‚Äù return the broad class/category (e.g., Dogs; magazine; opera), not subtypes.\n",
      "  - If the question pattern is ‚Äúeither X or what?‚Äù or ‚ÄúX and what other Y?‚Äù or contains ‚Äúother than/besides/except/apart from,‚Äù output only the alternative(s) to X and exclude any entity explicitly mentioned.\n",
      "  - For ‚Äúwhere‚Äù questions, output the place only (city/region/country). For cloze location prompts (‚Äú‚Ä¶ about 80 km southwest of ?‚Äù), output the referenced anchor place.\n",
      "  - For people with nicknames: if the exact entity page presents a nickname tightly bound to the name (quotes in lead) and it appears in summaries, you may include it; otherwise use the full name.\n",
      "  - For franchise totals (e.g., titles won), return all-time totals unless a timeframe is specified.\n",
      "  - When a specific named sub-event/entity and a broader umbrella event both appear, choose the most specific one that directly satisfies the question (e.g., ‚ÄúNorthern Rock crisis‚Äù for ‚Äúwhich financial crisis‚Äù).\n",
      "- Output an answer only if it appears explicitly in the summaries or is a direct comparison/computation from explicit values that match all constraints. Do not rely on external knowledge.\n",
      "- If summaries flag Missing evidence/Remaining gap, if one side of a required comparison is absent, or if the value is not explicitly supported, output \"Unknown\".\n",
      "- When multiple candidates exist, choose the one best supported by the most authoritative, most specific source (per summarize_2). If equal candidates remain for a singular ‚Äúwhich/what‚Äù question, output \"Unknown\".\n",
      "- Preserve names/titles exactly as shown in the selected passage (including punctuation/diacritics). Preserve numerals and units when possible.\n",
      "- For ‚Äúhow many,‚Äù output the numeral only. For comparisons (‚Äúwhich published first/older than/which supports more players/which had more contributors‚Äù), output the single winning entity/value only.\n",
      "- For sports teams ‚Äúfounded,‚Äù if both a franchise-award date and an establishment/first season year appear, prefer the conventional year used on the main team page/infobox when the question just says ‚Äúfounded‚Äù; otherwise honor the explicitly requested form.\n",
      "- For yes/no questions, output only \"Yes\" or \"No\".\n",
      "- For list answers, return a minimal comma-separated list without extra words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 (100.0%) | ‚è≥ 08:24<00:00 |  3.37s/it\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:27<00:00 | 10.96it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:36<00:00 |  8.17it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:28<00:00 | 10.56it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 (100.0%) | ‚è≥ 00:31<00:00 |  9.44it/s\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 213/300 (71.0%) | ‚è≥ 00:19<00:07 | 11.33it/s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in worker on attempt 1: raised APIConnectionError('Connection error.')\n",
      "Requeuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 299/300 (99.7%) | ‚è≥ 00:26<00:00 | 13.03it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5242068887586792, em: 0.41, prec: 0.5213418354521295, recall: 0.5724848484848486\n"
     ]
    }
   ],
   "source": [
    "prompts = {\n",
    "    \"create_query_1_prompt\": \"Given the fields {question}, produce a query.\",\n",
    "    \"summarize_1_prompt\": \"Given the fields {question}, {passages_1}, produce a summary.\",\n",
    "    \"create_query_2_prompt\": \"Given the fields {question}, {summary_1}, produce a query.\",\n",
    "    \"summarize_2_prompt\": \"Given the fields {question}, {summary_1}, {passages_2}, produce a summary.\",\n",
    "    \"final_answer_prompt\": \"\"\"Given the fields {question}, {summary_1}, {summary_2}, produce a concise and precise answer that directly and minimally responds to the question.  \n",
    "    - Provide the shortest possible answer that fully addresses the question.  \n",
    "    - If the question expects a specific name, date, number, phrase, or list, output only that without any additional explanation, context, or full sentences.  \n",
    "    - For yes/no questions, answer with \\\"yes\\\" or \\\"no\\\" only, without elaboration.  \n",
    "    - When a comparative judgment is required but explicit counts are missing, prefer the best-supported conclusion from the summaries (e.g., one entity has 'numerous named subsidiaries' vs a single example), and output only the winning entity.  \n",
    "    - Avoid adding extra context, background information, or verbose sentences.  \n",
    "    ...\n",
    "    - Keep answers minimal (e.g., for 'Where is X located about 80 km southwest of ?', answer 'Paris' only).  \n",
    "\n",
    "    Answer:\"\"\"\n",
    "}\n",
    "dev_path = hotpot_root / \"data/hotpot_dev_sample_300.json\"\n",
    "dev_dataset = pd.read_json(dev_path).sample(300, random_state=42)\n",
    "loops = {0: {\"prompts\": prompts, \"train_results\": None, \"train_metrics\": None, \"dev_metrics\": None}}\n",
    "for i in range(5):\n",
    "    train_results, train_metrics = run_train(prompts)\n",
    "    optimized_prompt_string = evaluate_optimize(train_results, prompts)\n",
    "    optimized_prompts = parse_prompts(optimized_prompt_string)\n",
    "    dev_metrics = run_dev(optimized_prompts, dev_dataset, dev_path)\n",
    "    loops[i+1] = {\"prompts\": optimized_prompts, \"train_results\": train_results, \"train_metrics\": train_metrics, \"dev_metrics\": dev_metrics}\n",
    "\n",
    "    prompts = optimized_prompts\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hover-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
