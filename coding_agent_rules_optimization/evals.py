import pandas as pd
import json
from typing import Dict
from phoenix.evals.llm import LLM
from phoenix.evals.utils import to_annotation_dataframe
from phoenix.evals import ClassificationEvaluator, async_evaluate_dataframe


async def evaluate_results(results: pd.DataFrame, model: str = "gpt-5") -> pd.DataFrame:

    print("Running Evals")
    # filter out excessively large patches
    results = results[results["coding_agent_patch"].str.len() <= 200000]

    prompt = """
    You are an expert software engineer, tasked with reviewing a coding agent's code changes. 

    You are given the following information:
    - problem_statement: the problem statement
    - coding agent patch: a patch generated by the coding agent, which is supposed to fix the problem.
    - ground truth patch: a ground truth solution/patch to the problem
    - test patch: a test patch that the coding agent's output should pass, which directly addresses the issue in the problem statement
    - pass_or_fail: either "pass" or "fail" indicating whether the coding agent's code changes passed the unit tests.

    Your task is to review the given information and determine why the coding agent's output is correct or incorrect. 
    pass_or_fail indicates whether the coding agent's output is correct or incorrect, based on whether the generated code passed the unit tests. 
    You must synthesize why the coding agent's output is correct or incorrect. Try to reason about the coding agent's approach, and why the coding agent may have taken that approach.
    If the coding agent is incorrect, reason about general improvement suggestions for the coding agent to improve its output.

    problem_statement: {problem_statement}
    ground truth patch: {ground_truth_patch}
    test patch: {test_patch}
    coding agent patch: {coding_agent_patch}
    pass_or_fail: {pass_or_fail}

    Return in the following JSON format:
    "correctness": "correct" if pass_or_fail is "pass", "incorrect" if pass_or_fail is "fail"
    "explanation": "explanation of your reasoning: why/why not the coding agent's output is correct, why the coding agent may have taken that approach, and general improvement suggestions for the coding agent to improve its output."
    """

    evaluator = ClassificationEvaluator(
        "correctness",
        LLM(provider="openai", model=model),
        prompt,
        choices={"correct": 1, "incorrect": 0},
    )
    results_df = await async_evaluate_dataframe(
        dataframe=results,
        evaluators=[evaluator],
        concurrency=20,
        tqdm_bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}",
    )
    results_df = results_df["correctness_score"]

    results_df = pd.DataFrame(
        results_df.apply(json.loads).tolist(), index=results_df.index
    )

    results["correctness"] = results_df["label"]
    results["explanation"] = results_df["explanation"]
    results["score"] = results_df["score"]

    return results
