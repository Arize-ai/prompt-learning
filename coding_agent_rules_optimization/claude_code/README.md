# Claude Code for SWE-bench

This directory contains tools for running Claude Code CLI on SWE-bench instances.

## Files

- **`claude_code_helpers.py`** - Core helper functions for running Claude Code on individual instances
- **`run_claude.py`** - Main script for running Claude Code on SWE-bench datasets in parallel
- **`test_claude_code.py`** - Test suite for validating the Claude Code integration

## Setup

1. **Install Claude Code CLI**
   ```bash
   # Follow instructions at: https://claude.ai/download
   ```

2. **Authenticate**
   ```bash
   claude login
   # OR set API key
   export ANTHROPIC_API_KEY="your-key"
   ```

3. **Install dependencies**
   ```bash
   conda activate cline  # or your environment with swebench
   pip install swebench pandas
   ```

## Usage

### Quick Test

**Single instance test:**
```bash
python test_claude_code.py --swebench
```

**Full integration test (5 instances in parallel):**
```bash
python test_run_claude.py
```
This will run Claude Code on 5 instances with 5 workers and validate the complete pipeline.

### Run on SWE-bench

**Single instance:**
```bash
python run_claude.py \
  --run_id test_run_1 \
  --instance_ids sympy__sympy-13177 \
  --wait_seconds 300
```

**Multiple instances:**
```bash
python run_claude.py \
  --run_id my_experiment \
  --instance_ids sympy__sympy-13177 django__django-11422 \
  --workers 2 \
  --wait_seconds 600
```

**Random subset:**
```bash
python run_claude.py \
  --run_id random_10 \
  --count 10 \
  --workers 4
```

**With ruleset:**
```bash
python run_claude.py \
  --run_id with_rules \
  --count 5 \
  --ruleset ../cline_act_mode/act_rulesets/ruleset_0.txt \
  --workers 2
```

**Custom output:**
```bash
python run_claude.py \
  --run_id experiment_1 \
  --count 20 \
  --output results/experiment_1.csv \
  --workers 4 \
  --wait_seconds 600
```

## Arguments

- `--run_id` (required): Unique identifier for this run
- `--dataset`: Dataset name (default: "SWE-bench/SWE-bench_Lite")
- `--instance_ids`: Specific instance IDs to run (space-separated)
- `--count`: Number of random instances to run
- `--workers`: Number of parallel workers (default: 4)
- `--wait_seconds`: Timeout per instance in seconds (default: 300)
- `--ruleset`: Path to ruleset text file to append to system prompt
- `--output`: Output CSV file path (default: `claude_results_{run_id}.csv`)

## Output

The script produces:

1. **CSV file** with columns:
   - `instance_id`: SWE-bench instance ID
   - `problem_statement`: The issue description
   - `patch`: Ground truth patch
   - `test_patch`: Test patch for evaluation
   - `claude_patch`: Patch generated by Claude Code
   - `pass_or_fail`: Whether Claude's patch passed tests

2. **Evaluation logs** in `logs/run_evaluation/{run_id}/claude-code/`

3. **Combined predictions** in `/tmp/claude_preds_{run_id}.jsonl`

## Comparison with Cline

| Feature | Cline (`run_act.py`) | Claude Code (`run_claude.py`) |
|---------|---------------------|-------------------------------|
| Requires server | Yes (gRPC) | No (CLI) |
| Port management | Yes | No |
| Default workers | 16 | 4 |
| Setup complexity | High | Low |
| Namespace | "none" → "cline" | "none" → "claude-code" |
| Patch column | `cline_patch` | `claude_patch` |

## Tips

- **Workers**: Start with 2-4 workers to avoid rate limits
- **Timeout**: 300s (5 min) is usually enough; increase for complex issues
- **Rulesets**: Use `--append-system-prompt` to inject custom instructions
- **Monitoring**: Watch `/tmp/claude_code_test/` for workspace artifacts

## Example Session

```bash
# Test with one instance
python run_claude.py --run_id test_1 --instance_ids sympy__sympy-13177

# Review results
cat claude_results_test_1.csv

# Run larger experiment
python run_claude.py \
  --run_id exp_baseline \
  --count 50 \
  --workers 4 \
  --wait_seconds 600 \
  --output results/baseline.csv
```

