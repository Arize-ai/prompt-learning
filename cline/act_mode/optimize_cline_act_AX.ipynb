{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6825ae",
   "metadata": {},
   "source": [
    "# Cline Prompt Learning Optimization on SWE-bench - Act Mode\n",
    "\n",
    "<p align=\"left\">\n",
    "  <img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/arize-logo.png\" width=\"600\" />\n",
    "  <img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/cline.png\" width=\"200\" />\n",
    "</p>\n",
    "\n",
    "This notebook demonstrates how we used Prompt Learning to optimize Cline's performance on the SWE-bench dataset in **Act Mode**. Cline is a popular and powerful open-source coding agent. We look to improve its performance on SWE-bench by optimizing its **rules**, which are user specified instructions that Cline appends to its system prompt. \n",
    "\n",
    "[More on Cline](https://www.google.com/search?q=cline&sca_esv=764700c983d0c1df&sxsrf=AE3TifOTqxMNetNu45T7bn53deGE6bPn3w%3A1759280858717&ei=2n7caKLCK6XK0PEPpuCv6AY&ved=0ahUKEwiil5P154GQAxUlJTQIHSbwC20Q4dUDCBA&uact=5&oq=cline&gs_lp=Egxnd3Mtd2l6LXNlcnAiBWNsaW5lMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMhMQLhiABBixAxjRAxhDGMcBGIoFMhMQLhiABBixAxjRAxgUGIcCGMcBMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgUQABiABDIFEAAYgAQyChAAGIAEGEMYigVIkw1Q8QNYpgxwA3gBkAEAmAG2AaABkAKqAQMxLjG4AQPIAQD4AQGYAgWgApwCwgIKEAAYsAMY1gQYR8ICDRAAGIAEGLADGEMYigXCAg4QABiwAxjkAhjWBNgBAcICDhAuGLADGLgGGMgD2AEBwgIQEC4YgAQY0QMYQxjHARiKBZgDAIgGAZAGE7oGBggBEAEYCZIHAzQuMaAHwhiyBwMxLjG4B5QCwgcDMC41yAcJ&sclient=gws-wiz-serp#:~:text=Cline%20%2D%20AI%20Coding,https%3A//cline.bot)\n",
    "\n",
    "[More on Prompt Learning](https://arize.com/blog/prompt-learning-using-english-feedback-to-optimize-llm-systems/)\n",
    "\n",
    "## Act Mode - Real Code Execution\n",
    "\n",
    "Unlike Plan Mode, this notebook runs Cline in **Act Mode**, where Cline actually edits the codebase and generates patches. We then run the SWE-bench tests to compute a definitive accuracy of whether Cline made the correct edits. This provides ground truth evaluation of Cline's performance.\n",
    "\n",
    "In Act Mode, Cline:\n",
    "1. Analyzes the problem statement\n",
    "2. Explores the codebase\n",
    "3. Makes actual code edits\n",
    "4. Generates patches\n",
    "5. Has its patches validated against the SWE-bench test suite\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Please visit README.md and complete all the Setup before running this notebook!**\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Running this notebook is computationally intensive and expensive as it involves:\n",
    "- Multiple API calls to Claude for each SWE-bench instance\n",
    "- Actually cloning repositories and running tests in isolated environments\n",
    "- Running SWE-bench harness to validate patches\n",
    "\n",
    "Consider adjusting the training and test set sizes based on your requirements, budget constraints, and computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_act import run_act\n",
    "from swebench.harness.utils import load_swebench_dataset\n",
    "import random\n",
    "from evals_act import evaluate_results\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "notebook_dir = Path().absolute()\n",
    "sys.path.insert(0, str(notebook_dir.parent.parent))\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "sys.path.insert(0, str(notebook_dir.parent))\n",
    "from constants import CLINE_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962e449",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "\n",
    "Set up your API keys for OpenAI, Anthropic, and Arize. If not already in your environment, you'll be prompted to enter them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\") or getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"ARIZE_API_KEY\"] = os.getenv(\"ARIZE_API_KEY\") or getpass.getpass(\"Enter your Arize API key: \")\n",
    "SPACE_ID = os.getenv(\"SPACE_ID\") or getpass.getpass(\"Enter your Arize space ID: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d137d39",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- LOOPS: number of Prompt Learning loops. How many times you want to optimize your prompt.\n",
    "- TRAIN_SIZE: size of training set.\n",
    "- TEST_SIZE: size of test set.\n",
    "- WORKERS: SWE-bench is set up to run in parallel, with however many workers you specify. Set this relative to your machine's capabilities and your Claude rate limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOPS = 5\n",
    "TRAIN_SIZE = 150\n",
    "TEST_SIZE = 150\n",
    "WORKERS = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f7ad9",
   "metadata": {},
   "source": [
    "## Cline Environment Configuration\n",
    "\n",
    "Set environment variables for Cline to run properly in Act Mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLINE_DISABLE_TERMINAL_REUSE\"] = \"1\"\n",
    "os.environ[\"CLINE_DEFAULT_TERMINAL_PROFILE\"] = \"bash\"\n",
    "os.environ[\"CLINE_SHELL_INTEGRATION_TIMEOUT_SEC\"] = \"10\"\n",
    "os.environ[\"CLINE_STANDALONE_CAPTURE_STDIO\"] = \"1\"\n",
    "os.environ[\"CLINE_SKIP_RESUME_CONFIRMATION\"] = \"1\"\n",
    "os.environ[\"CLINE_AUTO_FOLLOWUP\"] = \"1\"\n",
    "os.environ[\"CLINE_ENVIRONMENT\"] = \"local\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe74fd",
   "metadata": {},
   "source": [
    "## Train/Test Datasets\n",
    "\n",
    "This code splits SWE-bench Lite into train/test splits.\n",
    "\n",
    "The train set will be used to optimize the ruleset, while the test set will be used to measure the success of optimized rulesets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28654d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SWE-bench/SWE-bench_Lite\"\n",
    "dataset = load_swebench_dataset(dataset_name, \"test\")\n",
    "ids = [inst[\"instance_id\"] for inst in dataset]\n",
    "random.shuffle(ids)\n",
    "train_ids = ids[:TRAIN_SIZE]        \n",
    "test_ids = ids[len(ids) - TEST_SIZE:]\n",
    "train_dataset = dataset[:TRAIN_SIZE]\n",
    "test_dataset = dataset[len(dataset)-TEST_SIZE:]\n",
    "train_pd = pd.DataFrame.from_dict(train_dataset)\n",
    "test_pd = pd.DataFrame.from_dict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577508b2",
   "metadata": {},
   "source": [
    "## Upload Datasets to Arize\n",
    "\n",
    "Upload datasets to Arize for experiment tracking and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "arize_client = ArizeDatasetsClient(api_key=os.getenv(\"ARIZE_API_KEY\"))\n",
    "\n",
    "# Prepare expanded dataset with more columns\n",
    "def prepare_expanded_dataset(df):\n",
    "    expanded_df = pd.DataFrame({\n",
    "        'problem_statement': df['problem_statement'],\n",
    "        'patch': df['patch'],\n",
    "        'test_patch': df['test_patch'],\n",
    "        'instance_id': df['instance_id'],\n",
    "    })\n",
    "    return expanded_df\n",
    "\n",
    "# Prepare expanded datasets\n",
    "train_expanded = prepare_expanded_dataset(train_pd)\n",
    "\n",
    "# Upload expanded datasets to Arize\n",
    "train_arize_id = arize_client.create_dataset(\n",
    "    space_id=SPACE_ID,\n",
    "    dataset_name=\"Cline Act Mode: SWE-bench Train\",\n",
    "    dataset_type=GENERATIVE,\n",
    "    data=train_expanded\n",
    ")\n",
    "\n",
    "print(\"\\nCreated train dataset with ID:\", train_arize_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2df87",
   "metadata": {},
   "source": [
    "## Helper: Log Experiments to Arize\n",
    "\n",
    "This helper function logs experiment results to Arize, allowing us to visualize and track optimization progress across iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets.experiments.types import (\n",
    "    ExperimentTaskResultColumnNames,\n",
    "    EvaluationResultColumnNames,\n",
    ")\n",
    "\n",
    "task_columns = ExperimentTaskResultColumnNames(\n",
    "    example_id=\"example_id\", result=\"final_plan\"\n",
    ")\n",
    "evaluator_columns = EvaluationResultColumnNames(\n",
    "    label=\"correctness\",\n",
    "    explanation=\"explanation\",\n",
    "    score=\"score\"\n",
    ")\n",
    "\n",
    "# Get dataset with example_ids and merge with experiment results\n",
    "def log_experiment_with_ids(client, space_id, experiment_name, experiment_df, dataset_name):\n",
    "    # 1) fetch dataset and keep only instance_id -> id mapping\n",
    "    dataset = client.get_dataset(space_id=space_id, dataset_name=dataset_name)\n",
    "    id_map = dataset[['instance_id', 'id']].drop_duplicates()\n",
    "\n",
    "    # 2) merge and build a minimal payload with only required columns\n",
    "    merged = experiment_df.merge(id_map, on='instance_id', how='inner')\n",
    "    payload = merged.rename(columns={\"id\": \"example_id\"})[\n",
    "        [\"example_id\", \"cline_patch\", \"correctness\", \"explanation\", \"score\"]\n",
    "    ].copy()\n",
    "\n",
    "    return client.log_experiment(\n",
    "        space_id=space_id,\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_df=payload,\n",
    "        dataset_name=dataset_name,\n",
    "        task_columns=task_columns,\n",
    "        evaluator_columns={'correctness': evaluator_columns},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb7a69",
   "metadata": {},
   "source": [
    "## Ruleset Optimization Loop\n",
    "\n",
    "This is the main optimization loop. For each iteration:\n",
    "\n",
    "1. **Run Cline in Act Mode on training set** with the current ruleset, generating actual code patches\n",
    "2. **Run Cline in Act Mode on test set** with the current ruleset to measure generalization\n",
    "3. **Run SWE-bench tests** to validate patches and compute pass/fail metrics\n",
    "4. **Evaluate results** using LLM-as-judge to provide detailed feedback on patch quality\n",
    "5. **Optimize the ruleset** using Prompt Learning based on training results and feedback\n",
    "6. **Save results and rulesets** for tracking and analysis\n",
    "\n",
    "The optimization loop uses actual test execution results (pass/fail) as ground truth, combined with LLM evaluator feedback to iteratively improve the ruleset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37486130",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = \"\"\n",
    "\n",
    "for loop in range(LOOPS):\n",
    "    print(f\"Running for loop: {loop}\")\n",
    "\n",
    "    train_run_id = f\"claude_150_train_{loop}\"\n",
    "    test_run_id = f\"150_act_test_{loop}\"\n",
    "\n",
    "    train_df = run_act(dataset_name=dataset_name, instance_ids=train_ids, run_id=train_run_id, ruleset=ruleset, workers=WORKERS)\n",
    "    test_df = run_act(dataset_name=dataset_name, instance_ids=test_ids, run_id=test_run_id, ruleset=ruleset, workers=WORKERS)\n",
    "\n",
    "    test_df.to_csv(f\"act_results/test_results_{loop}.csv\", index=False)\n",
    "    \n",
    "    train_acc = sum(train_df[\"pass_or_fail\"] == \"pass\") / len(train_df)\n",
    "    test_acc = sum(test_df[\"pass_or_fail\"] == \"pass\") / len(test_df)\n",
    "    print(f\"Train Accuracy: {train_acc}\")\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "    # make sure any swebench package installations did not affect phoenix package\n",
    "    subprocess.run([\n",
    "        \"/opt/anaconda3/envs/cline/bin/python3\",\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"--upgrade\",\n",
    "        \"arize-phoenix\",\n",
    "        \"wrapt\",\n",
    "    ])\n",
    "    evaluated_train_results = evaluate_results(train_df)\n",
    "    evaluated_train_results.to_csv(f\"act_results/train_results_{loop}.csv\", index=False)\n",
    "    evaluated_train_results[\"score\"] = [1.0 if x == \"correct\" else 0.0 for x in evaluated_train_results[\"correctness\"]]\n",
    "    log_experiment_with_ids(arize_client, SPACE_ID, f\"Train {loop}\", evaluated_train_results, \"Cline Act Mode: SWE-bench Train\")\n",
    "\n",
    "    pl_optimizer = PromptLearningOptimizer(\n",
    "        prompt=CLINE_PROMPT,\n",
    "        model_choice=\"gpt-5\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    ruleset = pl_optimizer.optimize(\n",
    "        dataset=evaluated_train_results,\n",
    "        output_column=\"cline_patch\",\n",
    "        feedback_columns=[\"correctness\", \"explanation\"],\n",
    "        ruleset=ruleset,\n",
    "        context_size_k=400000\n",
    "    )\n",
    "    with open(f\"act_rulesets/ruleset_{loop}.txt\", \"w\") as f:\n",
    "        f.write(f\"train_accuracy: {train_acc} \\n\")\n",
    "        f.write(f\"test_accuracy: {test_acc} \\n\")\n",
    "        f.write(f\"optimized ruleset_{loop}: \\n {ruleset} \\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
