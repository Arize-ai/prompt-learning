{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6825ae",
   "metadata": {},
   "source": [
    "# Cline Prompt Learning Optimization on SWE-bench - Act Mode\n",
    "\n",
    "<p align=\"left\">\n",
    "  <span style=\"display: inline-block; width: 500px; height: auto; overflow: hidden; vertical-align: middle;\">\n",
    "    <img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix.jpeg\" style=\"margin: -10px -50px; width: 600px;\" />\n",
    "  </span>\n",
    "  <img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/cline.png\" width=\"200\" style=\"vertical-align: middle;\" />\n",
    "</p>\n",
    "\n",
    "This notebook demonstrates how we used Prompt Learning to optimize Cline's performance on the SWE-bench dataset in **Act Mode**. Cline is a popular and powerful open-source coding agent. We look to improve its performance on SWE-bench by optimizing its **rules**, which are user specified instructions that Cline appends to its system prompt. \n",
    "\n",
    "[More on Cline](https://www.google.com/search?q=cline&sca_esv=764700c983d0c1df&sxsrf=AE3TifOTqxMNetNu45T7bn53deGE6bPn3w%3A1759280858717&ei=2n7caKLCK6XK0PEPpuCv6AY&ved=0ahUKEwiil5P154GQAxUlJTQIHSbwC20Q4dUDCBA&uact=5&oq=cline&gs_lp=Egxnd3Mtd2l6LXNlcnAiBWNsaW5lMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMhMQLhiABBixAxjRAxhDGMcBGIoFMhMQLhiABBixAxjRAxgUGIcCGMcBMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgUQABiABDIFEAAYgAQyChAAGIAEGEMYigVIkw1Q8QNYpgxwA3gBkAEAmAG2AaABkAKqAQMxLjG4AQPIAQD4AQGYAgWgApwCwgIKEAAYsAMY1gQYR8ICDRAAGIAEGLADGEMYigXCAg4QABiwAxjkAhjWBNgBAcICDhAuGLADGLgGGMgD2AEBwgIQEC4YgAQY0QMYQxjHARiKBZgDAIgGAZAGE7oGBggBEAEYCZIHAzQuMaAHwhiyBwMxLjG4B5QCwgcDMC41yAcJ&sclient=gws-wiz-serp#:~:text=Cline%20%2D%20AI%20Coding,https%3A//cline.bot)\n",
    "\n",
    "[More on Prompt Learning](https://arize.com/blog/prompt-learning-using-english-feedback-to-optimize-llm-systems/)\n",
    "\n",
    "## Act Mode - Real Code Execution\n",
    "\n",
    "Unlike Plan Mode, this notebook runs Cline in **Act Mode**, where Cline actually edits the codebase and generates patches. We then run the SWE-bench tests to compute a definitive accuracy of whether Cline made the correct edits. This provides ground truth evaluation of Cline's performance.\n",
    "\n",
    "In Act Mode, Cline:\n",
    "1. Analyzes the problem statement\n",
    "2. Explores the codebase\n",
    "3. Makes actual code edits\n",
    "4. Generates patches\n",
    "5. Has its patches validated against the SWE-bench test suite\n",
    "\n",
    "## SWE Bench + Cline Setup\n",
    "\n",
    "**Please visit README.md and complete all the Setup before running this notebook!**\n",
    "\n",
    "## Phoenix\n",
    "\n",
    "We use Phoenix - an open source library for LLM development. We specifically leverage the experiments feature, so we can track Cline's improvements over time, as we optimize its ruleset.\n",
    "\n",
    "Visit phoenix.arize.com and sign-in/create account.\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Running this notebook is computationally intensive and expensive as it involves:\n",
    "- Multiple API calls to Claude for each SWE-bench instance\n",
    "- Actually cloning repositories and running tests in isolated environments\n",
    "- Running SWE-bench harness to validate patches\n",
    "\n",
    "Consider adjusting the training and test set sizes based on your requirements, budget constraints, and computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qq swebench arize-phoenix arize-phoenix-client pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_act import run_act\n",
    "from swebench.harness.utils import load_swebench_dataset\n",
    "import random\n",
    "from evals_act import evaluate_results\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "notebook_dir = Path().absolute()\n",
    "sys.path.insert(0, str(notebook_dir.parent.parent))\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "\n",
    "sys.path.insert(0, str(notebook_dir.parent))\n",
    "from constants import CLINE_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962e449",
   "metadata": {},
   "source": [
    "## API Keys\n",
    "\n",
    "Set up your API keys for OpenAI, Anthropic, and Arize. If not already in your environment, you'll be prompted to enter them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\") or getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"PHOENIX_API_KEY\"] = os.getenv(\"PHOENIX_API_KEY\") or getpass.getpass(\"Enter your Phoenix API key: \")\n",
    "HOSTNAME = os.getenv(\"PHOENIX_HOSTNAME\") or getpass.getpass(\"Enter your Phoenix hostname: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d137d39",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- LOOPS: number of Prompt Learning loops. How many times you want to optimize your rules.\n",
    "We will be starting with a blank, empty ruleset. So iteration #1 generates a set of rules from scratch, and all loops after that will look to optimize it. \n",
    "- TRAIN_SIZE: size of training set.\n",
    "- TEST_SIZE: size of test set.\n",
    "- WORKERS: SWE-bench with Cline is set up to run in parallel, with however many workers you specify. Set this relative to your machine's capabilities and your LLM rate limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOPS = 5\n",
    "TRAIN_SIZE = 150\n",
    "TEST_SIZE = 150\n",
    "WORKERS = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f7ad9",
   "metadata": {},
   "source": [
    "## Cline Environment Configuration\n",
    "\n",
    "Set environment variables for Cline to run properly in Act Mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLINE_DISABLE_TERMINAL_REUSE\"] = \"1\"\n",
    "os.environ[\"CLINE_DEFAULT_TERMINAL_PROFILE\"] = \"bash\"\n",
    "os.environ[\"CLINE_SHELL_INTEGRATION_TIMEOUT_SEC\"] = \"10\"\n",
    "os.environ[\"CLINE_STANDALONE_CAPTURE_STDIO\"] = \"1\"\n",
    "os.environ[\"CLINE_SKIP_RESUME_CONFIRMATION\"] = \"1\"\n",
    "os.environ[\"CLINE_AUTO_FOLLOWUP\"] = \"1\"\n",
    "os.environ[\"CLINE_ENVIRONMENT\"] = \"local\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe74fd",
   "metadata": {},
   "source": [
    "## Train/Test Datasets\n",
    "\n",
    "This code splits SWE-bench Lite into train/test splits.\n",
    "\n",
    "The train set will be used to optimize the ruleset, while the test set will be used to measure the success of optimized rulesets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28654d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"SWE-bench/SWE-bench_Lite\"\n",
    "dataset = load_swebench_dataset(dataset_name, \"test\")\n",
    "ids = [inst[\"instance_id\"] for inst in dataset]\n",
    "random.seed(40)\n",
    "random.shuffle(ids)\n",
    "train_ids = ids[:TRAIN_SIZE]        \n",
    "test_ids = ids[len(ids) - TEST_SIZE:]\n",
    "by_id = {ex[\"instance_id\"]: ex for ex in dataset}\n",
    "train_dataset = [by_id[i] for i in train_ids]\n",
    "test_dataset  = [by_id[i] for i in test_ids]\n",
    "\n",
    "train_pd = pd.DataFrame(train_dataset)\n",
    "test_pd  = pd.DataFrame(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577508b2",
   "metadata": {},
   "source": [
    "## Upload Datasets to Phoenix\n",
    "\n",
    "Upload datasets to Phoenix for experiment tracking and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "\n",
    "phoenix_client = Client(base_url=HOSTNAME, api_key=os.getenv(\"PHOENIX_API_KEY\"))\n",
    "\n",
    "train_dataset = phoenix_client.datasets.create_dataset(\n",
    "    name=\"Cline Act Mode: SWE-bench Train\",\n",
    "    dataset_description=\"Cline Act Mode: SWE-bench Train\",\n",
    "    dataframe=train_pd,\n",
    "    input_keys=['problem_statement'],\n",
    "    metadata_keys=['instance_id', 'test_patch'],\n",
    "    output_keys=[]\n",
    ")\n",
    "\n",
    "test_dataset = phoenix_client.datasets.create_dataset(\n",
    "    name=\"Cline Act Mode: SWE-bench Test\",\n",
    "    dataset_description=\"Cline Act Mode: SWE-bench Test\",\n",
    "    dataframe=test_pd,\n",
    "    input_keys=['problem_statement'],\n",
    "    metadata_keys=['instance_id', 'test_patch'],\n",
    "    output_keys=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2df87",
   "metadata": {},
   "source": [
    "## Helper: Log Experiments to Phoenix\n",
    "\n",
    "This helper function logs experiment results to Phoenix, allowing us to visualize and track optimization progress across iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0246a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix_experiments import log_experiment_to_phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb7a69",
   "metadata": {},
   "source": [
    "## Ruleset Optimization Loop\n",
    "\n",
    "This is the main optimization loop. For each iteration:\n",
    "\n",
    "1. **Run Cline in Act Mode on training set** with the current ruleset, generating actual code patches\n",
    "2. **Run Cline in Act Mode on test set** with the current ruleset to measure generalization\n",
    "3. **Run SWE-bench tests** to validate patches and compute pass/fail metrics\n",
    "4. **Evaluate results** using LLM-as-judge to provide detailed feedback on patch quality\n",
    "5. **Optimize the ruleset** using Prompt Learning based on training results and feedback\n",
    "6. **Save results and rulesets** for tracking and analysis\n",
    "\n",
    "The optimization loop uses actual test execution results (pass/fail) as ground truth, combined with LLM evaluator feedback to iteratively improve the ruleset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aff57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evals_act\n",
    "import importlib\n",
    "importlib.reload(evals_act)\n",
    "from evals_act import evaluate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37486130",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = \"\"\n",
    "\n",
    "for loop in range(LOOPS):\n",
    "    print(f\"Running for loop: {loop}\")\n",
    "\n",
    "    train_run_id = f\"train_{loop}\"\n",
    "    test_run_id = f\"test_{loop}\"\n",
    "\n",
    "    train_df = run_act(dataset_name=dataset_name, instance_ids=train_ids, run_id=train_run_id, ruleset=ruleset, workers=WORKERS)\n",
    "    test_df = run_act(dataset_name=dataset_name, instance_ids=test_ids, run_id=test_run_id, ruleset=ruleset, workers=WORKERS)\n",
    "\n",
    "    test_df.to_csv(f\"act_results/test_results_{loop}.csv\", index=False)\n",
    "    \n",
    "    train_acc = sum(train_df[\"pass_or_fail\"] == \"pass\") / len(train_df)\n",
    "    test_acc = sum(test_df[\"pass_or_fail\"] == \"pass\") / len(test_df)\n",
    "    print(f\"Train Accuracy: {train_acc}\")\n",
    "    print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "    # make sure any swebench package installations did not affect phoenix package\n",
    "    subprocess.run([\n",
    "        \"/opt/anaconda3/envs/cline/bin/python3\",\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"install\",\n",
    "        \"-qq\",\n",
    "        \"--upgrade\",\n",
    "        \"arize-phoenix\",\n",
    "        \"wrapt\",\n",
    "    ])\n",
    "    evaluated_train_results = evaluate_results(train_df)\n",
    "    evaluated_train_results.to_csv(f\"act_results/train_results_{loop}.csv\", index=False)\n",
    "    \n",
    "    # Log experiment to Phoenix using REST API\n",
    "    log_experiment_to_phoenix(\n",
    "        hostname=HOSTNAME,\n",
    "        api_key=os.getenv(\"PHOENIX_API_KEY\"),\n",
    "        dataset_obj=train_dataset,\n",
    "        experiment_name=f\"Train {loop}\",\n",
    "        experiment_df=evaluated_train_results,\n",
    "        metadata={\n",
    "            \"loop\": loop,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"train_size\": TRAIN_SIZE,\n",
    "            \"test_size\": TEST_SIZE\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pl_optimizer = PromptLearningOptimizer(\n",
    "        prompt=CLINE_PROMPT,\n",
    "        model_choice=\"gpt-5\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    ruleset = pl_optimizer.optimize(\n",
    "        dataset=evaluated_train_results,\n",
    "        output_column=\"cline_patch\",\n",
    "        feedback_columns=[\"correctness\", \"explanation\"],\n",
    "        ruleset=ruleset,\n",
    "        context_size_k=400000\n",
    "    )\n",
    "    with open(f\"act_rulesets/ruleset_{loop}.txt\", \"w\") as f:\n",
    "        f.write(f\"train_accuracy: {train_acc} \\n\")\n",
    "        f.write(f\"test_accuracy: {test_acc} \\n\")\n",
    "        f.write(f\"optimized ruleset_{loop}: \\n {ruleset} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5b03e",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Navigate to Phoenix Datasets and Experiments to view your Cline runs, where you can track its improvements. Your results will look something like this:\n",
    "\n",
    "![My Image](https://storage.googleapis.com/arize-phoenix-assets/assets/images/Screenshot%202025-10-13%20at%2011.28.40%E2%80%AFAM.png)\n",
    "\n",
    "As you can see, this run shows a 15% increase in Cline's accuracy on SWE Bench!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909f9b9",
   "metadata": {},
   "source": [
    "# Final Ruleset\n",
    "\n",
    "You can see the rulesets that Cline generated at each optimization in `act_rulesets`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
