import pandas as pd
import json
from typing import Dict
from phoenix.evals.llm import LLM
from phoenix.evals.utils import to_annotation_dataframe
from phoenix.evals import ClassificationEvaluator, evaluate_dataframe


def evaluate_results(results: pd.DataFrame) -> pd.DataFrame:

    results = results[results["cline_patch"].str.len() <= 200000]

    

    prompt = """
    You are an expert software engineer, tasked with reviewing a coding agent. 

    You are given the following information:
    - problem_statement: the problem statement
    - cline_patch: a patch generated by the coding agent, which is supposed to fix the problem.
    - patch: a ground truth solution/patch to the problem
    - test_patch: a test patch that the coding agent's output should pass, which directly addresses the issue in the problem statement
    - pass_or_fail: either "pass" or "fail" indicating whether the coding agent's code changes passed the unit tests.

    Your task is to review the given information and determine if the coding agent's output is correct, and why.
    Evaluate correctness based on the following factors:
    - Whether cline_patch fixes the problem.
    - Whether test_patch would pass after applying cline_patch.
    - Whether coding agent is taking the correct approach to solve the problem.

    You must synthesize why the coding agent's output is correct or incorrect. Try to reason about the coding agent's approach, and why the coding agent may have taken that approach.
    
    problem_statement: {problem_statement}
    ground truth patch: {patch}
    test patch: {test_patch}
    coding agent patch: {cline_patch}
    pass_or_fail: {pass_or_fail}

    Return in the following JSON format:
    "correctness": "correct" or "incorrect"
    "explanation": "brief explanation of your reasoning: why/why not the coding agent's output is correct, and why the coding agent may have taken that approach."
    """

    print("using updated 2.0 script")

    evaluator = ClassificationEvaluator(
        "correctness",
        LLM(provider="openai", model="gpt-5"),
        prompt,
        choices={"correct": 1, "incorrect": 0}
    )
    results_df = evaluate_dataframe(results, [evaluator])["correctness_score"]

    results_df = pd.DataFrame(results_df.apply(json.loads).tolist(), index=results_df.index)

    results["correctness"] = results_df["label"]
    results["explanation"] = results_df["explanation"]
    results["score"] = results_df["score"]

    return results 

