{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c03b478",
   "metadata": {},
   "source": [
    "# Cline Prompt Learning Optimization on SWE-bench\n",
    "\n",
    "This notebook demonstrates how we used Prompt Learning to optimize Cline's performance on the SWE-bench dataset. Cline is a popular and powerful open-source coding agent. We look to improve its performance on SWE-bench by optimizing its **rules**, which are user specified instructions that Cline appends to its system prompt. \n",
    "\n",
    "[More on Cline](https://www.google.com/search?q=cline&sca_esv=764700c983d0c1df&sxsrf=AE3TifOTqxMNetNu45T7bn53deGE6bPn3w%3A1759280858717&ei=2n7caKLCK6XK0PEPpuCv6AY&ved=0ahUKEwiil5P154GQAxUlJTQIHSbwC20Q4dUDCBA&uact=5&oq=cline&gs_lp=Egxnd3Mtd2l6LXNlcnAiBWNsaW5lMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMgoQIxiABBgnGIoFMhMQLhiABBixAxjRAxhDGMcBGIoFMhMQLhiABBixAxjRAxgUGIcCGMcBMgoQABiABBhDGIoFMgoQABiABBhDGIoFMgUQABiABDIFEAAYgAQyChAAGIAEGEMYigVIkw1Q8QNYpgxwA3gBkAEAmAG2AaABkAKqAQMxLjG4AQPIAQD4AQGYAgWgApwCwgIKEAAYsAMY1gQYR8ICDRAAGIAEGLADGEMYigXCAg4QABiwAxjkAhjWBNgBAcICDhAuGLADGLgGGMgD2AEBwgIQEC4YgAQY0QMYQxjHARiKBZgDAIgGAZAGE7oGBggBEAEYCZIHAzQuMaAHwhiyBwMxLjG4B5QCwgcDMC41yAcJ&sclient=gws-wiz-serp#:~:text=Cline%20%2D%20AI%20Coding,https%3A//cline.bot)\n",
    "\n",
    "[More on Prompt Learning](https://arize.com/blog/prompt-learning-using-english-feedback-to-optimize-llm-systems/)\n",
    "\n",
    "## Plan Mode (for now)\n",
    "\n",
    "This is a primitive stage of optimization. We are just looking at **Plan Mode** for Cline, which generates a plan for a given query, referencing the files in the codebase. We are then using an LLM-as-Judge evaluator to judge the generated plan. Therefore the accuracies should be taken lightly - they are not a perfect reflection of Cline's performance because Cline is not actually editing the codebase, and we're not actually running the SWE Bench tests to verify whether its edits are correct.\n",
    "\n",
    "We are still working on running Cline in **Act Mode**, and allowing it actually edit the codebase. Then we can use the tests in SWE bench to compute a firm accuracy of whether Cline made the right edits. Stay tuned.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Please visit README.md and complete all the Setup before running this notebook!**\n",
    "\n",
    "\n",
    "## Important Note\n",
    "\n",
    "Running this notebook can be computationally intensive and expensive as it involves multiple API calls to Claude for each SWE-bench instance. Consider adjusting the training and test set sizes based on your requirements and budget constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0d7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 16:02:25,770 - phoenix.config - INFO - ðŸ“‹ Ensuring phoenix working directory: /Users/priyanjindal/.phoenix\n",
      "2025-09-30 16:02:25,778 - phoenix.inferences.inferences - INFO - Dataset: phoenix_inferences_f39c392d-f102-4b50-9f8a-a68ca1499ee9 initialized\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from swebench.harness.utils import load_swebench_dataset\n",
    "\n",
    "# Add parent directory to Python path to find optimizer_sdk\n",
    "notebook_dir = Path().absolute()  # Get the current notebook's directory\n",
    "parent_dir = notebook_dir.parent  # Get the parent directory\n",
    "sys.path.append(str(parent_dir))  # Add it to Python path\n",
    "\n",
    "from constants import CLINE_PROMPT\n",
    "from evals import evaluate_results\n",
    "from optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n",
    "from run_cline import run_cline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4201b69d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- OPTIMIZATION_LOOPS: number of Prompt Learning loops. How many times you want to optimize your prompt. \n",
    "- TRAIN_SIZE: size of training set. \n",
    "- TEST_SIZE: size of test set.\n",
    "- MAX_WORKERS: SWE Bench is set up to run in parallel, with however many workers you specify. Set this relative to your machine and your Claude rate limits. \n",
    "- RULES: base starting ruleset. I suggest keeping the rule regarding resume_task, as I've noticed using the resume_task tool leads to unstable behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ac2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZATION_LOOPS = 5\n",
    "TRAIN_SIZE = 150 ## Lower this based on your Claude rate limits.\n",
    "TEST_SIZE = 150 ## Lower this based on your Claude rate limits.\n",
    "MAX_WORKERS = 50 ## Lower this based on your Claude rate limits + your machine's memory.\n",
    "RULES = \"do NOT use resume_task tool. Do NOT ask for user input/confirmation at any step of the process.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4324a0",
   "metadata": {},
   "source": [
    "## Train/Test Datasets\n",
    "\n",
    "This code splits SWE-Bench Lite into train/test splits. \n",
    "\n",
    "The train set will be used to optimize the ruleset, while the test set will be used to measure the success of optimized rulesets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_swebench_dataset(\"SWE-bench/SWE-bench_Lite\", \"test\")\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "train_dataset = dataset[:TRAIN_SIZE]\n",
    "test_dataset = dataset[len(dataset)-TEST_SIZE:]\n",
    "train_pd = pd.DataFrame.from_dict(train_dataset)\n",
    "test_pd = pd.DataFrame.from_dict(test_dataset)\n",
    "train_pd.to_csv(\"train_dataset.csv\")\n",
    "test_pd.to_csv(\"test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04a279",
   "metadata": {},
   "source": [
    "This helper function will help us convert our Cline runs on SWE Bench into data we can evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_swebench_results(all_results):\n",
    "    dataset_rows = []\n",
    "    for result in all_results:\n",
    "        instance_id = result[\"instance_id\"]\n",
    "    # Find the corresponding problem statement from train_dataset\n",
    "    problem_statement = next(inst[\"problem_statement\"] for inst in dataset if inst[\"instance_id\"] == instance_id)\n",
    "    patch = next(inst[\"patch\"] for inst in dataset if inst[\"instance_id\"] == instance_id)\n",
    "    test_patch = next(inst[\"test_patch\"] for inst in dataset if inst[\"instance_id\"] == instance_id)\n",
    "    if result[\"final_plan\"]:\n",
    "        dataset_rows.append({\n",
    "            \"instance_id\": instance_id,\n",
    "            \"problem_statement\": problem_statement,\n",
    "            \"final_plan\": result[\"final_plan\"],\n",
    "            \"test_patch\": test_patch,\n",
    "            \"patch\": patch,\n",
    "        })\n",
    "    train_df = pd.DataFrame(dataset_rows)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fdf84",
   "metadata": {},
   "source": [
    "## Helper: Running Cline on a dataset\n",
    "\n",
    "This helper function runs Cline on a dataset. It is meant to be used to run Cline on either your train or test split. \n",
    "\n",
    "It runs Cline in parallel, spinning up MAX_WORKERS # of Cline servers at a time, each server running on a specific row of SWE Bench. \n",
    "\n",
    "It also then evaluates the plans generated by Cline using our LLM-as-judge eval. We simply provide an LLM with the problem statement, the test patch, the ground truth patch, and Cline's generated plan, and ask it if the generated plan seems right. We use this to compute a rough measure of Cline's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae97e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cline_dataset(dataset):\n",
    "    all_results = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(run_cline, inst, i, RULES): inst[\"instance_id\"] for i, inst in enumerate(dataset)}\n",
    "        for fut in as_completed(futs):\n",
    "            r = fut.result()\n",
    "            all_results.append(r)\n",
    "            print(f\"[{r['instance_id']}] done\")\n",
    "    \n",
    "    df = collect_swebench_results(all_results)\n",
    "    evaluated_results = evaluate_results(df)\n",
    "\n",
    "    accuracy = sum(evaluated_results[\"correctness\"] == \"correct\") / len(evaluated_results)\n",
    "    return evaluated_results, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5dafe",
   "metadata": {},
   "source": [
    "## Ruleset Optimization\n",
    "\n",
    "This code optimizes our ruleset for Cline. Here are the steps:\n",
    "\n",
    "Repeats OPTIMIZATION_LOOPS # of times:\n",
    "1. Run Cline, with the current ruleset, on the training set, and compute training accuracy.\n",
    "2. Run Cline, with the current ruleset, on the test set, and compute test accuracy.\n",
    "3. Use the results on the training set to optimize the ruleset, using `PromptLearningOptimizer'\n",
    "4. Update the current ruleset to be the optimized ruleset, for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296941c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(OPTIMIZATION_LOOPS):\n",
    "    print(f\"Running for idx: {idx}\")\n",
    "\n",
    "    evaluated_train_results, train_accuracy = run_cline_dataset(train_dataset)\n",
    "    evaluated_train_results.to_csv(f\"results/train_{idx}.csv\")\n",
    "    evaluated_test_results, test_accuracy = run_cline_dataset(test_dataset)\n",
    "    evaluated_test_results.to_csv(f\"results/test_{idx}.csv\")\n",
    "    \n",
    "    pl_optimizer = PromptLearningOptimizer(\n",
    "        prompt=CLINE_PROMPT,\n",
    "        model_choice=\"gpt-4o\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    ruleset = pl_optimizer.optimize(\n",
    "        dataset=evaluated_train_results,\n",
    "        output_column=\"final_plan\",\n",
    "        feedback_columns=[\"correctness\", \"explanation\"],\n",
    "        ruleset = ruleset,\n",
    "        context_size_k=100000\n",
    "    )\n",
    "\n",
    "    with open(f\"rulesets/ruleset_{idx}.txt\", \"w\") as f:\n",
    "        f.write(f\"train_accuracy: {train_accuracy}\")\n",
    "        f.write(f\"test_accuracy: {test_accuracy}\")\n",
    "        f.write(f\"optimized_ruleset_{idx}: {ruleset}\")\n",
    "        f.write(ruleset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
